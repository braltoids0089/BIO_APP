{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/braltoids0089/BIO_APP/blob/main/(%2B%2B)Digital_Twin_Workflow_(APP_CONVERT)__From_Gene_Expression_to_Pathway_Analysis_Guided_Drug_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Digital Twin Workflow: From Gene Expression to Pathway Analysis-Guided Drug Selection**\n",
        "\n",
        "**A. Introduction**\n",
        "\n",
        "This notebook demonstrates a workflow for identifying potential drug therapies for cancer patients based on their gene expression data. The approach involves:\n",
        "- Loading and processing gene expression data (TPM values) from the Toil database.\n",
        "- Focusing on a subset of genes relevant to predefined signaling pathways.\n",
        "- Performing Principal Component Analysis (PCA) for dimensionality reduction and visualization of samples (Tumor vs Normal).\n",
        "- Scoring the activity of selected signaling pathways for each patient.\n",
        "- Formulating a Quadratic Unconstrained Binary Optimization (QUBO) problem to select a combination of drugs from a predefined panel, considering both potential benefit (based on pathway activity) and penalties for overlapping mechanisms.\n",
        "- Solving the QUBO problem using either an exact solver (for small problems) or a quantum Approximate Optimization Algorithm (QAOA) implemented with CUDA-Q (if available).\n",
        "- Analyzing the frequency and co-selection patterns of the selected drugs across multiple patients.\n",
        "- Generating a patient-level report summarizing selected drugs and pathway context.\n",
        "- Performing a synthetic classification validation to assess the predictive potential of the selected drug features.\n",
        "\n",
        "The goal is to provide a computational approach for personalized therapy selection by mapping biological pathway activity to a discrete optimization problem and exploring the potential for predicting outcomes based on the selected therapies.\n",
        "\n",
        "**B. Scope and Limitations**\n",
        "\n",
        "- **Data:** The model is designed to handle gene expression data, specifically TPM values, and relies on a predefined set of genes and pathways (`SIGS`). The current implementation uses a small subset of samples (`N_SAMPLES = 40`) for demonstration purposes.\n",
        "- **Pathway Definitions:** The drug panel (`example_drug_panel`) and the mapping of drugs to pathways are predefined and simplified.\n",
        "- **QUBO Formulation:** The QUBO formulation is a specific model that balances pathway activation (benefit) with drug overlap (penalty). Other formulations are possible and might yield different results. The penalty matrix (`build_penalty_matrix`) uses a fixed `base_overlap` and `sparsity` parameter.\n",
        "- **Solver:** The exact QUBO solver is limited to small numbers of drugs (K ≤ 20) due to the exponential complexity. The CUDA-Q QAOA solver's performance and availability depend on the CUDA-Q installation and the size of the problem.\n",
        "- **Multi-patient Analysis:** The multi-patient analysis is performed on a limited number of samples (`N=25`) and provides a basic assessment of drug selection frequency and co-selection.\n",
        "- **Patient-Level Reporting:** The patient-level report is a basic summary and does not include detailed clinical information or visualizations.\n",
        "- **Synthetic Classification:** The classification validation uses synthetic labels based on mean pathway activity and is intended as a demonstration of how selected drug features *could* potentially be used for prediction. It does not represent a real-world clinical prediction task and has significant limitations regarding biological complexity and external validity. The high performance metrics (ROC-AUC, PR-AUC) are expected due to the synthetic nature of the labels and should not be interpreted as indicative of real-world predictive accuracy.\n",
        "- **Biological Complexity:** The model simplifies complex biological interactions and drug mechanisms. It does not account for all potential factors influencing drug response, such as pharmacokinetics, pharmacodynamics, patient history, or other genetic variations.\n",
        "- **Clinical Validation:** The results from this model are computational predictions and require rigorous clinical validation before being used for actual treatment decisions.\n",
        "\n",
        "**C. Model Modules and Implementation**\n",
        "\n",
        "The model is implemented in Python and utilizes several key libraries:\n",
        "- **`pandas`:** For data loading, manipulation, and structuring (DataFrames).\n",
        "- **`numpy`:** For numerical operations, including matrix calculations for PCA (SVD) and QUBO formulation.\n",
        "- **`gzip` and `re`:** For reading and processing the compressed data file.\n",
        "- **`collections.Counter`:** For summarizing drug selection frequencies.\n",
        "- **`matplotlib.pyplot`:** For generating visualizations (PCA scatter plot, heatmap, bar plot, dendrogram).\n",
        "- **`scipy.cluster.hierarchy`:** For hierarchical clustering and dendrogram generation.\n",
        "- **`cudaq` (optional):** For attempting to solve the QUBO problem using QAOA on quantum hardware or simulators.\n",
        "- **`itertools`:** For generating combinations of drugs for co-selection analysis.\n",
        "- **`sklearn.linear_model.LogisticRegression` and `sklearn.metrics`:** For the synthetic classification validation.\n",
        "\n",
        "***The code is organized into distinct sections:***\n",
        "\n",
        "- **Setup:** Installs necessary libraries and handles environment setup.\n",
        "- **Parameters & data fetch:** Defines parameters and downloads the input data.\n",
        "- **Unified ingest → small expression matrix:** Reads and processes the raw data to create a small expression matrix (`expr_sym_small`).\n",
        "- **Baseline PCA:** Performs PCA on the expression data.\n",
        "- **Pathway scoring:** Implements functions for z-scoring and calculating pathway activity scores (`P`).\n",
        "- **QUBO → CUDA-Q QAOA:** Defines the drug panel, utility functions for QUBO formulation, and the QAOA solver (with an exact fallback).\n",
        "- **Multi-patient + Stability:** Runs the drug selection process for multiple patients and summarizes the frequency and size distribution of drug selections.\n",
        "- **Visualization:** Generates plots to visualize the results (drug frequency barplot, co-selection heatmap, dendrogram, clustered heatmap, selection size histogram, classification coefficients).\n",
        "- **Top co-selected pairs:** Analyzes and ranks the most frequently co-selected drug pairs.\n",
        "- **Patient-level report:** Generates a summary table for each patient, including selected drugs and pathway context.\n",
        "- **Synthetic Classification Validation:** Performs a classification task using selected drugs as features and synthetic labels to explore predictive potential.\n",
        "\n",
        "\n",
        "**D. Key Points**\n",
        "\n",
        "- The notebook provides a complete workflow from raw gene expression data to personalized drug therapy selection using a computational approach.\n",
        "- It demonstrates how to integrate biological pathway knowledge into a quantitative model.\n",
        "- The use of a QUBO formulation allows the problem to be potentially addressed by quantum computing approaches like QAOA, while also providing a classical exact solver fallback.\n",
        "- The analysis of drug selection frequency and co-selection patterns across patients provides insights into the potential stability and commonalities of the predicted therapies.\n",
        "- The notebook includes a patient-level report for summarizing individual therapy selections and their biological context.\n",
        "- A synthetic classification validation is performed to demonstrate how selected drug features could potentially be used for predicting outcomes.\n",
        "- The approach is modular, with distinct steps for data processing, analysis, optimization, reporting, and validation."
      ],
      "metadata": {
        "id": "rC6oAb25AhDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0) Setup (lean pins + CUDA-Q; single restart)**"
      ],
      "metadata": {
        "id": "guoTlxKrCAg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔧 Hotfix: align NumPy/Matplotlib/SciPy/Sklearn (auto-restart)\n",
        "import os, sys, subprocess\n",
        "\n",
        "pkgs = [\n",
        "    \"numpy<2\",              # 1.26.x\n",
        "    \"scipy<1.12\",           # built against numpy 1.26\n",
        "    \"scikit-learn==1.3.2\",  # compatible with numpy 1.26\n",
        "    \"matplotlib==3.8.4\",    # compatible with numpy 1.26\n",
        "    \"pandas==2.2.2\",\n",
        "]\n",
        "cmd=[sys.executable,\"-m\",\"pip\",\"install\",\"-q\",\"--no-cache-dir\",\"--force-reinstall\"]+pkgs\n",
        "print(\"Installing:\", pkgs)\n",
        "subprocess.check_call(cmd)\n",
        "\n",
        "# optional: (re)install CUDA-Q after aligning stack\n",
        "try:\n",
        "    subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",\"-q\",\"--no-cache-dir\",\"cudaq\"])\n",
        "    print(\"CUDA-Q OK\")\n",
        "except subprocess.CalledProcessError:\n",
        "    print(\"CUDA-Q install skipped (you can try again later)\")\n",
        "\n",
        "# one-time restart to load the new ABI\n",
        "flag=\"/content/.abi_aligned_np126\"\n",
        "if not os.path.exists(flag):\n",
        "    open(flag,\"w\").close()\n",
        "    print(\"🔁 Restarting runtime to finalize ABI alignment…\")\n",
        "    os._exit(0)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:30:59.177034Z",
          "iopub.execute_input": "2025-08-25T06:30:59.177306Z",
          "execution_failed": "2025-08-25T06:32:34.584Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9prc70AzCAg8",
        "outputId": "573844c6-57c3-47b6-d7a2-d94831d9056b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing: ['numpy<2', 'scipy<1.12', 'scikit-learn==1.3.2', 'matplotlib==3.8.4', 'pandas==2.2.2']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1) Parameters & data fetch (Toil TPM; tiny sample count)**"
      ],
      "metadata": {
        "id": "tkLnujh_Eloe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Params & download Toil TPM\n",
        "import os, requests\n",
        "\n",
        "# keep small to stay light\n",
        "N_SAMPLES = 40  # increase to 80–120 later if you want\n",
        "\n",
        "TPM_URL  = \"https://toil.xenahubs.net/download/TcgaTargetGtex_rsem_gene_tpm.gz\"\n",
        "TPM_PATH = \"/content/TcgaTargetGtex_rsem_gene_tpm.gz\"\n",
        "\n",
        "if not os.path.exists(TPM_PATH):\n",
        "    print(\"Downloading Toil TPM…\")\n",
        "    r = requests.get(TPM_URL, stream=True, timeout=300); r.raise_for_status()\n",
        "    with open(TPM_PATH,\"wb\") as f:\n",
        "        for ch in r.iter_content(1<<20):\n",
        "            if ch: f.write(ch)\n",
        "    print(\"✅ Saved:\", TPM_PATH)\n",
        "else:\n",
        "    print(\"Found:\", TPM_PATH)\n"
      ],
      "metadata": {
        "id": "yFUlbYEwHFF4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:36:04.285997Z",
          "iopub.execute_input": "2025-08-25T06:36:04.286952Z",
          "iopub.status.idle": "2025-08-25T06:36:04.293058Z",
          "shell.execute_reply.started": "2025-08-25T06:36:04.286920Z",
          "shell.execute_reply": "2025-08-25T06:36:04.292079Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2–6) Unified ingest → small expression matrix (expr_sym_small)**"
      ],
      "metadata": {
        "id": "ywHs1PwNCAg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2–6) One-shot ingest → expr_sym_small (streamed, robust)\n",
        "import re, gzip, pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# A) read header → gene_col + sample subset\n",
        "with gzip.open(TPM_PATH, \"rt\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "    header = f.readline().rstrip(\"\\n\")\n",
        "cols = header.split(\"\\t\")\n",
        "gene_col = cols[0]\n",
        "sample_cols = [c for c in cols[1:] if isinstance(c,str) and c.startswith(\"TCGA-\")]\n",
        "if not sample_cols: sample_cols = cols[1:]\n",
        "SELECTED_SAMPLES = sample_cols[:N_SAMPLES]\n",
        "print(f\"gene_col={gene_col}  |  selected_samples={len(SELECTED_SAMPLES)}\")\n",
        "\n",
        "# B) compact pathway signatures (editable)\n",
        "SIGS = {\n",
        "    \"REACTOME_SIGNALING_BY_EGFR\": [\n",
        "        \"EGFR\",\"ERBB2\",\"ERBB3\",\"GRB2\",\"SOS1\",\"SHC1\",\"PTPN11\",\"KRAS\",\"NRAS\",\"HRAS\",\n",
        "        \"BRAF\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"GAB1\"\n",
        "    ],\n",
        "    \"REACTOME_SIGNALING_BY_ALK\": [\n",
        "        \"ALK\",\"EML4\",\"GRB2\",\"SHC1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"STAT3\",\"MAP2K1\",\"MAPK1\",\"MAPK3\"\n",
        "    ],\n",
        "    \"REACTOME_MAPK1_MAPK3_SIGNALING\": [\n",
        "        \"BRAF\",\"RAF1\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"DUSP6\",\"DUSP4\",\"FOS\",\"JUN\",\"EGFR\"\n",
        "    ],\n",
        "    \"REACTOME_PI3K_AKT_SIGNALING\": [\n",
        "        \"PIK3CA\",\"PIK3CB\",\"PIK3CD\",\"PIK3R1\",\"PIK3R2\",\"AKT1\",\"AKT2\",\"AKT3\",\"PTEN\",\"MTOR\",\"RHEB\"\n",
        "    ],\n",
        "    \"REACTOME_MTORC1_MEDIATED_SIGNALLING\": [\n",
        "        \"MTOR\",\"RPTOR\",\"MLST8\",\"RHEB\",\"TSC1\",\"TSC2\",\"EIF4EBP1\",\"RPS6KB1\",\"RPS6\"\n",
        "    ],\n",
        "    \"REACTOME_PD1_SIGNALING\": [\n",
        "        \"PDCD1\",\"CD274\",\"PDCD1LG2\",\"JAK1\",\"JAK2\",\"STAT1\",\"IFNG\",\"GZMB\",\"LAG3\",\"TIGIT\",\"CXCL9\",\"CXCL10\"\n",
        "    ],\n",
        "    \"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\": [\n",
        "        \"VEGFA\",\"KDR\",\"FLT1\",\"FLT4\",\"PTPRB\",\"PLCG1\",\"MAP2K1\",\"MAPK1\",\"NOS3\"\n",
        "    ],\n",
        "    \"REACTOME_SIGNALING_BY_FGFR\": [\n",
        "        \"FGFR1\",\"FGFR2\",\"FGFR3\",\"FGFR4\",\"FRS2\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"MAP2K1\",\"MAPK1\"\n",
        "    ]\n",
        "}\n",
        "SIG_GENES = sorted({g for gs in SIGS.values() for g in gs})\n",
        "\n",
        "# C) built-in symbol→Ensembl map (no network calls)\n",
        "SYM2ENSG = {\n",
        "    \"EGFR\":\"ENSG00000146648\",\"ERBB2\":\"ENSG00000141736\",\"ERBB3\":\"ENSG00000065361\",\"GRB2\":\"ENSG00000177885\",\n",
        "    \"SOS1\":\"ENSG00000115904\",\"SHC1\":\"ENSG00000154639\",\"PTPN11\":\"ENSG00000179295\",\"KRAS\":\"ENSG00000133703\",\n",
        "    \"NRAS\":\"ENSG00000213281\",\"HRAS\":\"ENSG00000174775\",\"BRAF\":\"ENSG00000157764\",\"MAP2K1\":\"ENSG00000169032\",\n",
        "    \"MAP2K2\":\"ENSG00000126934\",\"MAPK1\":\"ENSG00000100030\",\"MAPK3\":\"ENSG00000102882\",\"PLCG1\":\"ENSG00000124181\",\n",
        "    \"PIK3CA\":\"ENSG00000121879\",\"PIK3R1\":\"ENSG00000145675\",\"AKT1\":\"ENSG00000142208\",\"AKT2\":\"ENSG00000105221\",\n",
        "    \"AKT3\":\"ENSG00000117020\",\"GAB1\":\"ENSG00000117676\",\n",
        "    \"ALK\":\"ENSG00000171094\",\"EML4\":\"ENSG00000143924\",\"STAT3\":\"ENSG00000168610\",\n",
        "    \"DUSP6\":\"ENSG00000139318\",\"DUSP4\":\"ENSG00000120875\",\"FOS\":\"ENSG00000170345\",\"JUN\":\"ENSG00000177606\",\"RAF1\":\"ENSG00000132155\",\n",
        "    \"PIK3CB\":\"ENSG00000119402\",\"PIK3CD\":\"ENSG00000171608\",\"PIK3R2\":\"ENSG00000189403\",\"PTEN\":\"ENSG00000171862\",\n",
        "    \"MTOR\":\"ENSG00000198793\",\"RHEB\":\"ENSG00000106615\",\n",
        "    \"RPTOR\":\"ENSG00000141564\",\"MLST8\":\"ENSG00000105705\",\"TSC1\":\"ENSG00000165699\",\"TSC2\":\"ENSG00000103197\",\n",
        "    \"EIF4EBP1\":\"ENSG00000187840\",\"RPS6KB1\":\"ENSG00000108443\",\"RPS6\":\"ENSG00000137154\",\n",
        "    \"PDCD1\":\"ENSG00000276977\",\"CD274\":\"ENSG00000120217\",\"PDCD1LG2\":\"ENSG00000197646\",\"JAK1\":\"ENSG00000162434\",\n",
        "    \"JAK2\":\"ENSG00000096968\",\"STAT1\":\"ENSG00000115415\",\"IFNG\":\"ENSG00000111537\",\"GZMB\":\"ENSG00000100453\",\n",
        "    \"LAG3\":\"ENSG00000089692\",\"TIGIT\":\"ENSG00000181847\",\"CXCL9\":\"ENSG00000138755\",\"CXCL10\":\"ENSG00000169245\",\n",
        "    \"VEGFA\":\"ENSG00000112715\",\"KDR\":\"ENSG00000128052\",\"FLT1\":\"ENSG00000102755\",\"FLT4\":\"ENSG00000037280\",\n",
        "    \"PTPRB\":\"ENSG00000160593\",\"NOS3\":\"ENSG00000164867\",\n",
        "    \"FGFR1\":\"ENSG00000077782\",\"FGFR2\":\"ENSG00000066468\",\"FGFR3\":\"ENSG00000068078\",\"FGFR4\":\"ENSG00000069535\",\n",
        "    \"FRS2\":\"ENSG00000181873\"\n",
        "}\n",
        "ENSG_SET = set(SYM2ENSG.get(g) for g in SIG_GENES if g in SYM2ENSG)\n",
        "\n",
        "# D) detect row ID type (Ensembl vs symbol)\n",
        "def detect_row_mode(path, scan_rows=50000):\n",
        "    seen = Counter(); total = 0\n",
        "    with gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        reader = pd.read_csv(f, sep=\"\\t\", chunksize=200_000, usecols=[0], dtype=str, header=0)\n",
        "        for ch in reader:\n",
        "            v = ch.iloc[:,0].astype(str)\n",
        "            vals = v.head(min(len(v), scan_rows-total)).tolist()\n",
        "            total += len(vals)\n",
        "            seen.update('ENSG' if x.startswith('ENSG') else 'OTHER' for x in vals)\n",
        "            if total >= scan_rows: break\n",
        "    ratio = seen['ENSG']/max(1,(seen['ENSG']+seen['OTHER']))\n",
        "    mode = 'ensembl' if ratio >= 0.6 else 'symbol'\n",
        "    print(f\"Row mode: {mode.upper()} (ENSG ratio={ratio:.2f})\")\n",
        "    return mode\n",
        "\n",
        "row_mode = detect_row_mode(TPM_PATH)\n",
        "\n",
        "# E) stream-select rows + columns (light)\n",
        "def norm_ensembl(x): return x.split('.',1)[0]\n",
        "def norm_symbol(x):  return re.sub(r'[^A-Za-z0-9_-]+','', x)\n",
        "\n",
        "TARGET_ROWS = ENSG_SET if row_mode=='ensembl' else set(SIG_GENES)\n",
        "normalize   = norm_ensembl if row_mode=='ensembl' else norm_symbol\n",
        "\n",
        "usecols = [gene_col] + SELECTED_SAMPLES\n",
        "kept, n_hits = [], 0\n",
        "with gzip.open(TPM_PATH, \"rt\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "    reader = pd.read_csv(f, sep=\"\\t\", chunksize=50_000, dtype=str,\n",
        "                         usecols=lambda c: (c in usecols) or (c == gene_col))\n",
        "    for ch in reader:\n",
        "        ch = ch.rename(columns={gene_col: \"row_id\"})\n",
        "        ids = ch[\"row_id\"].astype(str).map(normalize)\n",
        "        mask = ids.isin(TARGET_ROWS)\n",
        "        if mask.any():\n",
        "            out = ch.loc[mask].copy()\n",
        "            out[\"row_id\"] = out[\"row_id\"].map(normalize)\n",
        "            kept.append(out); n_hits += int(mask.sum())\n",
        "\n",
        "if n_hits == 0:\n",
        "    raise RuntimeError(\"No signature rows matched. Try smaller N_SAMPLES or a tinier SIGS.\")\n",
        "\n",
        "expr_small = pd.concat(kept, axis=0, ignore_index=False).drop_duplicates(subset=[\"row_id\"]).set_index(\"row_id\")\n",
        "\n",
        "# F) convert to symbols if needed\n",
        "if row_mode == 'ensembl':\n",
        "    ENSG2SYM = {v:k for k,v in SYM2ENSG.items()}\n",
        "    expr_sym_small = expr_small.copy()\n",
        "    expr_sym_small.index = [ENSG2SYM.get(e, e) for e in expr_small.index]\n",
        "else:\n",
        "    expr_sym_small = expr_small.copy()\n",
        "\n",
        "expr_sym_small = expr_sym_small[~expr_sym_small.index.duplicated(keep=\"first\")]\n",
        "print(f\"✅ expr_sym_small ready: {expr_sym_small.shape} (genes × samples)\")\n",
        "display(expr_sym_small.iloc[:5, :5])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:36:08.202830Z",
          "iopub.execute_input": "2025-08-25T06:36:08.203215Z",
          "iopub.status.idle": "2025-08-25T06:39:42.770454Z",
          "shell.execute_reply.started": "2025-08-25T06:36:08.203190Z",
          "shell.execute_reply": "2025-08-25T06:39:42.769433Z"
        },
        "id": "P0CGO3R5CAg_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7) Baseline PCA (TCGA Tumor vs Normal coloring; no phenotype needed)**"
      ],
      "metadata": {
        "id": "9KeNaQQKCAhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7) Clean → PCA (robust to string/NaN; uses NumPy SVD; optional plot)\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# 7a) Sanitize expression matrix (genes x samples, numeric float)\n",
        "E = expr_sym_small.copy()\n",
        "\n",
        "# force numeric for every cell (non-numeric -> NaN)\n",
        "E = E.apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# drop genes that are entirely NaN across samples\n",
        "E = E.loc[~E.isna().all(axis=1)]\n",
        "\n",
        "# if any sample (column) is entirely NaN, drop it\n",
        "E = E.loc[:, ~E.isna().all(axis=0)]\n",
        "\n",
        "# per-gene mean imputation for remaining NaNs (keeps z-scoring stable)\n",
        "gene_means = E.mean(axis=1)\n",
        "E = E.apply(lambda col: col.fillna(gene_means), axis=0)\n",
        "\n",
        "print(\"Cleaned expr shape (genes x samples):\", E.shape)\n",
        "\n",
        "# 7b) Build groups from TCGA barcode (Tumor vs Normal)\n",
        "def groups_from_barcode(ids):\n",
        "    def code(s):\n",
        "        p = s.split('-')\n",
        "        return int(p[3][:2]) if (len(p)>=4 and len(p[3])>=2 and p[3][:2].isdigit()) else None\n",
        "    def bucket(c):\n",
        "        if c is None: return \"Other\"\n",
        "        if 1 <= c <= 9:   return \"Tumor\"\n",
        "        if 10<= c <= 19:  return \"Normal\"\n",
        "        return \"Other\"\n",
        "    return pd.Series([bucket(code(s)) for s in ids], index=ids)\n",
        "\n",
        "X = E.T  # samples x genes\n",
        "groups = groups_from_barcode(list(X.index))\n",
        "\n",
        "# drop zero-variance genes (after imputation some can still be flat)\n",
        "std = X.std(axis=0)\n",
        "X = X.loc[:, (std > 0).values]\n",
        "\n",
        "# standardize features (genes)\n",
        "mu = X.mean(axis=0).to_numpy(na_value=0.0)\n",
        "sd = X.std(axis=0).to_numpy(na_value=0.0) + 1e-8\n",
        "Xz = (X.to_numpy() - mu) / sd\n",
        "\n",
        "# PCA via NumPy SVD (no sklearn needed)\n",
        "U, S, VT = np.linalg.svd(Xz, full_matrices=False)\n",
        "scores = U * S\n",
        "var = S**2\n",
        "explained = (var / var.sum()) * 100.0\n",
        "\n",
        "print(\"Explained variance (%):\", np.round(explained[:10], 2))\n",
        "\n",
        "# Optional scatter plot (PC1 vs PC2). If matplotlib missing, we just print EVR.\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(7,6))\n",
        "    labels = groups.values\n",
        "    for g in pd.unique(labels):\n",
        "        idx = (labels == g)\n",
        "        plt.scatter(scores[idx,0], scores[idx,1], label=g, alpha=0.75)\n",
        "    plt.xlabel(f\"PC1 ({explained[0]:.1f}% var)\")\n",
        "    plt.ylabel(f\"PC2 ({explained[1]:.1f}% var)\")\n",
        "    plt.title(\"PCA (NumPy SVD) — TCGA Tumor vs Normal\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"Plot skipped (matplotlib issue):\", repr(e))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:40:03.632162Z",
          "iopub.execute_input": "2025-08-25T06:40:03.632498Z",
          "iopub.status.idle": "2025-08-25T06:40:03.908028Z",
          "shell.execute_reply.started": "2025-08-25T06:40:03.632474Z",
          "shell.execute_reply": "2025-08-25T06:40:03.907035Z"
        },
        "id": "sVRTNHgQCAhA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8) Pathway scoring (lightweight, no extra deps)**"
      ],
      "metadata": {
        "id": "_3ozlJReCAhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8a) Helpers: z-scoring & pathway scoring (mean of member z's)\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def zscore_by_gene(expr_symbols: pd.DataFrame) -> pd.DataFrame:\n",
        "    E = expr_symbols.copy()\n",
        "    E = E.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    # drop all-NaN genes, then impute per-gene mean for remaining NaNs\n",
        "    E = E.loc[~E.isna().all(axis=1)]\n",
        "    gene_means = E.mean(axis=1)\n",
        "    E = E.apply(lambda col: col.fillna(gene_means), axis=0)\n",
        "    mu = E.mean(axis=1)\n",
        "    sd = E.std(axis=1) + 1e-8\n",
        "    return (E.sub(mu, axis=0)).div(sd, axis=0)\n",
        "\n",
        "def pathway_scores(expr_symbols: pd.DataFrame, signatures: dict) -> pd.DataFrame:\n",
        "    \"\"\"Return pathways x samples (mean z across member genes present).\"\"\"\n",
        "    Z = zscore_by_gene(expr_symbols)\n",
        "    rows = []\n",
        "    for pw, genes in signatures.items():\n",
        "        present = [g for g in genes if g in Z.index]\n",
        "        if present:\n",
        "            s = Z.loc[present].mean(axis=0)\n",
        "        else:\n",
        "            s = pd.Series([np.nan]*Z.shape[1], index=Z.columns)\n",
        "        s.name = pw\n",
        "        rows.append(s)\n",
        "    return pd.DataFrame(rows)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:40:57.342774Z",
          "iopub.execute_input": "2025-08-25T06:40:57.343115Z",
          "iopub.status.idle": "2025-08-25T06:40:57.351425Z",
          "shell.execute_reply.started": "2025-08-25T06:40:57.343089Z",
          "shell.execute_reply": "2025-08-25T06:40:57.350515Z"
        },
        "id": "A4bKIwjOCAhD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8b) Compute pathway matrix P (pathways × samples)\n",
        "# Uses SIGS defined earlier in your notebook (from Section 2–6)\n",
        "P = pathway_scores(expr_sym_small, SIGS)\n",
        "print(\"Pathways x samples:\", P.shape)\n",
        "display(P.iloc[:5, :5])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:41:02.429666Z",
          "iopub.execute_input": "2025-08-25T06:41:02.430249Z",
          "iopub.status.idle": "2025-08-25T06:41:02.465631Z",
          "shell.execute_reply.started": "2025-08-25T06:41:02.430221Z",
          "shell.execute_reply": "2025-08-25T06:41:02.464881Z"
        },
        "id": "Pj80W-ZVCAhD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8c) (Optional) Quick visualization: small heatmap (no seaborn)\n",
        "import numpy as np\n",
        "\n",
        "# pick at most 12 pathways with highest variance to keep it light\n",
        "var_by_pw = P.var(axis=1).sort_values(ascending=False)\n",
        "pw_keep = list(var_by_pw.index[:12])\n",
        "Ps = P.loc[pw_keep]\n",
        "\n",
        "# z-score per pathway for viz (center rows)\n",
        "Ps_vis = (Ps - Ps.mean(axis=1).values.reshape(-1,1)) / (Ps.std(axis=1).values.reshape(-1,1) + 1e-8)\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(min(12, 0.3*Ps_vis.shape[1]+3), 6))\n",
        "    plt.imshow(Ps_vis.values, aspect='auto', interpolation='nearest')\n",
        "    plt.xticks(range(Ps_vis.shape[1]), Ps_vis.columns.str.slice(0,12), rotation=90)\n",
        "    plt.yticks(range(Ps_vis.shape[0]), Ps_vis.index)\n",
        "    plt.colorbar(label=\"z (within pathway)\")\n",
        "    plt.title(\"Pathway activity (subset)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"Heatmap skipped:\", repr(e))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:41:07.629567Z",
          "iopub.execute_input": "2025-08-25T06:41:07.629859Z",
          "iopub.status.idle": "2025-08-25T06:41:08.103927Z",
          "shell.execute_reply.started": "2025-08-25T06:41:07.629837Z",
          "shell.execute_reply": "2025-08-25T06:41:08.103085Z"
        },
        "id": "ahj-tjj-CAhE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9) QUBO → CUDA-Q QAOA (discrete therapy selection)**"
      ],
      "metadata": {
        "id": "xPGo_BNOCAhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 9a) Drug panel & QUBO utilities (light)\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def example_drug_panel():\n",
        "    return {\n",
        "        \"EGFRi\": [\"REACTOME_SIGNALING_BY_EGFR\"],\n",
        "        \"ALKi\":  [\"REACTOME_SIGNALING_BY_ALK\"],\n",
        "        \"MEKi\":  [\"REACTOME_MAPK1_MAPK3_SIGNALING\"],\n",
        "        \"PI3Ki\": [\"REACTOME_PI3K_AKT_SIGNALING\"],\n",
        "        \"mTORi\": [\"REACTOME_MTORC1_MEDIATED_SIGNALLING\"],\n",
        "        \"PD1i\":  [\"REACTOME_PD1_SIGNALING\"],\n",
        "        \"VEGFi\": [\"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\"],\n",
        "        \"FGFRi\": [\"REACTOME_SIGNALING_BY_FGFR\"],\n",
        "    }\n",
        "\n",
        "def patient_vector(P: pd.DataFrame, sample_id: str) -> pd.Series:\n",
        "    \"\"\"z-normalize pathways across samples; return vector for one patient.\"\"\"\n",
        "    z = (P - P.mean(axis=1).values.reshape(-1,1)) / (P.std(axis=1).values.reshape(-1,1) + 1e-8)\n",
        "    return z[sample_id].fillna(0.0)\n",
        "\n",
        "def drug_benefit_prior(z_path: pd.Series, panel: dict) -> pd.Series:\n",
        "    \"\"\"Aggregate pathway z's per drug (ReLU to emphasize upregulated pathways).\"\"\"\n",
        "    s = pd.Series({d: float(np.sum([max(z_path.get(p, 0.0), 0.0) for p in pws])) for d, pws in panel.items()})\n",
        "    return s / s.max() if s.max() > 0 else s\n",
        "\n",
        "def build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10):\n",
        "    \"\"\"Pairwise penalties for overlapping mechanisms; diagonal = sparsity.\"\"\"\n",
        "    K = len(drugs); R = np.zeros((K, K), dtype=float)\n",
        "    for i in range(K):\n",
        "        for j in range(i+1, K):\n",
        "            overlap = len(set(panel[drugs[i]]) & set(panel[drugs[j]]))\n",
        "            if overlap > 0:\n",
        "                R[i, j] = R[j, i] = base_overlap * overlap\n",
        "    for i in range(K):\n",
        "        R[i, i] += sparsity\n",
        "    return R\n",
        "\n",
        "def build_qubo(b_hat, R, lam=1.0):\n",
        "    \"\"\"QUBO: minimize x^T (lam R) x + q^T x  where q = -b̂ + diag(lam R).\"\"\"\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy()\n",
        "    diag = np.diag(Q).copy()\n",
        "    np.fill_diagonal(Q, 0.0)  # keep off-diagonal in Q\n",
        "    q += diag\n",
        "    return q, np.triu(Q, 1)\n",
        "\n",
        "def qubo_to_ising(q, Q):\n",
        "    \"\"\"Map QUBO to Ising E(z)=h·z + z^T J z  (z ∈ {±1}).\"\"\"\n",
        "    K = len(q); Qf = Q + Q.T\n",
        "    h = np.zeros(K); J = np.zeros((K, K)); const = 0.0\n",
        "    for i in range(K):\n",
        "        const += 0.5 * q[i]\n",
        "        h[i] += -0.5 * q[i]\n",
        "    for i in range(K):\n",
        "        for j in range(i+1, K):\n",
        "            Qij = Qf[i, j]\n",
        "            const += 0.25 * Qij\n",
        "            h[i]  += -0.25 * Qij\n",
        "            h[j]  += -0.25 * Qij\n",
        "            J[i, j] += 0.25 * Qij\n",
        "    return h, J, const\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:43:33.443473Z",
          "iopub.execute_input": "2025-08-25T06:43:33.444085Z",
          "iopub.status.idle": "2025-08-25T06:43:33.457406Z",
          "shell.execute_reply.started": "2025-08-25T06:43:33.444057Z",
          "shell.execute_reply": "2025-08-25T06:43:33.456527Z"
        },
        "id": "y-fW3a52CAhF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 9b) CUDA-Q QAOA solver (p small; safe fallback built-in)\n",
        "# Will attempt to import cudaq; if not present, sets CUDAQ_OK=False\n",
        "CUDAQ_OK = True\n",
        "try:\n",
        "    import cudaq  # type: ignore\n",
        "    from cudaq import spin  # type: ignore\n",
        "except Exception:\n",
        "    CUDAQ_OK = False\n",
        "    print(\"⚠️ CUDA-Q not available; you can still inspect QUBO or plug a classical solver.\")\n",
        "\n",
        "def qaoa_solve(h, J, p=2, shots=2048, max_iters=60, seed=7):\n",
        "    \"\"\"Return dict with best_state bitstring and counts; None if CUDA-Q missing.\"\"\"\n",
        "    if not CUDAQ_OK:\n",
        "        return None\n",
        "    import numpy as np, cudaq\n",
        "    from cudaq import spin\n",
        "\n",
        "    def build_H(h,J):\n",
        "        H = 0.0 * spin.z(0); H = 0.0 * H\n",
        "        K=len(h)\n",
        "        for i in range(K):\n",
        "            if abs(h[i])>0: H += h[i]*spin.z(i)\n",
        "        for i in range(K):\n",
        "            for j in range(i+1,K):\n",
        "                if abs(J[i,j])>1e-12: H += J[i,j]*spin.z(i)*spin.z(j)\n",
        "        return H\n",
        "\n",
        "    H = build_H(h, J); K=len(h)\n",
        "\n",
        "    @cudaq.kernel\n",
        "    def ansatz(params: list[float]):\n",
        "        q = cudaq.qvector(K)\n",
        "        for i in range(K): cudaq.h(q[i])\n",
        "        for layer in range(p):\n",
        "            gamma = params[layer]\n",
        "            for i in range(K):\n",
        "                if abs(h[i])>0: cudaq.rz(2.0*gamma*h[i], q[i])\n",
        "            for i in range(K):\n",
        "                for j in range(i+1,K):\n",
        "                    if abs(J[i,j])>1e-12:\n",
        "                        cudaq.cx(q[i], q[j]); cudaq.rz(2.0*gamma*J[i,j], q[j]); cudaq.cx(q[i], q[j])\n",
        "            beta = params[p+layer]\n",
        "            for i in range(K): cudaq.rx(2.0*beta, q[i])\n",
        "\n",
        "    def objective(params):\n",
        "        return cudaq.observe(ansatz, H, params, shots_count=shots).expectation()\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    x0 = np.random.uniform(0,1,2*p).tolist()\n",
        "    try:\n",
        "        res = cudaq.optimize(objective, x0=x0, max_eval=max_iters)\n",
        "        params_opt = res.optimal_parameters\n",
        "    except Exception:\n",
        "        # tiny random search fallback\n",
        "        grid=[np.random.uniform(0,1,2*p) for _ in range(48)]\n",
        "        vals=[objective(g.tolist()) for g in grid]\n",
        "        params_opt = grid[int(np.argmin(vals))].tolist()\n",
        "\n",
        "    counts = cudaq.sample(ansatz, params_opt, shots_count=shots)\n",
        "    # choose lowest-energy state from samples\n",
        "    best_state=None; best_energy=float(\"inf\")\n",
        "    for bitstring, c in counts.items():\n",
        "        z = np.array([1 if b=='0' else -1 for b in bitstring[::-1]])\n",
        "        e = float(np.dot(h, z) + sum(J[i,j]*z[i]*z[j] for i in range(len(z)) for j in range(i+1,len(z))))\n",
        "        if e < best_energy:\n",
        "            best_energy, best_state = e, bitstring\n",
        "    return {\"best_state\": best_state, \"counts\": dict(counts)}\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:43:40.148719Z",
          "iopub.execute_input": "2025-08-25T06:43:40.149170Z",
          "iopub.status.idle": "2025-08-25T06:43:40.168015Z",
          "shell.execute_reply.started": "2025-08-25T06:43:40.149137Z",
          "shell.execute_reply": "2025-08-25T06:43:40.167181Z"
        },
        "id": "LDICrszICAhF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 9b) Robust solvers: exact QUBO (fast for K≤20) + optional CUDA-Q\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def exact_qubo_solve(b_hat: np.ndarray, R: np.ndarray, lam: float = 1.0):\n",
        "    \"\"\"\n",
        "    Exact minimization of QUBO:\n",
        "      E(x) = x^T (lam R) x + q^T x, with q = -b̂ + diag(lam R)\n",
        "    We enumerate all bitstrings (2^K). Returns best bitstring (as 0/1 np array).\n",
        "    \"\"\"\n",
        "    K = len(b_hat)\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy() + np.diag(Q)\n",
        "    np.fill_diagonal(Q, 0.0)  # keep only off-diagonal in Q\n",
        "\n",
        "    best_e = np.inf\n",
        "    best_x = None\n",
        "    # vectorize partial precomputations\n",
        "    upper_idx = np.triu_indices(K, 1)\n",
        "    for mask in range(1 << K):\n",
        "        # build x from bits\n",
        "        x = np.fromiter(((mask >> i) & 1 for i in range(K)), dtype=np.int8)\n",
        "        # E = x^T Q x + q^T x, where Q is strictly upper-triangular mirrored\n",
        "        e = np.dot(q, x) + 2.0 * np.sum(Q[upper_idx] * (x[upper_idx[0]] * x[upper_idx[1]]))\n",
        "        if e < best_e:\n",
        "            best_e, best_x = e, x\n",
        "    return best_x, float(best_e)\n",
        "\n",
        "# Optional: CUDA-Q runner (only used if your install exposes expected API)\n",
        "def try_cudaq_qaoa(h, J, p=2, shots=2048, max_iters=60, seed=7):\n",
        "    try:\n",
        "        import cudaq  # type: ignore\n",
        "        from cudaq import spin  # type: ignore\n",
        "    except Exception:\n",
        "        return None  # CUDA-Q not available\n",
        "\n",
        "    # Different wheels expose different APIs; we wrap cautiously\n",
        "    K = len(h)\n",
        "    try:\n",
        "        # Build Ising H = h·Z + sum J_ij Z_i Z_j\n",
        "        H = 0.0 * spin.z(0); H = 0.0 * H\n",
        "        for i in range(K):\n",
        "            if abs(h[i]) > 0: H += h[i] * spin.z(i)\n",
        "        for i in range(K):\n",
        "            for j in range(i+1, K):\n",
        "                if abs(J[i, j]) > 0: H += J[i, j] * spin.z(i) * spin.z(j)\n",
        "\n",
        "        # If your wheel has a high-level QAOA interface, try to use it:\n",
        "        try:\n",
        "            from cudaq.algorithms import QAOA  # type: ignore\n",
        "            qaoa = QAOA(H, steps=p)\n",
        "            res = qaoa.minimize()  # may not exist in some builds\n",
        "            # res may return bitstring directly in some builds; we normalize\n",
        "            bitstring = getattr(res, \"bitstring\", None)\n",
        "            if bitstring is None and isinstance(res, dict):\n",
        "                bitstring = res.get(\"bitstring\")\n",
        "            if bitstring:\n",
        "                return {\"best_state\": bitstring, \"counts\": {}}\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Fallback: simple parameter sweep (small) via observe/sample if present\n",
        "        import numpy as np\n",
        "        @cudaq.kernel\n",
        "        def ansatz(params: list[float]):\n",
        "            q = cudaq.qvector(K)\n",
        "            # Some builds don’t expose `cudaq.h`; try ry(π/2) as H-equivalent\n",
        "            for i in range(K):\n",
        "                try:\n",
        "                    cudaq.h(q[i])          # if available\n",
        "                except Exception:\n",
        "                    cudaq.ry(np.pi/2, q[i])\n",
        "            for layer in range(p):\n",
        "                gamma = params[layer]\n",
        "                for i in range(K):\n",
        "                    # phase separation via rz on Z terms\n",
        "                    cudaq.rz(2.0*gamma*h[i], q[i])\n",
        "                for i in range(K):\n",
        "                    for j in range(i+1, K):\n",
        "                        if abs(J[i,j]) > 0:\n",
        "                            cudaq.cx(q[i], q[j]); cudaq.rz(2.0*gamma*J[i,j], q[j]); cudaq.cx(q[i], q[j])\n",
        "                beta = params[p+layer]\n",
        "                for i in range(K):\n",
        "                    cudaq.rx(2.0*beta, q[i])\n",
        "\n",
        "        def energy_from_bitstring(bits):\n",
        "            z = np.array([1 if b=='0' else -1 for b in bits[::-1]])\n",
        "            return float(np.dot(h, z) + sum(J[i,j]*z[i]*z[j] for i in range(K) for j in range(i+1,K)))\n",
        "\n",
        "        # tiny random search since `cudaq.optimize` may not exist\n",
        "        rng = np.random.default_rng(seed)\n",
        "        best_state, best_e = None, np.inf\n",
        "        for _ in range(64):\n",
        "            params = rng.random(2*p).tolist()\n",
        "            try:\n",
        "                counts = cudaq.sample(ansatz, params, shots_count=shots)\n",
        "            except Exception:\n",
        "                return None  # give up gracefully\n",
        "            for bitstring, c in counts.items():\n",
        "                e = energy_from_bitstring(bitstring)\n",
        "                if e < best_e:\n",
        "                    best_e, best_state = e, bitstring\n",
        "        if best_state:\n",
        "            return {\"best_state\": best_state, \"counts\": {}}\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:43:45.393652Z",
          "iopub.execute_input": "2025-08-25T06:43:45.394483Z",
          "iopub.status.idle": "2025-08-25T06:43:45.411933Z",
          "shell.execute_reply.started": "2025-08-25T06:43:45.394449Z",
          "shell.execute_reply": "2025-08-25T06:43:45.411117Z"
        },
        "id": "vdM-dy3ZCAhG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 9c) Build QUBO → run (CUDA-Q if available else exact)\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "panel = example_drug_panel()\n",
        "drugs = list(panel.keys())\n",
        "\n",
        "# pick a patient (first column for demo)\n",
        "sample_id = P.columns[0]\n",
        "z_path = patient_vector(P, sample_id)\n",
        "\n",
        "b_series = drug_benefit_prior(z_path, panel).reindex(drugs).fillna(0.0)\n",
        "R = build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10)\n",
        "\n",
        "b = b_series.to_numpy(float)\n",
        "q, Q = build_qubo(b, R, lam=1.0)\n",
        "\n",
        "# Ising mapping (for CUDA-Q path)\n",
        "h, J, const = qubo_to_ising(q, Q)\n",
        "\n",
        "# Try CUDA-Q first\n",
        "res = try_cudaq_qaoa(h, J, p=2, shots=2048, max_iters=60, seed=7)\n",
        "if res and res.get(\"best_state\"):\n",
        "    bitstring = res[\"best_state\"]\n",
        "    sel = [drugs[i] for i, bit in enumerate(bitstring[::-1]) if bit == '1']\n",
        "    solver_used = \"CUDA-Q QAOA\"\n",
        "else:\n",
        "    # exact QUBO fallback (guaranteed optimum for K≤20)\n",
        "    x_star, e_star = exact_qubo_solve(b, R, lam=1.0)\n",
        "    sel = [drugs[i] for i, xi in enumerate(x_star) if xi == 1]\n",
        "    solver_used = \"Exact QUBO (enumeration)\"\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    \"drug\": drugs,\n",
        "    \"b_hat\": b_series.values,\n",
        "    \"selected\": [d in sel for d in drugs],\n",
        "    \"targets\": [\", \".join(panel[d]) for d in drugs]\n",
        "}).sort_values(\"b_hat\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"Patient:\", sample_id)\n",
        "print(\"Solver:\", solver_used)\n",
        "print(\"Selected therapies:\", sel if sel else \"(none)\")\n",
        "display(summary)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:44:06.768435Z",
          "iopub.execute_input": "2025-08-25T06:44:06.768800Z",
          "iopub.status.idle": "2025-08-25T06:44:06.848687Z",
          "shell.execute_reply.started": "2025-08-25T06:44:06.768775Z",
          "shell.execute_reply": "2025-08-25T06:44:06.847781Z"
        },
        "id": "TZVvTxwCCAhG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Multi-patient + Stability******"
      ],
      "metadata": {
        "id": "mkBkq6vTCAhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔄 Multi-patient QUBO run + stability summary\n",
        "import pandas as pd, numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "panel = example_drug_panel()\n",
        "drugs = list(panel.keys())\n",
        "\n",
        "def run_for_patient(sample_id):\n",
        "    z_path = patient_vector(P, sample_id)\n",
        "    b_series = drug_benefit_prior(z_path, panel).reindex(drugs).fillna(0.0)\n",
        "    R = build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10)\n",
        "    b = b_series.to_numpy(float)\n",
        "    q, Q = build_qubo(b, R, lam=1.0)\n",
        "    # exact solve (stable & fast)\n",
        "    x_star, e_star = exact_qubo_solve(b, R, lam=1.0)\n",
        "    sel = [drugs[i] for i, xi in enumerate(x_star) if xi == 1]\n",
        "    return sel, b_series\n",
        "\n",
        "# --- run over first N patients\n",
        "N = 25\n",
        "all_sels = []\n",
        "all_bhats = []\n",
        "\n",
        "for sid in P.columns[:N]:\n",
        "    sel, b_series = run_for_patient(sid)\n",
        "    all_sels.append(sel)\n",
        "    all_bhats.append(b_series)\n",
        "\n",
        "# --- frequency summary\n",
        "flat = [d for sel in all_sels for d in sel]\n",
        "freq = Counter(flat)\n",
        "freq_df = pd.DataFrame({\"drug\": drugs, \"frequency\": [freq[d] for d in drugs]})\n",
        "freq_df[\"frequency_pct\"] = 100 * freq_df[\"frequency\"] / N\n",
        "freq_df = freq_df.sort_values(\"frequency_pct\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(f\"Ran {N} patients\")\n",
        "display(freq_df)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:44:14.304747Z",
          "iopub.execute_input": "2025-08-25T06:44:14.305759Z",
          "iopub.status.idle": "2025-08-25T06:44:14.456761Z",
          "shell.execute_reply.started": "2025-08-25T06:44:14.305723Z",
          "shell.execute_reply": "2025-08-25T06:44:14.455732Z"
        },
        "id": "6lGiFE7UCAhH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11. Visualization**"
      ],
      "metadata": {
        "id": "geJI8MT9CAhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 📊 Drug selection frequency barplot\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.bar(freq_df[\"drug\"], freq_df[\"frequency_pct\"], color=\"steelblue\")\n",
        "    plt.ylabel(\"% patients selected\")\n",
        "    plt.title(\"Drug selection stability across patients\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"Plot skipped:\", repr(e))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:44:22.421722Z",
          "iopub.execute_input": "2025-08-25T06:44:22.422607Z",
          "iopub.status.idle": "2025-08-25T06:44:22.636729Z",
          "shell.execute_reply.started": "2025-08-25T06:44:22.422575Z",
          "shell.execute_reply": "2025-08-25T06:44:22.635848Z"
        },
        "id": "-uZatMwtCAhI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **12. Hierarchical Clustering: Heatmap, Dendrogram, and Clustered Heatmap**"
      ],
      "metadata": {
        "id": "Bu4biw16CAhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔗 Co-selection analysis (rebuild if needed) + heatmap + dendrogram\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# --- prerequisites: panel, P, and the helper funcs from Section 9a must exist ---\n",
        "# If all_sels/drugs are missing, rebuild them quickly.\n",
        "needs_run = (\"all_sels\" not in globals()) or (\"drugs\" not in globals())\n",
        "if needs_run:\n",
        "    panel = example_drug_panel()\n",
        "    drugs = list(panel.keys())\n",
        "\n",
        "    def run_for_patient(sample_id):\n",
        "        z_path = patient_vector(P, sample_id)\n",
        "        b_series = drug_benefit_prior(z_path, panel).reindex(drugs).fillna(0.0)\n",
        "        R = build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10)\n",
        "        b = b_series.to_numpy(float)\n",
        "        x_star, e_star = exact_qubo_solve(b, R, lam=1.0)\n",
        "        sel = [drugs[i] for i, xi in enumerate(x_star) if xi == 1]\n",
        "        return sel\n",
        "\n",
        "    N = min(25, P.shape[1])\n",
        "    all_sels = [run_for_patient(sid) for sid in P.columns[:N]]\n",
        "\n",
        "# --- build co-selection counts ---\n",
        "co_mat = pd.DataFrame(0, index=drugs, columns=drugs, dtype=int)\n",
        "for sel in all_sels:\n",
        "    # ensure unique set per patient (avoid double counting same drug)\n",
        "    uniq = list(dict.fromkeys(sel))\n",
        "    for i in range(len(uniq)):\n",
        "        for j in range(i, len(uniq)):\n",
        "            di, dj = uniq[i], uniq[j]\n",
        "            co_mat.loc[di, dj] += 1\n",
        "            if i != j:\n",
        "                co_mat.loc[dj, di] += 1\n",
        "\n",
        "# normalize to % of patients\n",
        "n_pat = max(1, len(all_sels))\n",
        "co_pct = co_mat / n_pat * 100.0\n",
        "\n",
        "print(\"Co-selection matrix (% of patients):\")\n",
        "display(co_pct.round(1))\n",
        "\n",
        "# --- heatmap ---\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(7,6))\n",
        "    im = plt.imshow(co_pct.values, cmap=\"Blues\", interpolation=\"nearest\")\n",
        "    plt.xticks(range(len(drugs)), drugs, rotation=45, ha=\"right\")\n",
        "    plt.yticks(range(len(drugs)), drugs)\n",
        "    plt.colorbar(im, label=\"% patients co-selected\")\n",
        "    plt.title(\"Drug co-selection heatmap\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"Heatmap skipped:\", repr(e))\n",
        "\n",
        "# --- dendrogram + clustered heatmap (requires scipy) ---\n",
        "try:\n",
        "    from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
        "\n",
        "    # distance = 1 - correlation on rows\n",
        "    corr = np.corrcoef(co_pct.values)\n",
        "    # safety: numeric issues -> clip to [-1,1]\n",
        "    corr = np.clip(corr, -1.0, 1.0)\n",
        "    dist = 1.0 - corr\n",
        "\n",
        "    Z = linkage(dist, method=\"average\")\n",
        "    # dendrogram\n",
        "    plt.figure(figsize=(8,5))\n",
        "    dendrogram(Z, labels=drugs, leaf_rotation=90, leaf_font_size=10,\n",
        "               color_threshold=0.7 * np.max(Z[:,2]))\n",
        "    plt.title(\"Clustered dendrogram of drug co-selection\")\n",
        "    plt.ylabel(\"Distance (1 - correlation)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # clustered heatmap\n",
        "    order = leaves_list(Z)\n",
        "    co_pct_ordered = co_pct.iloc[order, order]\n",
        "\n",
        "    plt.figure(figsize=(7,6))\n",
        "    im = plt.imshow(co_pct_ordered.values, cmap=\"Blues\", interpolation=\"nearest\")\n",
        "    plt.xticks(range(len(order)), co_pct_ordered.columns, rotation=45, ha=\"right\")\n",
        "    plt.yticks(range(len(order)), co_pct_ordered.index)\n",
        "    plt.colorbar(im, label=\"% patients co-selected\")\n",
        "    plt.title(\"Clustered heatmap of drug co-selection\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    display(co_pct_ordered.round(1))\n",
        "except Exception as e:\n",
        "    print(\"Clustering skipped (scipy missing or version issue):\", repr(e))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-25T06:56:18.390734Z",
          "iopub.execute_input": "2025-08-25T06:56:18.391083Z",
          "iopub.status.idle": "2025-08-25T06:56:19.161702Z",
          "shell.execute_reply.started": "2025-08-25T06:56:18.391060Z",
          "shell.execute_reply": "2025-08-25T06:56:19.160639Z"
        },
        "id": "zWMc_TrHCAhI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **13. Top co-selected pairs (ranked)**"
      ],
      "metadata": {
        "id": "IQmHldmF8dlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 📈 Top co-selected drug pairs (ranked table)\n",
        "import itertools, pandas as pd, numpy as np\n",
        "\n",
        "# rebuild pair counts from all_sels to be safe\n",
        "pair_counts = {}\n",
        "N_pat = max(1, len(all_sels))\n",
        "for sel in all_sels:\n",
        "    uniq = sorted(set(sel))\n",
        "    for (a,b) in itertools.combinations(uniq, 2):\n",
        "        pair_counts[(a,b)] = pair_counts.get((a,b), 0) + 1\n",
        "\n",
        "pairs_df = pd.DataFrame(\n",
        "    [(a,b,c, 100.0*c/N_pat) for (a,b),c in pair_counts.items()],\n",
        "    columns=[\"drug_a\",\"drug_b\",\"count\",\"pct_patients\"]\n",
        ").sort_values([\"pct_patients\",\"count\"], ascending=False, ignore_index=True)\n",
        "\n",
        "print(f\"Pairs observed: {len(pairs_df)}  (N patients={N_pat})\")\n",
        "display(pairs_df.head(15))\n"
      ],
      "metadata": {
        "id": "MucaIEOi80S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **14. Patient-level report (what each patient got + pathway context)**"
      ],
      "metadata": {
        "id": "5_RQC4xH8dbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧾 Patient-level report (selected drugs + pathway context)\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "panel = example_drug_panel()\n",
        "drugs = list(panel.keys())\n",
        "\n",
        "def run_patient_once(sample_id):\n",
        "    z_path = patient_vector(P, sample_id)\n",
        "    b_series = drug_benefit_prior(z_path, panel).reindex(drugs).fillna(0.0)\n",
        "    R = build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10)\n",
        "    x_star, e_star = exact_qubo_solve(b_series.to_numpy(float), R, lam=1.0)\n",
        "    sel = [drugs[i] for i, xi in enumerate(x_star) if xi == 1]\n",
        "    return sel, b_series, z_path\n",
        "\n",
        "rows = []\n",
        "for sid in P.columns[:len(all_sels)]:  # align with prior run size\n",
        "    sel, b_hat, z_path = run_patient_once(sid)\n",
        "    # top pathways (absolute z) for context\n",
        "    top_pw = z_path.abs().sort_values(ascending=False).head(5)\n",
        "    rows.append({\n",
        "        \"patient\": sid,\n",
        "        \"selected_drugs\": \", \".join(sel) if sel else \"(none)\",\n",
        "        \"top_pathways\": \"; \".join([f\"{p}:{z_path[p]:+.2f}\" for p in top_pw.index]),\n",
        "        **{f\"b̂.{d}\": float(b_hat.get(d,0.0)) for d in drugs}\n",
        "    })\n",
        "\n",
        "patient_report = pd.DataFrame(rows)\n",
        "display(patient_report.head(10))\n",
        "\n",
        "# optional export\n",
        "out_csv = \"/content/patient_report.csv\"\n",
        "patient_report.to_csv(out_csv, index=False)\n",
        "print(\"Saved:\", out_csv)\n"
      ],
      "metadata": {
        "id": "prmxl_cv865l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **15. Selection stability summary (frequencies, co-selection, size distribution)**"
      ],
      "metadata": {
        "id": "hHw6o3i_8dRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧮 Stability summary (freqs + selection size histogram)\n",
        "import pandas as pd, numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# frequencies (reuse or recompute)\n",
        "flat = [d for sel in all_sels for d in sel]\n",
        "freq = Counter(flat)\n",
        "freq_df = pd.DataFrame({\"drug\": drugs, \"freq\": [freq[d] for d in drugs]})\n",
        "freq_df[\"pct\"] = 100.0 * freq_df[\"freq\"] / max(1,len(all_sels))\n",
        "freq_df = freq_df.sort_values(\"pct\", ascending=False).reset_index(drop=True)\n",
        "display(freq_df)\n",
        "\n",
        "# selection size distribution\n",
        "sizes = [len(set(sel)) for sel in all_sels]\n",
        "size_hist = pd.Series(sizes).value_counts().sort_index()\n",
        "print(\"Selection size distribution (unique drugs per patient):\")\n",
        "display(pd.DataFrame({\"k_drugs\": size_hist.index, \"n_patients\": size_hist.values}))\n",
        "\n",
        "# quick plots (skip if matplotlib missing)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.bar(freq_df[\"drug\"], freq_df[\"pct\"])\n",
        "    plt.ylabel(\"% patients selected\")\n",
        "    plt.title(\"Drug selection frequency\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.bar(size_hist.index, size_hist.values)\n",
        "    plt.xlabel(\"# drugs selected\")\n",
        "    plt.ylabel(\"# patients\")\n",
        "    plt.title(\"Selection size distribution\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "except Exception as e:\n",
        "    print(\"Plots skipped:\", repr(e))\n"
      ],
      "metadata": {
        "id": "6BwXd4gT8904"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **16. Code-cell: Synthetic Classification Validation**\n",
        "\n",
        "\n",
        "The classification-style validation on top of the survival proxy has been added.\n",
        "Because the notebook does not have a ground-truth patient response labels (as of the moment of the analysis),  we simulated binary risk classes (high vs low) from pathway signals, then test whether drug selections can predict them."
      ],
      "metadata": {
        "id": "RRT0_-V58dCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧪 Classification validation (synthetic labels → ROC/PR metrics)\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "panel = example_drug_panel()\n",
        "drugs = list(panel.keys())\n",
        "\n",
        "# 1) Construct patient-drug feature matrix\n",
        "N = len(all_sels)\n",
        "X = np.zeros((N, len(drugs)))\n",
        "for i, sel in enumerate(all_sels):\n",
        "    for d in sel:\n",
        "        if d in drugs:\n",
        "            X[i, drugs.index(d)] = 1\n",
        "\n",
        "# 2) Synthetic binary labels\n",
        "# Risk score = mean pathway z → label = high (1) if above median\n",
        "risk_scores = []\n",
        "for sid in P.columns[:N]:\n",
        "    z_path = patient_vector(P, sid)\n",
        "    risk_scores.append(z_path.mean())\n",
        "risk_scores = np.array(risk_scores)\n",
        "y = (risk_scores > np.median(risk_scores)).astype(int)\n",
        "\n",
        "print(\"Synthetic labels (0=low risk, 1=high risk):\")\n",
        "print(np.bincount(y))\n",
        "\n",
        "# 3) Train simple logistic regression\n",
        "clf = LogisticRegression(max_iter=200)\n",
        "clf.fit(X, y)\n",
        "y_pred = clf.predict_proba(X)[:,1]\n",
        "\n",
        "# 4) Metrics\n",
        "roc_auc = roc_auc_score(y, y_pred)\n",
        "pr_auc = average_precision_score(y, y_pred)\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "print(f\"PR-AUC: {pr_auc:.3f}\")\n",
        "\n",
        "# Optional barplot of learned coefficients\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    coefs = pd.Series(clf.coef_[0], index=drugs).sort_values()\n",
        "    plt.figure(figsize=(7,4))\n",
        "    coefs.plot(kind=\"barh\", color=[\"steelblue\" if v>0 else \"salmon\" for v in coefs])\n",
        "    plt.title(\"Drug selection coefficients (synthetic label prediction)\")\n",
        "    plt.xlabel(\"Weight\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"Plot skipped:\", repr(e))\n"
      ],
      "metadata": {
        "id": "akTiBZvq-st5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7beae58a"
      },
      "source": [
        "# **Summary:**\n",
        "\n",
        "This notebook presented a comprehensive workflow demonstrating the application of the \"Medical-Patient Digital Twin\" concept to personalized cancer therapy selection. Starting with raw gene expression data, we showed how to build a computational representation of a patient's biological state, specifically focusing on key signaling pathways.\n",
        "\n",
        "The core of the approach involved formulating the drug selection problem as a Quadratic Unconstrained Binary Optimization (QUBO) problem, balancing the potential therapeutic benefit (derived from pathway activity scores) with penalties for overlapping drug mechanisms. We explored solving this optimization problem using both an exact classical method and an optional quantum Approximate Optimization Algorithm (QAOA) using CUDA-Q, highlighting the potential for leveraging advanced computing paradigms.\n",
        "\n",
        "Beyond individual patient selection, the notebook included analysis of drug selection patterns across a cohort of patients, examining the frequency and co-selection of different therapies. We also generated a patient-level report to summarize individual therapy recommendations and their pathway context. Finally, a synthetic classification validation was performed to illustrate how the features derived from the selected drug profiles could potentially be used for predictive modeling, although the inherent limitations of this synthetic approach were emphasized.\n",
        "\n",
        "In essence, this notebook serves as a proof-of-concept, illustrating how integrating biological data, pathway analysis, and optimization techniques within a digital twin framework can provide a powerful computational tool to inform personalized cancer therapy. While acknowledging the necessary simplifications and the critical need for rigorous clinical validation, this work demonstrates the potential of such computational approaches to contribute to the future of precision medicine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6d80eaf"
      },
      "source": [
        "# Task\n",
        "Convert the provided Jupyter notebook into a web application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d3ea853"
      },
      "source": [
        "## Identify core logic\n",
        "\n",
        "### Subtask:\n",
        "Extract the key Python functions and classes from the notebook that perform the data processing, pathway scoring, QUBO formulation, and drug selection.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e67c2976"
      },
      "source": [
        "## Choose a web framework\n",
        "\n",
        "### Subtask:\n",
        "Select a suitable Python web framework (e.g., Flask, Django, or Streamlit) to build the web application. Streamlit is often a good choice for data science applications due to its ease of use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeda55e0"
      },
      "source": [
        "## Create a user interface\n",
        "\n",
        "### Subtask:\n",
        "Design and implement the front-end of the web application using the chosen framework (Streamlit). This will involve creating input fields for uploading gene expression data, selecting parameters (e.g., number of samples, lambda value), and displaying the results (e.g., selected drugs, pathway scores, visualizations).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "135198d0"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new Streamlit script `app.py` and add the basic structure including title, description, file uploader, parameter inputs for N_SAMPLES and lambda, and placeholders for results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTq03JHD1kh2"
      },
      "source": [
        "%pip install streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2857ffae"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that Streamlit is installed, recreate the Streamlit application script with the basic UI elements as described in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12d3997f"
      },
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Assume the core logic functions from the notebook are available,\n",
        "# either by importing them from a separate file or including them here.\n",
        "# For this step, we only set up the UI elements.\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "st.title(\"Digital Twin Workflow: Gene Expression to Drug Selection\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "This application demonstrates a workflow for identifying potential drug therapies\n",
        "for cancer patients based on their gene expression data, using pathway analysis\n",
        "and QUBO optimization.\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.header(\"Input Data\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload gene expression data (CSV or TSV)\", type=[\"csv\", \"tsv\", \"gz\"])\n",
        "\n",
        "st.sidebar.header(\"Parameters\")\n",
        "n_samples = st.sidebar.slider(\"Number of samples to process\", min_value=10, max_value=100, value=40, step=10)\n",
        "lam_value = st.sidebar.slider(\"Lambda (penalty) value for QUBO\", min_value=0.1, max_value=5.0, value=1.0, step=0.1)\n",
        "\n",
        "st.header(\"Analysis Results\")\n",
        "\n",
        "# Placeholders for results\n",
        "results_placeholder = st.empty()\n",
        "pathway_scores_placeholder = st.empty()\n",
        "selected_drugs_placeholder = st.empty()\n",
        "visualizations_placeholder = st.empty()\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    try:\n",
        "        # In a real app, you would load and process the data here\n",
        "        # For now, just acknowledge the upload\n",
        "        st.sidebar.success(\"File uploaded successfully!\")\n",
        "        st.write(\"Click 'Run Analysis' to process the data and get results.\")\n",
        "\n",
        "        # Add a button to trigger analysis\n",
        "        if st.button(\"Run Analysis\"):\n",
        "            # This is where the data processing and analysis functions would be called\n",
        "            # For this step, we just show a message\n",
        "            with st.spinner(\"Running analysis...\"):\n",
        "                # Simulate analysis time\n",
        "                import time\n",
        "                time.sleep(3)\n",
        "                results_placeholder.write(\"Analysis complete! Results will appear below.\")\n",
        "                # In a real app, populate the placeholders with actual results\n",
        "                # pathway_scores_placeholder.dataframe(...)\n",
        "                # selected_drugs_placeholder.write(...)\n",
        "                # visualizations_placeholder.pyplot(...)\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading or processing file: {e}\")\n",
        "else:\n",
        "    st.info(\"Please upload a gene expression data file to begin.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a5fcbd0"
      },
      "source": [
        "## Connect backend logic\n",
        "\n",
        "### Subtask:\n",
        "Integrate the extracted Python functions into the web framework (Streamlit) to handle user requests. This will involve setting up routes or endpoints to receive data from the front-end, process it using the core logic, and return the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "755d5d1f"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function `run_analysis` that encapsulates the core workflow, including data loading, processing, pathway scoring, QUBO formulation, and drug selection. This function will take the uploaded file, number of samples, and lambda value as input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72b8ef7d"
      },
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip\n",
        "import re\n",
        "from collections import Counter\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import io\n",
        "\n",
        "# Assume the core logic functions from the notebook are available here.\n",
        "# For demonstration, I'll include them directly. In a larger project,\n",
        "# these would be in a separate module.\n",
        "\n",
        "def norm_ensembl(x): return x.split('.',1)[0]\n",
        "def norm_symbol(x):  return re.sub(r'[^A-Za-z0-9_-]+','', x)\n",
        "\n",
        "def detect_row_mode(file_object, scan_rows=50000):\n",
        "    \"\"\"Detect row ID type (Ensembl vs symbol) from file-like object.\"\"\"\n",
        "    seen = Counter(); total = 0\n",
        "    # Use a TextIOWrapper to treat the bytes stream as text\n",
        "    with io.TextIOWrapper(file_object, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        # Read the header line to skip it for row detection\n",
        "        header = f.readline()\n",
        "        reader = pd.read_csv(f, sep=\"\\t\", chunksize=200_000, usecols=[0], dtype=str, header=None) # header=None because we already read it\n",
        "        for ch in reader:\n",
        "            v = ch.iloc[:,0].astype(str)\n",
        "            vals = v.head(min(len(v), scan_rows-total)).tolist()\n",
        "            total += len(vals)\n",
        "            seen.update('ENSG' if x.startswith('ENSG') else 'OTHER' for x in vals)\n",
        "            if total >= scan_rows: break\n",
        "    ratio = seen['ENSG']/max(1,(seen['ENSG']+seen['OTHER']))\n",
        "    mode = 'ensembl' if ratio >= 0.6 else 'symbol'\n",
        "    return mode\n",
        "\n",
        "def stream_select_rows_columns(file_object, selected_samples, sig_genes, sym2ensg, row_mode, gene_col_name=\"Name\"):\n",
        "    \"\"\"Stream-select rows and columns from file-like object.\"\"\"\n",
        "    if row_mode == 'ensembl':\n",
        "        target_rows = set(sym2ensg.get(g) for g in sig_genes if g in sym2ensg)\n",
        "        normalize = norm_ensembl\n",
        "    else:\n",
        "        target_rows = set(sig_genes)\n",
        "        normalize = norm_symbol\n",
        "\n",
        "    usecols_filter = [gene_col_name] + selected_samples\n",
        "    kept = []\n",
        "\n",
        "    file_object.seek(0) # Reset file pointer to the beginning\n",
        "\n",
        "    with io.TextIOWrapper(file_object, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        # Read header to find the actual gene_col_name if it's not \"Name\"\n",
        "        header = f.readline().rstrip(\"\\n\").split(\"\\t\")\n",
        "        try:\n",
        "            gene_col_idx = header.index(gene_col_name)\n",
        "        except ValueError:\n",
        "             # If \"Name\" not found, assume the first column is the gene column\n",
        "            gene_col_name = header[0]\n",
        "            gene_col_idx = 0\n",
        "\n",
        "        reader = pd.read_csv(f, sep=\"\\t\", chunksize=50_000, dtype=str, header=None) # header=None because we already read it\n",
        "        for i, ch in enumerate(reader):\n",
        "            # Assign correct column names based on header\n",
        "            if i == 0:\n",
        "                 ch.columns = header\n",
        "            else:\n",
        "                 # For subsequent chunks, pandas might not automatically use the header,\n",
        "                 # so we need to ensure we are selecting the correct columns by index\n",
        "                 ch.columns = header # Assign header to the chunk\n",
        "                 ch = ch[usecols_filter] # Select only necessary columns\n",
        "\n",
        "            ch = ch.rename(columns={gene_col_name: \"row_id\"})\n",
        "            # Ensure 'row_id' column exists after renaming\n",
        "            if 'row_id' not in ch.columns:\n",
        "                 st.error(f\"Error: Could not find gene identifier column '{gene_col_name}' in the uploaded file.\")\n",
        "                 return pd.DataFrame() # Return empty DataFrame on error\n",
        "\n",
        "            ids = ch[\"row_id\"].astype(str).map(normalize)\n",
        "            mask = ids.isin(target_rows)\n",
        "\n",
        "            if mask.any():\n",
        "                out = ch.loc[mask].copy()\n",
        "                out[\"row_id\"] = out[\"row_id\"].map(normalize)\n",
        "                kept.append(out)\n",
        "\n",
        "    if not kept:\n",
        "        st.warning(\"No signature rows matched the provided gene list.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    expr_small = pd.concat(kept, axis=0, ignore_index=False).drop_duplicates(subset=[\"row_id\"]).set_index(\"row_id\")\n",
        "\n",
        "    if row_mode == 'ensembl':\n",
        "        ensg2sym = {v:k for k,v in sym2ensg.items()}\n",
        "        expr_sym_small = expr_small.copy()\n",
        "        expr_sym_small.index = [ensg2sym.get(e, e) for e in expr_small.index]\n",
        "    else:\n",
        "        expr_sym_small = expr_small.copy()\n",
        "\n",
        "    expr_sym_small = expr_sym_small[~expr_sym_small.index.duplicated(keep=\"first\")]\n",
        "    return expr_sym_small\n",
        "\n",
        "\n",
        "def zscore_by_gene(expr_symbols: pd.DataFrame) -> pd.DataFrame:\n",
        "    E = expr_symbols.copy()\n",
        "    E = E.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    E = E.loc[~E.isna().all(axis=1)]\n",
        "    gene_means = E.mean(axis=1)\n",
        "    E = E.apply(lambda col: col.fillna(gene_means), axis=0)\n",
        "    mu = E.mean(axis=1)\n",
        "    sd = E.std(axis=1) + 1e-8\n",
        "    return (E.sub(mu, axis=0)).div(sd, axis=0)\n",
        "\n",
        "def pathway_scores(expr_symbols: pd.DataFrame, signatures: dict) -> pd.DataFrame:\n",
        "    \"\"\"Return pathways x samples (mean z across member genes present).\"\"\"\n",
        "    Z = zscore_by_gene(expr_symbols)\n",
        "    rows = []\n",
        "    for pw, genes in signatures.items():\n",
        "        present = [g for g in genes if g in Z.index]\n",
        "        if present:\n",
        "            s = Z.loc[present].mean(axis=0)\n",
        "        else:\n",
        "            s = pd.Series([np.nan]*Z.shape[1], index=Z.columns)\n",
        "        s.name = pw\n",
        "        rows.append(s)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def example_drug_panel():\n",
        "    return {\n",
        "        \"EGFRi\": [\"REACTOME_SIGNALING_BY_EGFR\"],\n",
        "        \"ALKi\":  [\"REACTOME_SIGNALING_BY_ALK\"],\n",
        "        \"MEKi\":  [\"REACTOME_MAPK1_MAPK3_SIGNALING\"],\n",
        "        \"PI3Ki\": [\"REACTOME_PI3K_AKT_SIGNALING\"],\n",
        "        \"mTORi\": [\"REACTOME_MTORC1_MEDIATED_SIGNALLING\"],\n",
        "        \"PD1i\":  [\"REACTOME_PD1_SIGNALING\"],\n",
        "        \"VEGFi\": [\"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\"],\n",
        "        \"FGFRi\": [\"REACTOME_SIGNALING_BY_FGFR\"],\n",
        "    }\n",
        "\n",
        "def patient_vector(P: pd.DataFrame, sample_id: str) -> pd.Series:\n",
        "    \"\"\"z-normalize pathways across samples; return vector for one patient.\"\"\"\n",
        "    z = (P - P.mean(axis=1).values.reshape(-1,1)) / (P.std(axis=1).values.reshape(-1,1) + 1e-8)\n",
        "    return z[sample_id].fillna(0.0)\n",
        "\n",
        "def drug_benefit_prior(z_path: pd.Series, panel: dict) -> pd.Series:\n",
        "    \"\"\"Aggregate pathway z's per drug (ReLU to emphasize upregulated pathways).\"\"\"\n",
        "    s = pd.Series({d: float(np.sum([max(z_path.get(p, 0.0), 0.0) for p in pws])) for d, pws in panel.items()})\n",
        "    return s / s.max() if s.max() > 0 else s\n",
        "\n",
        "def build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10):\n",
        "    \"\"\"Pairwise penalties for overlapping mechanisms; diagonal = sparsity.\"\"\"\n",
        "    K = len(drugs); R = np.zeros((K, K), dtype=float)\n",
        "    for i in range(K):\n",
        "        for j in range(i+1, K):\n",
        "            overlap = len(set(panel[drugs[i]]) & set(panel[drugs[j]]))\n",
        "            if overlap > 0:\n",
        "                R[i, j] = R[j, i] = base_overlap * overlap\n",
        "    for i in range(K):\n",
        "        R[i, i] += sparsity\n",
        "    return R\n",
        "\n",
        "def build_qubo(b_hat, R, lam=1.0):\n",
        "    \"\"\"QUBO: minimize x^T (lam R) x + q^T x  where q = -b̂ + diag(lam R).\"\"\"\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy()\n",
        "    diag = np.diag(Q).copy()\n",
        "    np.fill_diagonal(Q, 0.0)  # keep off-diagonal in Q\n",
        "    q += diag\n",
        "    return q, np.triu(Q, 1)\n",
        "\n",
        "def exact_qubo_solve(b_hat: np.ndarray, R: np.ndarray, lam: float = 1.0):\n",
        "    \"\"\"\n",
        "    Exact minimization of QUBO:\n",
        "      E(x) = x^T (lam R) x + q^T x, with q = -b̂ + diag(lam R)\n",
        "    We enumerate all bitstrings (2^K). Returns best bitstring (as 0/1 np array).\n",
        "    \"\"\"\n",
        "    K = len(b_hat)\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy() + np.diag(Q)\n",
        "    np.fill_diagonal(Q, 0.0)  # keep only off-diagonal in Q\n",
        "\n",
        "    best_e = np.inf\n",
        "    best_x = None\n",
        "    # vectorize partial precomputations\n",
        "    upper_idx = np.triu_indices(K, 1)\n",
        "    for mask in range(1 << K):\n",
        "        # build x from bits\n",
        "        x = np.fromiter(((mask >> i) & 1 for i in range(K)), dtype=np.int8)\n",
        "        # E = x^T Q x + q^T x, where Q is strictly upper-triangular mirrored\n",
        "        e = np.dot(q, x) + 2.0 * np.sum(Q[upper_idx] * (x[upper_idx[0]] * x[upper_idx[1]]))\n",
        "        if e < best_e:\n",
        "            best_e, best_x = e, x\n",
        "    return best_x, float(best_e)\n",
        "\n",
        "\n",
        "# Placeholder for CUDA-Q function if needed later, currently uses exact solve\n",
        "def try_cudaq_qaoa(h, J, p=2, shots=2048, max_iters=60, seed=7):\n",
        "    return None # Not implemented in this web app version\n",
        "\n",
        "# --- Main analysis function ---\n",
        "def run_analysis(uploaded_file, n_samples, lam_value):\n",
        "    \"\"\"\n",
        "    Runs the full analysis workflow for the uploaded data.\n",
        "    Returns pathway scores, drug selection summary, and patient report.\n",
        "    \"\"\"\n",
        "    SIGS = {\n",
        "        \"REACTOME_SIGNALING_BY_EGFR\": [\n",
        "            \"EGFR\",\"ERBB2\",\"ERBB3\",\"GRB2\",\"SOS1\",\"SHC1\",\"PTPN11\",\"KRAS\",\"NRAS\",\"HRAS\",\n",
        "            \"BRAF\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"GAB1\"\n",
        "        ],\n",
        "        \"REACTOME_SIGNALING_BY_ALK\": [\n",
        "            \"ALK\",\"EML4\",\"GRB2\",\"SHC1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"STAT3\",\"MAP2K1\",\"MAPK1\",\"MAPK3\"\n",
        "        ],\n",
        "        \"REACTOME_MAPK1_MAPK3_SIGNALING\": [\n",
        "            \"BRAF\",\"RAF1\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"DUSP6\",\"DUSP4\",\"FOS\",\"JUN\",\"EGFR\"\n",
        "        ],\n",
        "        \"REACTOME_PI3K_AKT_SIGNALING\": [\n",
        "            \"PIK3CA\",\"PIK3CB\",\"PIK3CD\",\"PIK3R1\",\"PIK3R2\",\"AKT1\",\"AKT2\",\"AKT3\",\"PTEN\",\"MTOR\",\"RHEB\"\n",
        "        ],\n",
        "        \"REACTOME_MTORC1_MEDIATED_SIGNALLING\": [\n",
        "            \"MTOR\",\"RPTOR\",\"MLST8\",\"RHEB\",\"TSC1\",\"TSC2\",\"EIF4EBP1\",\"RPS6KB1\",\"RPS6\"\n",
        "        ],\n",
        "        \"REACTOME_PD1_SIGNALING\": [\n",
        "            \"PDCD1\",\"CD274\",\"PDCD1LG2\",\"PDCD1LG2\",\"JAK1\",\"JAK2\",\"STAT1\",\"IFNG\",\"GZMB\",\"LAG3\",\"TIGIT\",\"CXCL9\",\"CXCL10\"\n",
        "        ],\n",
        "        \"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\": [\n",
        "            \"VEGFA\",\"KDR\",\"FLT1\",\"FLT4\",\"PTPRB\",\"PLCG1\",\"MAP2K1\",\"MAPK1\",\"NOS3\"\n",
        "        ],\n",
        "        \"REACTOME_SIGNALING_BY_FGFR\": [\n",
        "            \"FGFR1\",\"FGFR2\",\"FGFR3\",\"FGFR4\",\"FRS2\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"MAP2K1\",\"MAPK1\"\n",
        "        ]\n",
        "    }\n",
        "    SIG_GENES = sorted({g for gs in SIGS.values() for g in gs})\n",
        "    SYM2ENSG = {\n",
        "        \"EGFR\":\"ENSG00000146648\",\"ERBB2\":\"ENSG00000141736\",\"ERBB3\":\"ENSG00000065361\",\"GRB2\":\"ENSG00000177885\",\n",
        "        \"SOS1\":\"ENSG00000115904\",\"SHC1\":\"ENSG00000154639\",\"PTPN11\":\"ENSG00000179295\",\"KRAS\":\"ENSG00000133703\",\n",
        "        \"NRAS\":\"ENSG00000213281\",\"HRAS\":\"ENSG00000174775\",\"BRAF\":\"ENSG00000157764\",\"MAP2K1\":\"ENSG00000169032\",\n",
        "        \"MAP2K2\":\"ENSG00000126934\",\"MAPK1\":\"ENSG00000100030\",\"MAPK3\":\"ENSG00000102882\",\"PLCG1\":\"ENSG00000124181\",\n",
        "        \"PIK3CA\":\"ENSG00000121879\",\"PIK3R1\":\"ENSG00000145675\",\"AKT1\":\"ENSG00000142208\",\"AKT2\":\"ENSG00000105221\",\n",
        "        \"AKT3\":\"ENSG00000117020\",\"GAB1\":\"ENSG00000117676\",\n",
        "        \"ALK\":\"ENSG00000171094\",\"EML4\":\"ENSG00000143924\",\"STAT3\":\"ENSG00000168610\",\n",
        "        \"DUSP6\":\"ENSG00000139318\",\"DUSP4\":\"ENSG00000120875\",\"FOS\":\"ENSG00000170345\",\"JUN\":\"ENSG00000177606\",\"RAF1\":\"ENSG00000132155\",\n",
        "        \"PIK3CB\":\"ENSG00000119402\",\"PIK3CD\":\"ENSG00000171608\",\"PIK3R2\":\"ENSG00000189403\",\"PTEN\":\"ENSG00000171862\",\n",
        "        \"MTOR\":\"ENSG00000198793\",\"RHEB\":\"ENSG00000106615\",\n",
        "        \"RPTOR\":\"ENSG00000141564\",\"MLST8\":\"ENSG00000105705\",\"TSC1\":\"ENSG00000165699\",\"TSC2\":\"ENSG00000103197\",\n",
        "        \"EIF4EBP1\":\"ENSG00000187840\",\"RPS6KB1\":\"ENSG00000108443\",\"RPS6\":\"ENSG00000137154\",\n",
        "        \"PDCD1\":\"ENSG00000276977\",\"CD274\":\"ENSG00000120217\",\"PDCD1LG2\":\"ENSG00000197646\",\"JAK1\":\"ENSG00000162434\",\n",
        "        \"JAK2\":\"ENSG00000096968\",\"STAT1\":\"ENSG00000115415\",\"IFNG\":\"ENSG00000111537\",\"GZMB\":\"ENSG00000100453\",\n",
        "        \"LAG3\":\"ENSG00000089692\",\"TIGIT\":\"ENSG00000181847\",\"CXCL9\":\"ENSG00000138755\",\"CXCL10\":\"ENSG00000169245\",\n",
        "        \"VEGFA\":\"ENSG00000112715\",\"KDR\":\"ENSG00000128052\",\"FLT1\":\"ENSG00000102755\",\"FLT4\":\"ENSG00000037280\",\n",
        "        \"PTPRB\":\"ENSG00000160593\",\"NOS3\":\"ENSG00000164867\",\n",
        "        \"FGFR1\":\"ENSG00000077782\",\"FGFR2\":\"ENSG00000066468\",\"FGFR3\":\"ENSG00000068078\",\"FGFR4\":\"ENSG00000069535\",\n",
        "        \"FRS2\":\"ENSG00000181873\"\n",
        "    }\n",
        "\n",
        "\n",
        "    # Data Loading and Initial Processing\n",
        "    # Handle potential gzip compression\n",
        "    if uploaded_file.name.endswith('.gz'):\n",
        "        gz_file = gzip.GzipFile(fileobj=uploaded_file)\n",
        "        # Need to read into memory or save to a temp file to use seek(0)\n",
        "        # Reading into memory for simplicity here, but be mindful of large files\n",
        "        file_content = io.BytesIO(gz_file.read())\n",
        "    else:\n",
        "        file_content = io.BytesIO(uploaded_file.getvalue())\n",
        "\n",
        "\n",
        "    # Detect row mode\n",
        "    row_mode = detect_row_mode(file_content)\n",
        "    st.write(f\"Detected row mode: {row_mode.upper()}\")\n",
        "\n",
        "    # Get header to identify sample columns and gene column\n",
        "    file_content.seek(0)\n",
        "    header_line = io.TextIOWrapper(file_content, encoding=\"utf-8\", errors=\"replace\").readline().rstrip(\"\\n\")\n",
        "    cols = header_line.split(\"\\t\")\n",
        "    # Assuming the first column is the gene ID/symbol column\n",
        "    gene_col = cols[0]\n",
        "    # Assuming sample columns start from the second column\n",
        "    sample_cols_full = cols[1:]\n",
        "\n",
        "    # Select samples based on n_samples parameter\n",
        "    if n_samples > len(sample_cols_full):\n",
        "        st.warning(f\"Requested {n_samples} samples, but only {len(sample_cols_full)} available. Using all available samples.\")\n",
        "        selected_samples = sample_cols_full\n",
        "    else:\n",
        "        selected_samples = sample_cols_full[:n_samples]\n",
        "\n",
        "    st.write(f\"Processing {len(selected_samples)} samples.\")\n",
        "    st.write(f\"Using gene column: {gene_col}\")\n",
        "\n",
        "    # Stream-select rows and columns\n",
        "    file_content.seek(0) # Reset file pointer before passing to stream_select\n",
        "    expr_sym_small = stream_select_rows_columns(file_content, selected_samples, SIG_GENES, SYM2ENSG, row_mode, gene_col_name=gene_col)\n",
        "\n",
        "    if expr_sym_small.empty:\n",
        "        return None, None, None\n",
        "\n",
        "    st.write(\"Expression matrix shape (genes x samples):\", expr_sym_small.shape)\n",
        "    st.dataframe(expr_sym_small.head())\n",
        "\n",
        "    # Pathway Scoring\n",
        "    st.subheader(\"Pathway Activity Scores\")\n",
        "    P = pathway_scores(expr_sym_small, SIGS)\n",
        "    st.write(\"Pathway scores matrix shape (pathways x samples):\", P.shape)\n",
        "    st.dataframe(P)\n",
        "\n",
        "    # QUBO Formulation and Drug Selection\n",
        "    st.subheader(\"Drug Selection (QUBO)\")\n",
        "    panel = example_drug_panel()\n",
        "    drugs = list(panel.keys())\n",
        "\n",
        "    all_sels = []\n",
        "    all_bhats = []\n",
        "    patient_reports_data = []\n",
        "\n",
        "    # Run analysis for each selected sample\n",
        "    for sample_id in P.columns:\n",
        "        z_path = patient_vector(P, sample_id)\n",
        "        b_series = drug_benefit_prior(z_path, panel).reindex(drugs).fillna(0.0)\n",
        "        R = build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10)\n",
        "        b = b_series.to_numpy(float)\n",
        "        q, Q = build_qubo(b, R, lam=lam_value)\n",
        "\n",
        "        # Use exact solver for now\n",
        "        x_star, e_star = exact_qubo_solve(b, R, lam=lam_value)\n",
        "        sel = [drugs[i] for i, xi in enumerate(x_star) if xi == 1]\n",
        "        all_sels.append(sel)\n",
        "        all_bhats.append(b_series)\n",
        "\n",
        "        # Prepare data for patient report\n",
        "        top_pw = z_path.abs().sort_values(ascending=False).head(5)\n",
        "        patient_reports_data.append({\n",
        "            \"patient\": sample_id,\n",
        "            \"selected_drugs\": \", \".join(sel) if sel else \"(none)\",\n",
        "            \"top_pathways\": \"; \".join([f\"{p}:{z_path[p]:+.2f}\" for p in top_pw.index]),\n",
        "            **{f\"b̂.{d}\": float(b_series.get(d,0.0)) for d in drugs}\n",
        "        })\n",
        "\n",
        "    # Drug Selection Summary\n",
        "    st.subheader(\"Drug Selection Summary\")\n",
        "    flat = [d for sel in all_sels for d in sel]\n",
        "    freq = Counter(flat)\n",
        "    freq_df = pd.DataFrame({\"drug\": drugs, \"frequency\": [freq[d] for d in drugs]})\n",
        "    freq_df[\"frequency_pct\"] = 100 * freq_df[\"frequency\"] / len(selected_samples)\n",
        "    freq_df = freq_df.sort_values(\"frequency_pct\", ascending=False).reset_index(drop=True)\n",
        "    st.dataframe(freq_df)\n",
        "\n",
        "    # Patient-level Report\n",
        "    st.subheader(\"Patient-level Report\")\n",
        "    patient_report_df = pd.DataFrame(patient_reports_data)\n",
        "    st.dataframe(patient_report_df)\n",
        "\n",
        "    return P, freq_df, patient_report_df, all_sels, drugs, panel\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "st.title(\"Digital Twin Workflow: Gene Expression to Drug Selection\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "This application demonstrates a workflow for identifying potential drug therapies\n",
        "for cancer patients based on their gene expression data, using pathway analysis\n",
        "and QUBO optimization.\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.header(\"Input Data\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload gene expression data (CSV or TSV, can be gzipped)\", type=[\"csv\", \"tsv\", \"gz\"])\n",
        "\n",
        "st.sidebar.header(\"Parameters\")\n",
        "n_samples = st.sidebar.slider(\"Number of samples to process\", min_value=1, max_value=100, value=40, step=1)\n",
        "lam_value = st.sidebar.slider(\"Lambda (penalty) value for QUBO\", min_value=0.1, max_value=5.0, value=1.0, step=0.1)\n",
        "\n",
        "st.header(\"Analysis Results\")\n",
        "\n",
        "# Placeholders for results\n",
        "# results_placeholder = st.empty() # Not strictly needed with direct writes\n",
        "# pathway_scores_placeholder = st.empty() # Displayed directly in run_analysis\n",
        "# selected_drugs_placeholder = st.empty() # Displayed directly in run_analysis\n",
        "visualizations_placeholder = st.empty()\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Reset file pointer after file_uploader reads it initially\n",
        "    uploaded_file.seek(0)\n",
        "    file_extension = uploaded_file.name.split('.')[-1]\n",
        "    if file_extension == 'gz':\n",
        "         # Check the extension before .gz\n",
        "         inner_extension = uploaded_file.name.split('.')[-2]\n",
        "         sep = ',' if inner_extension == 'csv' else '\\t'\n",
        "    else:\n",
        "        sep = ',' if file_extension == 'csv' else '\\t'\n",
        "\n",
        "    st.sidebar.success(\"File uploaded successfully!\")\n",
        "    st.write(\"Click 'Run Analysis' to process the data and get results.\")\n",
        "\n",
        "    if st.button(\"Run Analysis\"):\n",
        "        with st.spinner(f\"Running analysis for {n_samples} samples with lambda={lam_value}...\"):\n",
        "            pathway_scores_df, freq_df, patient_report_df, all_sels, drugs, panel = run_analysis(uploaded_file, n_samples, lam_value)\n",
        "\n",
        "            if pathway_scores_df is not None:\n",
        "                visualizations_placeholder.subheader(\"Visualizations\")\n",
        "\n",
        "                # Drug selection frequency barplot\n",
        "                try:\n",
        "                    fig1, ax1 = plt.subplots(figsize=(7,5))\n",
        "                    ax1.bar(freq_df[\"drug\"], freq_df[\"frequency_pct\"], color=\"steelblue\")\n",
        "                    ax1.set_ylabel(\"% patients selected\")\n",
        "                    ax1.set_title(\"Drug selection stability across patients\")\n",
        "                    plt.xticks(rotation=45, ha=\"right\")\n",
        "                    plt.tight_layout()\n",
        "                    visualizations_placeholder.pyplot(fig1)\n",
        "                except Exception as e:\n",
        "                    visualizations_placeholder.write(f\"Could not generate frequency barplot: {e}\")\n",
        "\n",
        "                # Co-selection heatmap and dendrogram\n",
        "                try:\n",
        "                    st.subheader(\"Co-selection Analysis\")\n",
        "                     # --- build co-selection counts ---\n",
        "                    co_mat = pd.DataFrame(0, index=drugs, columns=drugs, dtype=int)\n",
        "                    n_pat = max(1, len(all_sels))\n",
        "                    for sel in all_sels:\n",
        "                        uniq = list(dict.fromkeys(sel))\n",
        "                        for i in range(len(uniq)):\n",
        "                            for j in range(i, len(uniq)):\n",
        "                                di, dj = uniq[i], uniq[j]\n",
        "                                co_mat.loc[di, dj] += 1\n",
        "                                if i != j:\n",
        "                                    co_mat.loc[dj, di] += 1\n",
        "\n",
        "                    # normalize to % of patients\n",
        "                    co_pct = co_mat / n_pat * 100.0\n",
        "                    st.write(\"Co-selection matrix (% of patients):\")\n",
        "                    st.dataframe(co_pct.round(1))\n",
        "\n",
        "                    # heatmap\n",
        "                    fig2, ax2 = plt.subplots(figsize=(7,6))\n",
        "                    im = ax2.imshow(co_pct.values, cmap=\"Blues\", interpolation=\"nearest\")\n",
        "                    ax2.set_xticks(range(len(drugs)), drugs, rotation=45, ha=\"right\")\n",
        "                    ax2.set_yticks(range(len(drugs)), drugs)\n",
        "                    plt.colorbar(im, ax=ax2, label=\"% patients co-selected\")\n",
        "                    ax2.set_title(\"Drug co-selection heatmap\")\n",
        "                    plt.tight_layout()\n",
        "                    visualizations_placeholder.pyplot(fig2)\n",
        "\n",
        "                    # dendrogram\n",
        "                    corr = np.corrcoef(co_pct.values)\n",
        "                    corr = np.clip(corr, -1.0, 1.0)\n",
        "                    dist = 1.0 - corr\n",
        "                    Z = linkage(dist, method=\"average\")\n",
        "\n",
        "                    fig3, ax3 = plt.subplots(figsize=(8,5))\n",
        "                    dendrogram(Z, labels=drugs, leaf_rotation=90, leaf_font_size=10,\n",
        "                               color_threshold=0.7 * np.max(Z[:,2]), ax=ax3)\n",
        "                    ax3.set_title(\"Clustered dendrogram of drug co-selection\")\n",
        "                    ax3.set_ylabel(\"Distance (1 - correlation)\")\n",
        "                    plt.tight_layout()\n",
        "                    visualizations_placeholder.pyplot(fig3)\n",
        "\n",
        "                    # clustered heatmap\n",
        "                    order = leaves_list(Z)\n",
        "                    co_pct_ordered = co_pct.iloc[order, order]\n",
        "\n",
        "                    fig4, ax4 = plt.subplots(figsize=(7,6))\n",
        "                    im = ax4.imshow(co_pct_ordered.values, cmap=\"Blues\", interpolation=\"nearest\")\n",
        "                    ax4.set_xticks(range(len(order)), co_pct_ordered.columns, rotation=45, ha=\"right\")\n",
        "                    ax4.set_yticks(range(len(order)), co_pct_ordered.index)\n",
        "                    plt.colorbar(im, ax=ax4, label=\"% patients co-selected\")\n",
        "                    ax4.set_title(\"Clustered heatmap of drug co-selection\")\n",
        "                    plt.tight_layout()\n",
        "                    visualizations_placeholder.pyplot(fig4)\n",
        "                    st.write(\"Clustered co-selection matrix:\")\n",
        "                    st.dataframe(co_pct_ordered.round(1))\n",
        "\n",
        "                except Exception as e:\n",
        "                    visualizations_placeholder.write(f\"Could not generate co-selection plots: {e}\")\n",
        "\n",
        "                # Selection size distribution histogram\n",
        "                try:\n",
        "                    sizes = [len(set(sel)) for sel in all_sels]\n",
        "                    size_hist = pd.Series(sizes).value_counts().sort_index()\n",
        "                    fig5, ax5 = plt.subplots(figsize=(5,4))\n",
        "                    ax5.bar(size_hist.index, size_hist.values)\n",
        "                    ax5.set_xlabel(\"# drugs selected\")\n",
        "                    ax5.set_ylabel(\"# patients\")\n",
        "                    ax5.set_title(\"Selection size distribution\")\n",
        "                    plt.tight_layout()\n",
        "                    visualizations_placeholder.pyplot(fig5)\n",
        "                except Exception as e:\n",
        "                    visualizations_placeholder.write(f\"Could not generate selection size histogram: {e}\")\n",
        "\n",
        "                # Synthetic Classification Validation\n",
        "                try:\n",
        "                    st.subheader(\"Synthetic Classification Validation\")\n",
        "                    # 1) Construct patient-drug feature matrix\n",
        "                    N = len(all_sels)\n",
        "                    X = np.zeros((N, len(drugs)))\n",
        "                    for i, sel in enumerate(all_sels):\n",
        "                        for d in sel:\n",
        "                            if d in drugs:\n",
        "                                X[i, drugs.index(d)] = 1\n",
        "\n",
        "                    # 2) Synthetic binary labels\n",
        "                    risk_scores = []\n",
        "                    for sid in P.columns[:N]:\n",
        "                         z_path = patient_vector(P, sid)\n",
        "                         risk_scores.append(z_path.mean())\n",
        "                    risk_scores = np.array(risk_scores)\n",
        "                    # Handle case where all risk scores are the same\n",
        "                    if np.std(risk_scores) == 0:\n",
        "                         st.warning(\"Cannot generate synthetic labels: all pathway mean scores are the same.\")\n",
        "                         y = np.zeros(N) # Or handle as appropriate\n",
        "                         st.write(\"Synthetic labels (all 0):\")\n",
        "                         st.write(np.bincount(y))\n",
        "                    else:\n",
        "                         y = (risk_scores > np.median(risk_scores)).astype(int)\n",
        "                         st.write(\"Synthetic labels (0=low risk, 1=high risk):\")\n",
        "                         st.write(np.bincount(y))\n",
        "\n",
        "                    # 3) Train simple logistic regression\n",
        "                    if len(np.unique(y)) > 1: # Only train if there's more than one class\n",
        "                        clf = LogisticRegression(max_iter=200)\n",
        "                        clf.fit(X, y)\n",
        "                        y_pred = clf.predict_proba(X)[:,1]\n",
        "\n",
        "                        # 4) Metrics\n",
        "                        roc_auc = roc_auc_score(y, y_pred)\n",
        "                        pr_auc = average_precision_score(y, y_pred)\n",
        "\n",
        "                        st.write(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "                        st.write(f\"PR-AUC: {pr_auc:.3f}\")\n",
        "\n",
        "                        # Optional barplot of learned coefficients\n",
        "                        coefs = pd.Series(clf.coef_[0], index=drugs).sort_values()\n",
        "                        fig6, ax6 = plt.subplots(figsize=(7,4))\n",
        "                        coefs.plot(kind=\"barh\", ax=ax6, color=[\"steelblue\" if v>0 else \"salmon\" for v in coefs])\n",
        "                        ax6.set_title(\"Drug selection coefficients (synthetic label prediction)\")\n",
        "                        ax6.set_xlabel(\"Weight\")\n",
        "                        plt.tight_layout()\n",
        "                        visualizations_placeholder.pyplot(fig6)\n",
        "                    else:\n",
        "                         st.warning(\"Classification skipped: only one unique synthetic label class found.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    visualizations_placeholder.write(f\"Could not perform synthetic classification validation: {e}\")\n",
        "\n",
        "\n",
        "            st.success(\"Analysis complete!\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload a gene expression data file to begin.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efe06542"
      },
      "source": [
        "## Handle data input and output\n",
        "\n",
        "### Subtask:\n",
        "Implement mechanisms for users to upload their gene expression data (e.g., CSV file) through the web interface and for the application to output the results in a user-friendly format (e.g., tables, plots, downloadable reports).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a769607a"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement file upload, data loading, and display results using Streamlit widgets and the core logic functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e560687f"
      },
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip\n",
        "import re\n",
        "from collections import Counter\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import io\n",
        "import base64\n",
        "\n",
        "# Assume the core logic functions from the notebook are available here.\n",
        "# For demonstration, I'll include them directly. In a larger project,\n",
        "# these would be in a separate module.\n",
        "\n",
        "def norm_ensembl(x): return x.split('.',1)[0]\n",
        "def norm_symbol(x):  return re.sub(r'[^A-Za-z0-9_-]+','', x)\n",
        "\n",
        "def detect_row_mode(file_object, scan_rows=50000):\n",
        "    \"\"\"Detect row ID type (Ensembl vs symbol) from file-like object.\"\"\"\n",
        "    seen = Counter(); total = 0\n",
        "    # Use a TextIOWrapper to treat the bytes stream as text\n",
        "    # Ensure file_object is at the beginning\n",
        "    file_object.seek(0)\n",
        "    with io.TextIOWrapper(file_object, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        # Read the header line to skip it for row detection\n",
        "        header = f.readline()\n",
        "        # Use header=None because we already read the header\n",
        "        reader = pd.read_csv(f, sep=\"\\t\", chunksize=200_000, usecols=[0], dtype=str, header=None)\n",
        "        for ch in reader:\n",
        "            v = ch.iloc[:,0].astype(str)\n",
        "            vals = v.head(min(len(v), scan_rows-total)).tolist()\n",
        "            total += len(vals)\n",
        "            seen.update('ENSG' if x.startswith('ENSG') else 'OTHER' for x in vals)\n",
        "            if total >= scan_rows: break\n",
        "    ratio = seen['ENSG']/max(1,(seen['ENSG']+seen['OTHER']))\n",
        "    mode = 'ensembl' if ratio >= 0.6 else 'symbol'\n",
        "    return mode\n",
        "\n",
        "def stream_select_rows_columns(file_object, selected_samples, sig_genes, sym2ensg, row_mode, gene_col_name=\"Name\", sep='\\t'):\n",
        "    \"\"\"Stream-select rows and columns from file-like object.\"\"\"\n",
        "    if row_mode == 'ensembl':\n",
        "        target_rows = set(sym2ensg.get(g) for g in sig_genes if g in sym2ensg)\n",
        "        normalize = norm_ensembl\n",
        "    else:\n",
        "        target_rows = set(sig_genes)\n",
        "        normalize = norm_symbol\n",
        "\n",
        "    # Ensure file_object is at the beginning\n",
        "    file_object.seek(0)\n",
        "\n",
        "    with io.TextIOWrapper(file_object, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        # Read header to find the actual gene_col_name and sample columns\n",
        "        header = f.readline().rstrip(\"\\n\").split(sep)\n",
        "        try:\n",
        "            gene_col_idx = header.index(gene_col_name)\n",
        "        except ValueError:\n",
        "             # If gene_col_name not found, assume the first column is the gene column\n",
        "            gene_col_name = header[0]\n",
        "            gene_col_idx = 0\n",
        "\n",
        "        # Filter header to include only the gene column and selected sample columns\n",
        "        usecols_filter = [gene_col_name] + selected_samples\n",
        "\n",
        "        # Read data in chunks, selecting only necessary columns\n",
        "        # Use header=None as we've already read the header\n",
        "        reader = pd.read_csv(f, sep=sep, chunksize=50_000, dtype=str, header=None)\n",
        "        kept = []\n",
        "        for i, ch in enumerate(reader):\n",
        "            # Assign original header to the chunk\n",
        "            ch.columns = header\n",
        "            # Select only the columns we need\n",
        "            ch = ch[usecols_filter]\n",
        "\n",
        "            ch = ch.rename(columns={gene_col_name: \"row_id\"})\n",
        "\n",
        "            # Ensure 'row_id' column exists after renaming\n",
        "            if 'row_id' not in ch.columns:\n",
        "                 st.error(f\"Error: Could not find gene identifier column '{gene_col_name}' in the uploaded file.\")\n",
        "                 return pd.DataFrame() # Return empty DataFrame on error\n",
        "\n",
        "            ids = ch[\"row_id\"].astype(str).map(normalize)\n",
        "            mask = ids.isin(target_rows)\n",
        "\n",
        "            if mask.any():\n",
        "                out = ch.loc[mask].copy()\n",
        "                out[\"row_id\"] = out[\"row_id\"].map(normalize)\n",
        "                kept.append(out)\n",
        "\n",
        "    if not kept:\n",
        "        st.warning(\"No signature rows matched the provided gene list.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    expr_small = pd.concat(kept, axis=0, ignore_index=False).drop_duplicates(subset=[\"row_id\"]).set_index(\"row_id\")\n",
        "\n",
        "    if row_mode == 'ensembl':\n",
        "        ensg2sym = {v:k for k,v in sym2ensg.items()}\n",
        "        expr_sym_small = expr_small.copy()\n",
        "        expr_sym_small.index = [ensg2sym.get(e, e) for e in expr_small.index]\n",
        "    else:\n",
        "        expr_sym_small = expr_small.copy()\n",
        "\n",
        "    expr_sym_small = expr_sym_small[~expr_sym_small.index.duplicated(keep=\"first\")]\n",
        "    return expr_sym_small\n",
        "\n",
        "\n",
        "def zscore_by_gene(expr_symbols: pd.DataFrame) -> pd.DataFrame:\n",
        "    E = expr_symbols.copy()\n",
        "    E = E.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    E = E.loc[~E.isna().all(axis=1)]\n",
        "    gene_means = E.mean(axis=1)\n",
        "    E = E.apply(lambda col: col.fillna(gene_means), axis=0)\n",
        "    mu = E.mean(axis=1)\n",
        "    sd = E.std(axis=1) + 1e-8\n",
        "    return (E.sub(mu, axis=0)).div(sd, axis=0)\n",
        "\n",
        "def pathway_scores(expr_symbols: pd.DataFrame, signatures: dict) -> pd.DataFrame:\n",
        "    \"\"\"Return pathways x samples (mean z across member genes present).\"\"\"\n",
        "    Z = zscore_by_gene(expr_symbols)\n",
        "    rows = []\n",
        "    for pw, genes in signatures.items():\n",
        "        present = [g for g in genes if g in Z.index]\n",
        "        if present:\n",
        "            s = Z.loc[present].mean(axis=0)\n",
        "        else:\n",
        "            s = pd.Series([np.nan]*Z.shape[1], index=Z.columns)\n",
        "        s.name = pw\n",
        "        rows.append(s)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def example_drug_panel():\n",
        "    return {\n",
        "        \"EGFRi\": [\"REACTOME_SIGNALING_BY_EGFR\"],\n",
        "        \"ALKi\":  [\"REACTOME_SIGNALING_BY_ALK\"],\n",
        "        \"MEKi\":  [\"REACTOME_MAPK1_MAPK3_SIGNALING\"],\n",
        "        \"PI3Ki\": [\"REACTOME_PI3K_AKT_SIGNALING\"],\n",
        "        \"mTORi\": [\"REACTOME_MTORC1_MEDIATED_SIGNALLING\"],\n",
        "        \"PD1i\":  [\"REACTOME_PD1_SIGNALING\"],\n",
        "        \"VEGFi\": [\"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\"],\n",
        "        \"FGFRi\": [\"REACTOME_SIGNALING_BY_FGFR\"],\n",
        "    }\n",
        "\n",
        "def patient_vector(P: pd.DataFrame, sample_id: str) -> pd.Series:\n",
        "    \"\"\"z-normalize pathways across samples; return vector for one patient.\"\"\"\n",
        "    z = (P - P.mean(axis=1).values.reshape(-1,1)) / (P.std(axis=1).values.reshape(-1,1) + 1e-8)\n",
        "    return z[sample_id].fillna(0.0)\n",
        "\n",
        "def drug_benefit_prior(z_path: pd.Series, panel: dict) -> pd.Series:\n",
        "    \"\"\"Aggregate pathway z's per drug (ReLU to emphasize upregulated pathways).\"\"\"\n",
        "    s = pd.Series({d: float(np.sum([max(z_path.get(p, 0.0), 0.0) for p in pws])) for d, pws in panel.items()})\n",
        "    return s / s.max() if s.max() > 0 else s\n",
        "\n",
        "def build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10):\n",
        "    \"\"\"Pairwise penalties for overlapping mechanisms; diagonal = sparsity.\"\"\"\n",
        "    K = len(drugs); R = np.zeros((K, K), dtype=float)\n",
        "    for i in range(K):\n",
        "        for j in range(i+1, K):\n",
        "            overlap = len(set(panel[drugs[i]]) & set(panel[drugs[j]]))\n",
        "            if overlap > 0:\n",
        "                R[i, j] = R[j, i] = base_overlap * overlap\n",
        "    for i in range(K):\n",
        "        R[i, i] += sparsity\n",
        "    return R\n",
        "\n",
        "def build_qubo(b_hat, R, lam=1.0):\n",
        "    \"\"\"QUBO: minimize x^T (lam R) x + q^T x  where q = -b̂ + diag(lam R).\"\"\"\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy()\n",
        "    diag = np.diag(Q).copy()\n",
        "    np.fill_diagonal(Q, 0.0)  # keep off-diagonal in Q\n",
        "    q += diag\n",
        "    return q, np.triu(Q, 1)\n",
        "\n",
        "def exact_qubo_solve(b_hat: np.ndarray, R: np.ndarray, lam: float = 1.0):\n",
        "    \"\"\"\n",
        "    Exact minimization of QUBO:\n",
        "      E(x) = x^T (lam R) x + q^T x, with q = -b̂ + diag(lam R)\n",
        "    We enumerate all bitstrings (2^K). Returns best bitstring (as 0/1 np array).\n",
        "    \"\"\"\n",
        "    K = len(b_hat)\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy() + np.diag(Q)\n",
        "    np.fill_diagonal(Q, 0.0)  # keep only off-diagonal in Q\n",
        "\n",
        "    best_e = np.inf\n",
        "    best_x = None\n",
        "    # vectorize partial precomputations\n",
        "    upper_idx = np.triu_indices(K, 1)\n",
        "    for mask in range(1 << K):\n",
        "        # build x from bits\n",
        "        x = np.fromiter(((mask >> i) & 1 for i in range(K)), dtype=np.int8)\n",
        "        # E = x^T Q x + q^T x, where Q is strictly upper-triangular mirrored\n",
        "        e = np.dot(q, x) + 2.0 * np.sum(Q[upper_idx] * (x[upper_idx[0]] * x[upper_idx[1]]))\n",
        "        if e < best_e:\n",
        "            best_e, best_x = e, x\n",
        "    return best_x, float(best_e)\n",
        "\n",
        "\n",
        "# Placeholder for CUDA-Q function if needed later, currently uses exact solve\n",
        "def try_cudaq_qaoa(h, J, p=2, shots=2048, max_iters=60, seed=7):\n",
        "    return None # Not implemented in this web app version\n",
        "\n",
        "# --- Main analysis function ---\n",
        "def run_analysis(uploaded_file, n_samples, lam_value):\n",
        "    \"\"\"\n",
        "    Runs the full analysis workflow for the uploaded data.\n",
        "    Returns pathway scores, drug selection summary, and patient report.\n",
        "    \"\"\"\n",
        "    SIGS = {\n",
        "        \"REACTOME_SIGNALING_BY_EGFR\": [\n",
        "            \"EGFR\",\"ERBB2\",\"ERBB3\",\"GRB2\",\"SOS1\",\"SHC1\",\"PTPN11\",\"KRAS\",\"NRAS\",\"HRAS\",\n",
        "            \"BRAF\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"GAB1\"\n",
        "        ],\n",
        "        \"REACTOME_SIGNALING_BY_ALK\": [\n",
        "            \"ALK\",\"EML4\",\"GRB2\",\"SHC1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"STAT3\",\"MAP2K1\",\"MAPK1\",\"MAPK3\"\n",
        "        ],\n",
        "        \"REACTOME_MAPK1_MAPK3_SIGNALING\": [\n",
        "            \"BRAF\",\"RAF1\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"DUSP6\",\"DUSP4\",\"FOS\",\"JUN\",\"EGFR\"\n",
        "        ],\n",
        "        \"REACTOME_PI3K_AKT_SIGNALING\": [\n",
        "            \"PIK3CA\",\"PIK3CB\",\"PIK3CD\",\"PIK3R1\",\"PIK3R2\",\"AKT1\",\"AKT2\",\"AKT3\",\"PTEN\",\"MTOR\",\"RHEB\"\n",
        "        ],\n",
        "        \"REACTOME_MTORC1_MEDIATED_SIGNALLING\": [\n",
        "            \"MTOR\",\"RPTOR\",\"MLST8\",\"RHEB\",\"TSC1\",\"TSC2\",\"EIF4EBP1\",\"RPS6KB1\",\"RPS6\"\n",
        "        ],\n",
        "        \"REACTOME_PD1_SIGNALING\": [\n",
        "            \"PDCD1\",\"CD274\",\"PDCD1LG2\",\"PDCD1LG2\",\"JAK1\",\"JAK2\",\"STAT1\",\"IFNG\",\"GZMB\",\"LAG3\",\"TIGIT\",\"CXCL9\",\"CXCL10\"\n",
        "        ],\n",
        "        \"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\": [\n",
        "            \"VEGFA\",\"KDR\",\"FLT1\",\"FLT4\",\"PTPRB\",\"PLCG1\",\"MAP2K1\",\"MAPK1\",\"NOS3\"\n",
        "        ],\n",
        "        \"REACTOME_SIGNALING_BY_FGFR\": [\n",
        "            \"FGFR1\",\"FGFR2\",\"FGFR3\",\"FGFR4\",\"FRS2\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"MAP2K1\",\"MAPK1\"\n",
        "        ]\n",
        "    }\n",
        "    SIG_GENES = sorted({g for gs in SIGS.values() for g in gs})\n",
        "    SYM2ENSG = {\n",
        "        \"EGFR\":\"ENSG00000146648\",\"ERBB2\":\"ENSG00000141736\",\"ERBB3\":\"ENSG00000065361\",\"GRB2\":\"ENSG00000177885\",\n",
        "        \"SOS1\":\"ENSG00000115904\",\"SHC1\":\"ENSG00000154639\",\"PTPN11\":\"ENSG00000179295\",\"KRAS\":\"ENSG00000133703\",\n",
        "        \"NRAS\":\"ENSG00000213281\",\"HRAS\":\"ENSG00000174775\",\"BRAF\":\"ENSG00000157764\",\"MAP2K1\":\"ENSG00000169032\",\n",
        "        \"MAP2K2\":\"ENSG00000126934\",\"MAPK1\":\"ENSG00000100030\",\"MAPK3\":\"ENSG00000102882\",\"PLCG1\":\"ENSG00000124181\",\n",
        "        \"PIK3CA\":\"ENSG00000121879\",\"PIK3R1\":\"ENSG00000145675\",\"AKT1\":\"ENSG00000142208\",\"AKT2\":\"ENSG00000105221\",\n",
        "        \"AKT3\":\"ENSG00000117020\",\"GAB1\":\"ENSG00000117676\",\n",
        "        \"ALK\":\"ENSG00000171094\",\"EML4\":\"ENSG00000143924\",\"STAT3\":\"ENSG00000168610\",\n",
        "        \"DUSP6\":\"ENSG00000139318\",\"DUSP4\":\"ENSG00000120875\",\"FOS\":\"ENSG00000170345\",\"JUN\":\"ENSG00000177606\",\"RAF1\":\"ENSG00000132155\",\n",
        "        \"PIK3CB\":\"ENSG00000119402\",\"PIK3CD\":\"ENSG00000171608\",\"PIK3R2\":\"ENSG00000189403\",\"PTEN\":\"ENSG00000171862\",\n",
        "        \"MTOR\":\"ENSG00000198793\",\"RHEB\":\"ENSG00000106615\",\n",
        "        \"RPTOR\":\"ENSG00000141564\",\"MLST8\":\"ENSG00000105705\",\"TSC1\":\"ENSG00000165699\",\"TSC2\":\"ENSG00000103197\",\n",
        "        \"EIF4EBP1\":\"ENSG00000187840\",\"RPS6KB1\":\"ENSG00000108443\",\"RPS6\":\"ENSG00000137154\",\n",
        "        \"PDCD1\":\"ENSG00000276977\",\"CD274\":\"ENSG00000120217\",\"PDCD1LG2\":\"ENSG00000197646\",\"JAK1\":\"ENSG00000162434\",\n",
        "        \"JAK2\":\"ENSG00000096968\",\"STAT1\":\"ENSG00000115415\",\"IFNG\":\"ENSG00000111537\",\"GZMB\":\"ENSG00000100453\",\n",
        "        \"LAG3\":\"ENSG00000089692\",\"TIGIT\":\"ENSG00000181847\",\"CXCL9\":\"ENSG00000138755\",\"CXCL10\":\"ENSG00000169245\",\n",
        "        \"VEGFA\":\"ENSG00000112715\",\"KDR\":\"ENSG00000128052\",\"FLT1\":\"ENSG00000102755\",\"FLT4\":\"ENSG00000037280\",\n",
        "        \"PTPRB\":\"ENSG00000160593\",\"NOS3\":\"ENSG00000164867\",\n",
        "        \"FGFR1\":\"ENSG00000077782\",\"FGFR2\":\"ENSG00000066468\",\"FGFR3\":\"ENSG00000068078\",\"FGFR4\":\"ENSG00000069535\",\n",
        "        \"FRS2\":\"ENSG00000181873\"\n",
        "    }\n",
        "\n",
        "    # Data Loading and Initial Processing\n",
        "    # Handle potential gzip compression and determine delimiter\n",
        "    file_content = uploaded_file.getvalue()\n",
        "    if uploaded_file.name.endswith('.gz'):\n",
        "        try:\n",
        "            gz_file = gzip.GzipFile(fileobj=io.BytesIO(file_content))\n",
        "            # Peek at the first line after decompression to determine delimiter\n",
        "            with io.TextIOWrapper(gz_file, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                header_peek = f.readline().rstrip(\"\\n\")\n",
        "                sep = ',' if ',' in header_peek.split() else '\\t'\n",
        "            # Reset gz_file for actual reading\n",
        "            gz_file.seek(0)\n",
        "            file_stream = io.BytesIO(gz_file.read())\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error reading gzipped file: {e}\")\n",
        "            return None, None, None, None, None, None\n",
        "    else:\n",
        "        # Determine delimiter for non-gzipped files\n",
        "        try:\n",
        "            header_peek = io.BytesIO(file_content).readline().decode('utf-8').rstrip(\"\\n\")\n",
        "            sep = ',' if ',' in header_peek.split() else '\\t'\n",
        "        except Exception as e:\n",
        "             st.error(f\"Error reading file header: {e}\")\n",
        "             return None, None, None, None, None, None\n",
        "        file_stream = io.BytesIO(file_content)\n",
        "\n",
        "    st.write(f\"Detected delimiter: '{sep}'\")\n",
        "\n",
        "    # Detect row mode\n",
        "    # Need a fresh file object for detect_row_mode\n",
        "    file_stream_for_detect = io.BytesIO(file_stream.getvalue())\n",
        "    row_mode = detect_row_mode(file_stream_for_detect)\n",
        "    st.write(f\"Detected row mode: {row_mode.upper()}\")\n",
        "\n",
        "    # Get header to identify sample columns and gene column\n",
        "    file_stream.seek(0) # Ensure stream is at the beginning\n",
        "    header_line = io.TextIOWrapper(file_stream, encoding=\"utf-8\", errors=\"replace\").readline().rstrip(\"\\n\")\n",
        "    cols = header_line.split(sep)\n",
        "    # Assuming the first column is the gene ID/symbol column\n",
        "    gene_col = cols[0]\n",
        "    # Assuming sample columns start from the second column\n",
        "    sample_cols_full = cols[1:]\n",
        "\n",
        "    # Select samples based on n_samples parameter\n",
        "    if n_samples > len(sample_cols_full):\n",
        "        st.warning(f\"Requested {n_samples} samples, but only {len(sample_cols_full)} available. Using all available samples.\")\n",
        "        selected_samples = sample_cols_full\n",
        "    else:\n",
        "        selected_samples = sample_cols_full[:n_samples]\n",
        "\n",
        "    st.write(f\"Processing {len(selected_samples)} samples.\")\n",
        "    st.write(f\"Using gene column: '{gene_col}'\")\n",
        "\n",
        "    # Stream-select rows and columns\n",
        "    # Need a fresh file object for stream_select_rows_columns\n",
        "    file_stream_for_select = io.BytesIO(file_stream.getvalue())\n",
        "    expr_sym_small = stream_select_rows_columns(file_stream_for_select, selected_samples, SIG_GENES, SYM2ENSG, row_mode, gene_col_name=gene_col, sep=sep)\n",
        "\n",
        "    if expr_sym_small.empty:\n",
        "        st.error(\"Failed to load expression data. Please check file format and contents.\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "    st.subheader(\"Expression Matrix (Subset)\")\n",
        "    st.write(\"Shape (genes x samples):\", expr_sym_small.shape)\n",
        "    st.dataframe(expr_sym_small.head())\n",
        "\n",
        "    # Pathway Scoring\n",
        "    st.subheader(\"Pathway Activity Scores\")\n",
        "    P = pathway_scores(expr_sym_small, SIGS)\n",
        "    st.write(\"Shape (pathways x samples):\", P.shape)\n",
        "    st.dataframe(P)\n",
        "\n",
        "    # QUBO Formulation and Drug Selection\n",
        "    st.subheader(\"Drug Selection Results\")\n",
        "    panel = example_drug_panel()\n",
        "    drugs = list(panel.keys())\n",
        "\n",
        "    all_sels = []\n",
        "    all_bhats = []\n",
        "    patient_reports_data = []\n",
        "\n",
        "    # Run analysis for each selected sample\n",
        "    if P.empty:\n",
        "        st.warning(\"No pathway scores computed. Cannot perform drug selection.\")\n",
        "        return P, pd.DataFrame(), pd.DataFrame(), [], drugs, panel\n",
        "\n",
        "    for sample_id in P.columns:\n",
        "        z_path = patient_vector(P, sample_id)\n",
        "        b_series = drug_benefit_prior(z_path, panel).reindex(drugs).fillna(0.0)\n",
        "        R = build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10)\n",
        "        b = b_series.to_numpy(float)\n",
        "        q, Q = build_qubo(b, R, lam=lam_value)\n",
        "\n",
        "        # Use exact solver for now\n",
        "        x_star, e_star = exact_qubo_solve(b, R, lam=lam_value)\n",
        "        sel = [drugs[i] for i, xi in enumerate(x_star) if xi == 1]\n",
        "        all_sels.append(sel)\n",
        "        all_bhats.append(b_series)\n",
        "\n",
        "        # Prepare data for patient report\n",
        "        top_pw = z_path.abs().sort_values(ascending=False).head(5)\n",
        "        patient_reports_data.append({\n",
        "            \"patient\": sample_id,\n",
        "            \"selected_drugs\": \", \".join(sel) if sel else \"(none)\",\n",
        "            \"top_pathways\": \"; \".join([f\"{p}:{z_path[p]:+.2f}\" for p in top_pw.index]),\n",
        "            **{f\"b̂.{d}\": float(b_series.get(d,0.0)) for d in drugs}\n",
        "        })\n",
        "\n",
        "    # Drug Selection Summary\n",
        "    st.subheader(\"Drug Selection Frequency\")\n",
        "    if not all_sels:\n",
        "        st.info(\"No drugs were selected for any patient.\")\n",
        "        freq_df = pd.DataFrame({\"drug\": drugs, \"frequency\": 0, \"frequency_pct\": 0.0})\n",
        "    else:\n",
        "        flat = [d for sel in all_sels for d in sel]\n",
        "        freq = Counter(flat)\n",
        "        freq_df = pd.DataFrame({\"drug\": drugs, \"frequency\": [freq[d] for d in drugs]})\n",
        "        freq_df[\"frequency_pct\"] = 100 * freq_df[\"frequency\"] / len(selected_samples)\n",
        "        freq_df = freq_df.sort_values(\"frequency_pct\", ascending=False).reset_index(drop=True)\n",
        "    st.dataframe(freq_df)\n",
        "\n",
        "    # Patient-level Report\n",
        "    st.subheader(\"Patient-level Report\")\n",
        "    patient_report_df = pd.DataFrame(patient_reports_data)\n",
        "    st.dataframe(patient_report_df)\n",
        "\n",
        "    return P, freq_df, patient_report_df, all_sels, drugs, panel\n",
        "\n",
        "# Function to create a download link\n",
        "def create_download_link(df, filename, text):\n",
        "    csv = df.to_csv(index=False)\n",
        "    b64 = base64.b64encode(csv.encode()).decode()\n",
        "    href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"{filename}\">{text}</a>'\n",
        "    return href\n",
        "\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "st.title(\"Digital Twin Workflow: Gene Expression to Drug Selection\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "This application demonstrates a workflow for identifying potential drug therapies\n",
        "for cancer patients based on their gene expression data, using pathway analysis\n",
        "and QUBO optimization.\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.header(\"Input Data\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload gene expression data (CSV, TSV, or gzipped)\", type=[\"csv\", \"tsv\", \"gz\"])\n",
        "\n",
        "st.sidebar.header(\"Parameters\")\n",
        "n_samples = st.sidebar.slider(\"Number of samples to process\", min_value=1, max_value=100, value=40, step=1)\n",
        "lam_value = st.sidebar.slider(\"Lambda (penalty) value for QUBO\", min_value=0.1, max_value=5.0, value=1.0, step=0.1)\n",
        "\n",
        "st.header(\"Analysis Results\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    if st.button(\"Run Analysis\"):\n",
        "        with st.spinner(f\"Running analysis for {n_samples} samples with lambda={lam_value}...\"):\n",
        "            pathway_scores_df, freq_df, patient_report_df, all_sels, drugs, panel = run_analysis(uploaded_file, n_samples, lam_value)\n",
        "\n",
        "            if pathway_scores_df is not None:\n",
        "                st.subheader(\"Visualizations\")\n",
        "\n",
        "                # Drug selection frequency barplot\n",
        "                if freq_df is not None and not freq_df.empty:\n",
        "                    try:\n",
        "                        fig1, ax1 = plt.subplots(figsize=(7,5))\n",
        "                        ax1.bar(freq_df[\"drug\"], freq_df[\"frequency_pct\"], color=\"steelblue\")\n",
        "                        ax1.set_ylabel(\"% patients selected\")\n",
        "                        ax1.set_title(\"Drug selection stability across patients\")\n",
        "                        plt.xticks(rotation=45, ha=\"right\")\n",
        "                        plt.tight_layout()\n",
        "                        st.pyplot(fig1)\n",
        "                        plt.close(fig1) # Close figure to free memory\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not generate frequency barplot: {e}\")\n",
        "\n",
        "                # Co-selection heatmap and dendrogram\n",
        "                if all_sels and drugs and panel:\n",
        "                    try:\n",
        "                        st.subheader(\"Co-selection Analysis\")\n",
        "                         # --- build co-selection counts ---\n",
        "                        co_mat = pd.DataFrame(0, index=drugs, columns=drugs, dtype=int)\n",
        "                        n_pat = max(1, len(all_sels))\n",
        "                        for sel in all_sels:\n",
        "                            uniq = list(dict.fromkeys(sel))\n",
        "                            for i in range(len(uniq)):\n",
        "                                for j in range(i, len(uniq)):\n",
        "                                    di, dj = uniq[i], uniq[j]\n",
        "                                    co_mat.loc[di, dj] += 1\n",
        "                                    if i != j:\n",
        "                                        co_mat.loc[dj, di] += 1\n",
        "\n",
        "                        # normalize to % of patients\n",
        "                        co_pct = co_mat / n_pat * 100.0\n",
        "                        st.write(\"Co-selection matrix (% of patients):\")\n",
        "                        st.dataframe(co_pct.round(1))\n",
        "\n",
        "                        # heatmap\n",
        "                        fig2, ax2 = plt.subplots(figsize=(7,6))\n",
        "                        im = ax2.imshow(co_pct.values, cmap=\"Blues\", interpolation=\"nearest\")\n",
        "                        ax2.set_xticks(range(len(drugs)), drugs, rotation=45, ha=\"right\")\n",
        "                        ax2.set_yticks(range(len(drugs)), drugs)\n",
        "                        plt.colorbar(im, ax=ax2, label=\"% patients co-selected\")\n",
        "                        ax2.set_title(\"Drug co-selection heatmap\")\n",
        "                        plt.tight_layout()\n",
        "                        st.pyplot(fig2)\n",
        "                        plt.close(fig2) # Close figure\n",
        "\n",
        "                        # dendrogram\n",
        "                        corr = np.corrcoef(co_pct.values)\n",
        "                        corr = np.clip(corr, -1.0, 1.0)\n",
        "                        dist = 1.0 - corr\n",
        "                        Z = linkage(dist, method=\"average\")\n",
        "\n",
        "                        fig3, ax3 = plt.subplots(figsize=(8,5))\n",
        "                        dendrogram(Z, labels=drugs, leaf_rotation=90, leaf_font_size=10,\n",
        "                                color_threshold=0.7 * np.max(Z[:,2]), ax=ax3)\n",
        "                        ax3.set_title(\"Clustered dendrogram of drug co-selection\")\n",
        "                        ax3.set_ylabel(\"Distance (1 - correlation)\")\n",
        "                        plt.tight_layout()\n",
        "                        st.pyplot(fig3)\n",
        "                        plt.close(fig3) # Close figure\n",
        "\n",
        "                        # clustered heatmap\n",
        "                        order = leaves_list(Z)\n",
        "                        co_pct_ordered = co_pct.iloc[order, order]\n",
        "\n",
        "                        fig4, ax4 = plt.subplots(figsize=(7,6))\n",
        "                        im = ax4.imshow(co_pct_ordered.values, cmap=\"Blues\", interpolation=\"nearest\")\n",
        "                        ax4.set_xticks(range(len(order)), co_pct_ordered.columns, rotation=45, ha=\"right\")\n",
        "                        ax4.set_yticks(range(len(order)), co_pct_ordered.index)\n",
        "                        plt.colorbar(im, ax=ax4, label=\"% patients co-selected\")\n",
        "                        ax4.set_title(\"Clustered heatmap of drug co-selection\")\n",
        "                        plt.tight_layout()\n",
        "                        st.pyplot(fig4)\n",
        "                        plt.close(fig4) # Close figure\n",
        "                        st.write(\"Clustered co-selection matrix:\")\n",
        "                        st.dataframe(co_pct_ordered.round(1))\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not generate co-selection plots: {e}\")\n",
        "\n",
        "                # Selection size distribution histogram\n",
        "                if all_sels:\n",
        "                    try:\n",
        "                        sizes = [len(set(sel)) for sel in all_sels]\n",
        "                        size_hist = pd.Series(sizes).value_counts().sort_index()\n",
        "                        fig5, ax5 = plt.subplots(figsize=(5,4))\n",
        "                        ax5.bar(size_hist.index, size_hist.values)\n",
        "                        ax5.set_xlabel(\"# drugs selected\")\n",
        "                        ax5.set_ylabel(\"# patients\")\n",
        "                        ax5.set_title(\"Selection size distribution\")\n",
        "                        plt.tight_layout()\n",
        "                        st.pyplot(fig5)\n",
        "                        plt.close(fig5) # Close figure\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not generate selection size histogram: {e}\")\n",
        "\n",
        "                # Synthetic Classification Validation\n",
        "                if all_sels and drugs and pathway_scores_df is not None and not pathway_scores_df.empty:\n",
        "                    try:\n",
        "                        st.subheader(\"Synthetic Classification Validation\")\n",
        "                        # 1) Construct patient-drug feature matrix\n",
        "                        N = len(all_sels)\n",
        "                        X = np.zeros((N, len(drugs)))\n",
        "                        for i, sel in enumerate(all_sels):\n",
        "                            for d in drugs: # Iterate through drugs to ensure correct indexing\n",
        "                                if d in sel:\n",
        "                                    X[i, drugs.index(d)] = 1\n",
        "\n",
        "                        # 2) Synthetic binary labels\n",
        "                        risk_scores = []\n",
        "                        # Ensure we only process samples that were successfully processed in run_analysis\n",
        "                        processed_sample_ids = pathway_scores_df.columns\n",
        "                        if len(processed_sample_ids) < N:\n",
        "                             st.warning(f\"Only {len(processed_sample_ids)} samples processed for classification.\")\n",
        "                             # Adjust N and X to match processed samples if necessary, or skip classification\n",
        "                             # For simplicity, skipping if mismatch or no processed samples\n",
        "                             if len(processed_sample_ids) == 0:\n",
        "                                  st.warning(\"No samples processed for synthetic classification.\")\n",
        "                                  raise ValueError(\"No processed samples\") # Trigger exception to skip\n",
        "                             N = len(processed_sample_ids)\n",
        "                             X = np.zeros((N, len(drugs))) # Rebuild X with correct size\n",
        "                             # Need to re-run selection logic for the processed samples to rebuild X correctly\n",
        "                             # This is getting complex; a simpler approach is to ensure run_analysis\n",
        "                             # returns data consistently sized or handle size mismatches more robustly.\n",
        "                             # For now, rely on the check and skip if needed.\n",
        "                             st.warning(\"Skipping synthetic classification due to sample count mismatch.\")\n",
        "                             raise ValueError(\"Sample count mismatch\") # Trigger exception to skip\n",
        "\n",
        "                        for sid in processed_sample_ids:\n",
        "                             z_path = patient_vector(pathway_scores_df, sid)\n",
        "                             risk_scores.append(z_path.mean())\n",
        "                        risk_scores = np.array(risk_scores)\n",
        "\n",
        "                        # Handle case where all risk scores are the same\n",
        "                        if np.std(risk_scores) == 0:\n",
        "                             st.warning(\"Cannot generate synthetic labels: all pathway mean scores are the same.\")\n",
        "                             st.write(\"Synthetic labels (all 0):\")\n",
        "                             st.write(np.zeros(N, dtype=int))\n",
        "                        else:\n",
        "                             y = (risk_scores > np.median(risk_scores)).astype(int)\n",
        "                             st.write(\"Synthetic labels (0=low risk, 1=high risk):\")\n",
        "                             st.write(np.bincount(y))\n",
        "\n",
        "                             # 3) Train simple logistic regression\n",
        "                             if len(np.unique(y)) > 1: # Only train if there's more than one class\n",
        "                                 clf = LogisticRegression(max_iter=200)\n",
        "                                 clf.fit(X[:N], y) # Use adjusted N\n",
        "                                 y_pred = clf.predict_proba(X[:N])[:,1] # Use adjusted N\n",
        "\n",
        "                                 # 4) Metrics\n",
        "                                 roc_auc = roc_auc_score(y, y_pred)\n",
        "                                 pr_auc = average_precision_score(y, y_pred)\n",
        "\n",
        "                                 st.write(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "                                 st.write(f\"PR-AUC: {pr_auc:.3f}\")\n",
        "\n",
        "                                 # Optional barplot of learned coefficients\n",
        "                                 coefs = pd.Series(clf.coef_[0], index=drugs).sort_values()\n",
        "                                 fig6, ax6 = plt.subplots(figsize=(7,4))\n",
        "                                 coefs.plot(kind=\"barh\", ax=ax6, color=[\"steelblue\" if v>0 else \"salmon\" for v in coefs])\n",
        "                                 ax6.set_title(\"Drug selection coefficients (synthetic label prediction)\")\n",
        "                                 ax6.set_xlabel(\"Weight\")\n",
        "                                 plt.tight_layout()\n",
        "                                 st.pyplot(fig6)\n",
        "                                 plt.close(fig6) # Close figure\n",
        "                             else:\n",
        "                                 st.warning(\"Classification skipped: only one unique synthetic label class found.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not perform synthetic classification validation: {e}\")\n",
        "\n",
        "                # Downloadable reports\n",
        "                if patient_report_df is not None and not patient_report_df.empty:\n",
        "                    st.subheader(\"Download Reports\")\n",
        "                    st.markdown(create_download_link(patient_report_df, \"patient_report.csv\", \"Download Patient Report (CSV)\"), unsafe_allow_html=True)\n",
        "\n",
        "                if freq_df is not None and not freq_df.empty:\n",
        "                     st.markdown(create_download_link(freq_df, \"drug_frequency_report.csv\", \"Download Drug Frequency Report (CSV)\"), unsafe_allow_html=True)\n",
        "\n",
        "                st.success(\"Analysis complete!\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload a gene expression data file to begin.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "472517a4"
      },
      "source": [
        "## Add visualizations\n",
        "\n",
        "### Subtask:\n",
        "Convert the matplotlib visualizations from the notebook into interactive plots for the web application using libraries like Plotly or Bokeh, or by embedding the matplotlib plots in the web framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cc3b57c"
      },
      "source": [
        "**Reasoning**:\n",
        "Convert the Matplotlib plots generated in the Streamlit application to interactive Plotly plots for enhanced user interaction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7f352fa"
      },
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip\n",
        "import re\n",
        "from collections import Counter\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import io\n",
        "import base64\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Assume the core logic functions from the notebook are available here.\n",
        "# For demonstration, I'll include them directly. In a larger project,\n",
        "# these would be in a separate module.\n",
        "\n",
        "def norm_ensembl(x): return x.split('.',1)[0]\n",
        "def norm_symbol(x):  return re.sub(r'[^A-Za-z0-9_-]+','', x)\n",
        "\n",
        "def detect_row_mode(file_object, scan_rows=50000):\n",
        "    \"\"\"Detect row ID type (Ensembl vs symbol) from file-like object.\"\"\"\n",
        "    seen = Counter(); total = 0\n",
        "    # Use a TextIOWrapper to treat the bytes stream as text\n",
        "    # Ensure file_object is at the beginning\n",
        "    file_object.seek(0)\n",
        "    with io.TextIOWrapper(file_object, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        # Read the header line to skip it for row detection\n",
        "        header = f.readline()\n",
        "        # Use header=None because we already read the header\n",
        "        reader = pd.read_csv(f, sep=\"\\t\", chunksize=200_000, usecols=[0], dtype=str, header=None)\n",
        "        for ch in reader:\n",
        "            v = ch.iloc[:,0].astype(str)\n",
        "            vals = v.head(min(len(v), scan_rows-total)).tolist()\n",
        "            total += len(vals)\n",
        "            seen.update('ENSG' if x.startswith('ENSG') else 'OTHER' for x in vals)\n",
        "            if total >= scan_rows: break\n",
        "    ratio = seen['ENSG']/max(1,(seen['ENSG']+seen['OTHER']))\n",
        "    mode = 'ensembl' if ratio >= 0.6 else 'symbol'\n",
        "    return mode\n",
        "\n",
        "def stream_select_rows_columns(file_object, selected_samples, sig_genes, sym2ensg, row_mode, gene_col_name=\"Name\", sep='\\t'):\n",
        "    \"\"\"Stream-select rows and columns from file-like object.\"\"\"\n",
        "    if row_mode == 'ensembl':\n",
        "        target_rows = set(sym2ensg.get(g) for g in sig_genes if g in sym2ensg)\n",
        "        normalize = norm_ensembl\n",
        "    else:\n",
        "        target_rows = set(sig_genes)\n",
        "        normalize = norm_symbol\n",
        "\n",
        "    # Ensure file_object is at the beginning\n",
        "    file_object.seek(0)\n",
        "\n",
        "    with io.TextIOWrapper(file_object, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        # Read header to find the actual gene_col_name and sample columns\n",
        "        header = f.readline().rstrip(\"\\n\").split(sep)\n",
        "        try:\n",
        "            gene_col_idx = header.index(gene_col_name)\n",
        "        except ValueError:\n",
        "             # If gene_col_name not found, assume the first column is the gene column\n",
        "            gene_col_name = header[0]\n",
        "            gene_col_idx = 0\n",
        "\n",
        "        # Filter header to include only the gene column and selected sample columns\n",
        "        usecols_filter = [gene_col_name] + selected_samples\n",
        "\n",
        "        # Read data in chunks, selecting only necessary columns\n",
        "        # Use header=None as we've already read the header\n",
        "        reader = pd.read_csv(f, sep=sep, chunksize=50_000, dtype=str, header=None)\n",
        "        kept = []\n",
        "        for i, ch in enumerate(reader):\n",
        "            # Assign original header to the chunk\n",
        "            ch.columns = header\n",
        "            # Select only the columns we need\n",
        "            ch = ch[usecols_filter]\n",
        "\n",
        "            ch = ch.rename(columns={gene_col_name: \"row_id\"})\n",
        "\n",
        "            # Ensure 'row_id' column exists after renaming\n",
        "            if 'row_id' not in ch.columns:\n",
        "                 st.error(f\"Error: Could not find gene identifier column '{gene_col_name}' in the uploaded file.\")\n",
        "                 return pd.DataFrame() # Return empty DataFrame on error\n",
        "\n",
        "            ids = ch[\"row_id\"].astype(str).map(normalize)\n",
        "            mask = ids.isin(target_rows)\n",
        "\n",
        "            if mask.any():\n",
        "                out = ch.loc[mask].copy()\n",
        "                out[\"row_id\"] = out[\"row_id\"].map(normalize)\n",
        "                kept.append(out)\n",
        "\n",
        "    if not kept:\n",
        "        st.warning(\"No signature rows matched the provided gene list.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    expr_small = pd.concat(kept, axis=0, ignore_index=False).drop_duplicates(subset=[\"row_id\"]).set_index(\"row_id\")\n",
        "\n",
        "    if row_mode == 'ensembl':\n",
        "        ensg2sym = {v:k for k,v in SYM2ENSG.items()}\n",
        "        expr_sym_small = expr_small.copy()\n",
        "        expr_sym_small.index = [ensg2sym.get(e, e) for e in expr_small.index]\n",
        "    else:\n",
        "        expr_sym_small = expr_small.copy()\n",
        "\n",
        "    expr_sym_small = expr_sym_small[~expr_sym_small.index.duplicated(keep=\"first\")]\n",
        "    return expr_sym_small\n",
        "\n",
        "\n",
        "def zscore_by_gene(expr_symbols: pd.DataFrame) -> pd.DataFrame:\n",
        "    E = expr_symbols.copy()\n",
        "    E = E.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    E = E.loc[~E.isna().all(axis=1)]\n",
        "    gene_means = E.mean(axis=1)\n",
        "    E = E.apply(lambda col: col.fillna(gene_means), axis=0)\n",
        "    mu = E.mean(axis=1)\n",
        "    sd = E.std(axis=1) + 1e-8\n",
        "    return (E.sub(mu, axis=0)).div(sd, axis=0)\n",
        "\n",
        "def pathway_scores(expr_symbols: pd.DataFrame, signatures: dict) -> pd.DataFrame:\n",
        "    \"\"\"Return pathways x samples (mean z across member genes present).\"\"\"\n",
        "    Z = zscore_by_gene(expr_symbols)\n",
        "    rows = []\n",
        "    for pw, genes in signatures.items():\n",
        "        present = [g for g in genes if g in Z.index]\n",
        "        if present:\n",
        "            s = Z.loc[present].mean(axis=0)\n",
        "        else:\n",
        "            s = pd.Series([np.nan]*Z.shape[1], index=Z.columns)\n",
        "        s.name = pw\n",
        "        rows.append(s)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def example_drug_panel():\n",
        "    return {\n",
        "        \"EGFRi\": [\"REACTOME_SIGNALING_BY_EGFR\"],\n",
        "        \"ALKi\":  [\"REACTOME_SIGNALING_BY_ALK\"],\n",
        "        \"MEKi\":  [\"REACTOME_MAPK1_MAPK3_SIGNALING\"],\n",
        "        \"PI3Ki\": [\"REACTOME_PI3K_AKT_SIGNALING\"],\n",
        "        \"mTORi\": [\"REACTOME_MTORC1_MEDIATED_SIGNALLING\"],\n",
        "        \"PD1i\":  [\"REACTOME_PD1_SIGNALING\"],\n",
        "        \"VEGFi\": [\"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\"],\n",
        "        \"FGFRi\": [\"REACTOME_SIGNALING_BY_FGFR\"],\n",
        "    }\n",
        "\n",
        "def patient_vector(P: pd.DataFrame, sample_id: str) -> pd.Series:\n",
        "    \"\"\"z-normalize pathways across samples; return vector for one patient.\"\"\"\n",
        "    z = (P - P.mean(axis=1).values.reshape(-1,1)) / (P.std(axis=1).values.reshape(-1,1) + 1e-8)\n",
        "    return z[sample_id].fillna(0.0)\n",
        "\n",
        "def drug_benefit_prior(z_path: pd.Series, panel: dict) -> pd.Series:\n",
        "    \"\"\"Aggregate pathway z's per drug (ReLU to emphasize upregulated pathways).\"\"\"\n",
        "    s = pd.Series({d: float(np.sum([max(z_path.get(p, 0.0), 0.0) for p in pws])) for d, pws in panel.items()})\n",
        "    return s / s.max() if s.max() > 0 else s\n",
        "\n",
        "def build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10):\n",
        "    \"\"\"Pairwise penalties for overlapping mechanisms; diagonal = sparsity.\"\"\"\n",
        "    K = len(drugs); R = np.zeros((K, K), dtype=float)\n",
        "    for i in range(K):\n",
        "        for j in range(i+1, K):\n",
        "            overlap = len(set(panel[drugs[i]]) & set(panel[drugs[j]]))\n",
        "            if overlap > 0:\n",
        "                R[i, j] = R[j, i] = base_overlap * overlap\n",
        "    for i in range(K):\n",
        "        R[i, i] += sparsity\n",
        "    return R\n",
        "\n",
        "def build_qubo(b_hat, R, lam=1.0):\n",
        "    \"\"\"QUBO: minimize x^T (lam R) x + q^T x  where q = -b̂ + diag(lam R).\"\"\"\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy()\n",
        "    diag = np.diag(Q).copy()\n",
        "    np.fill_diagonal(Q, 0.0)  # keep off-diagonal in Q\n",
        "    q += diag\n",
        "    return q, np.triu(Q, 1)\n",
        "\n",
        "def exact_qubo_solve(b_hat: np.ndarray, R: np.ndarray, lam: float = 1.0):\n",
        "    \"\"\"\n",
        "    Exact minimization of QUBO:\n",
        "      E(x) = x^T (lam R) x + q^T x, with q = -b̂ + diag(lam R)\n",
        "    We enumerate all bitstrings (2^K). Returns best bitstring (as 0/1 np array).\n",
        "    \"\"\"\n",
        "    K = len(b_hat)\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy() + np.diag(Q)\n",
        "    np.fill_diagonal(Q, 0.0)  # keep only off-diagonal in Q\n",
        "\n",
        "    best_e = np.inf\n",
        "    best_x = None\n",
        "    # vectorize partial precomputations\n",
        "    upper_idx = np.triu_indices(K, 1)\n",
        "    for mask in range(1 << K):\n",
        "        # build x from bits\n",
        "        x = np.fromiter(((mask >> i) & 1 for i in range(K)), dtype=np.int8)\n",
        "        # E = x^T Q x + q^T x, where Q is strictly upper-triangular mirrored\n",
        "        e = np.dot(q, x) + 2.0 * np.sum(Q[upper_idx] * (x[upper_idx[0]] * x[upper_idx[1]]))\n",
        "        if e < best_e:\n",
        "            best_e, best_x = e, x\n",
        "    return best_x, float(best_e)\n",
        "\n",
        "\n",
        "# Placeholder for CUDA-Q function if needed later, currently uses exact solve\n",
        "def try_cudaq_qaoa(h, J, p=2, shots=2048, max_iters=60, seed=7):\n",
        "    return None # Not implemented in this web app version\n",
        "\n",
        "# --- Main analysis function ---\n",
        "def run_analysis(uploaded_file, n_samples, lam_value):\n",
        "    \"\"\"\n",
        "    Runs the full analysis workflow for the uploaded data.\n",
        "    Returns pathway scores, drug selection summary, and patient report.\n",
        "    \"\"\"\n",
        "    SIGS = {\n",
        "        \"REACTOME_SIGNALING_BY_EGFR\": [\n",
        "            \"EGFR\",\"ERBB2\",\"ERBB3\",\"GRB2\",\"SOS1\",\"SHC1\",\"PTPN11\",\"KRAS\",\"NRAS\",\"HRAS\",\n",
        "            \"BRAF\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"GAB1\"\n",
        "        ],\n",
        "        \"REACTOME_SIGNALING_BY_ALK\": [\n",
        "            \"ALK\",\"EML4\",\"GRB2\",\"SHC1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"STAT3\",\"MAP2K1\",\"MAPK1\",\"MAPK3\"\n",
        "        ],\n",
        "        \"REACTOME_MAPK1_MAPK3_SIGNALING\": [\n",
        "            \"BRAF\",\"RAF1\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"DUSP6\",\"DUSP4\",\"FOS\",\"JUN\",\"EGFR\"\n",
        "        ],\n",
        "        \"REACTOME_PI3K_AKT_SIGNALING\": [\n",
        "            \"PIK3CA\",\"PIK3CB\",\"PIK3CD\",\"PIK3R1\",\"PIK3R2\",\"AKT1\",\"AKT2\",\"AKT3\",\"PTEN\",\"MTOR\",\"RHEB\"\n",
        "        ],\n",
        "        \"REACTOME_MTORC1_MEDIATED_SIGNALLING\": [\n",
        "            \"MTOR\",\"RPTOR\",\"MLST8\",\"RHEB\",\"TSC1\",\"TSC2\",\"EIF4EBP1\",\"RPS6KB1\",\"RPS6\"\n",
        "        ],\n",
        "        \"REACTOME_PD1_SIGNALING\": [\n",
        "            \"PDCD1\",\"CD274\",\"PDCD1LG2\",\"PDCD1LG2\",\"JAK1\",\"JAK2\",\"STAT1\",\"IFNG\",\"GZMB\",\"LAG3\",\"TIGIT\",\"CXCL9\",\"CXCL10\"\n",
        "        ],\n",
        "        \"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\": [\n",
        "            \"VEGFA\",\"KDR\",\"FLT1\",\"FLT4\",\"PTPRB\",\"PLCG1\",\"MAP2K1\",\"MAPK1\",\"NOS3\"\n",
        "        ],\n",
        "        \"REACTOME_SIGNALING_BY_FGFR\": [\n",
        "            \"FGFR1\",\"FGFR2\",\"FGFR3\",\"FGFR4\",\"FRS2\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"MAP2K1\",\"MAPK1\"\n",
        "        ]\n",
        "    }\n",
        "    SIG_GENES = sorted({g for gs in SIGS.values() for g in gs})\n",
        "    SYM2ENSG = {\n",
        "        \"EGFR\":\"ENSG00000146648\",\"ERBB2\":\"ENSG00000141736\",\"ERBB3\":\"ENSG00000065361\",\"GRB2\":\"ENSG00000177885\",\n",
        "        \"SOS1\":\"ENSG00000115904\",\"SHC1\":\"ENSG00000154639\",\"PTPN11\":\"ENSG00000179295\",\"KRAS\":\"ENSG00000133703\",\n",
        "        \"NRAS\":\"ENSG00000213281\",\"HRAS\":\"ENSG00000174775\",\"BRAF\":\"ENSG00000157764\",\"MAP2K1\":\"ENSG00000169032\",\n",
        "        \"MAP2K2\":\"ENSG00000126934\",\"MAPK1\":\"ENSG00000100030\",\"MAPK3\":\"ENSG00000102882\",\"PLCG1\":\"ENSG00000124181\",\n",
        "        \"PIK3CA\":\"ENSG00000121879\",\"PIK3R1\":\"ENSG00000145675\",\"AKT1\":\"ENSG00000142208\",\"AKT2\":\"ENSG00000105221\",\n",
        "        \"AKT3\":\"ENSG00000117020\",\"GAB1\":\"ENSG00000117676\",\n",
        "        \"ALK\":\"ENSG00000171094\",\"EML4\":\"ENSG00000143924\",\"STAT3\":\"ENSG00000168610\",\n",
        "        \"DUSP6\":\"ENSG00000139318\",\"DUSP4\":\"ENSG00000120875\",\"FOS\":\"ENSG00000170345\",\"JUN\":\"ENSG00000177606\",\"RAF1\":\"ENSG00000132155\",\n",
        "        \"PIK3CB\":\"ENSG00000119402\",\"PIK3CD\":\"ENSG00000171608\",\"PIK3R2\":\"ENSG00000189403\",\"PTEN\":\"ENSG00000171862\",\n",
        "        \"MTOR\":\"ENSG00000198793\",\"RHEB\":\"ENSG00000106615\",\n",
        "        \"RPTOR\":\"ENSG00000141564\",\"MLST8\":\"ENSG00000105705\",\"TSC1\":\"ENSG00000165699\",\"TSC2\":\"ENSG00000103197\",\n",
        "        \"EIF4EBP1\":\"ENSG00000187840\",\"RPS6KB1\":\"ENSG00000108443\",\"RPS6\":\"ENSG00000137154\",\n",
        "        \"PDCD1\":\"ENSG00000276977\",\"CD274\":\"ENSG00000120217\",\"PDCD1LG2\":\"ENSG00000197646\",\"JAK1\":\"ENSG00000162434\",\n",
        "        \"JAK2\":\"ENSG00000096968\",\"STAT1\":\"ENSG00000115415\",\"IFNG\":\"ENSG00000111537\",\"GZMB\":\"ENSG00000100453\",\n",
        "        \"LAG3\":\"ENSG00000089692\",\"TIGIT\":\"ENSG00000181847\",\"CXCL9\":\"ENSG00000138755\",\"CXCL10\":\"ENSG00000169245\",\n",
        "        \"VEGFA\":\"ENSG00000112715\",\"KDR\":\"ENSG00000128052\",\"FLT1\":\"ENSG00000102755\",\"FLT4\":\"ENSG00000037280\",\n",
        "        \"PTPRB\":\"ENSG00000160593\",\"NOS3\":\"ENSG00000164867\",\n",
        "        \"FGFR1\":\"ENSG00000077782\",\"FGFR2\":\"ENSG00000066468\",\"FGFR3\":\"ENSG00000068078\",\"FGFR4\":\"ENSG00000069535\",\n",
        "        \"FRS2\":\"ENSG00000181873\"\n",
        "    }\n",
        "\n",
        "    # Data Loading and Initial Processing\n",
        "    # Handle potential gzip compression and determine delimiter\n",
        "    file_content = uploaded_file.getvalue()\n",
        "    if uploaded_file.name.endswith('.gz'):\n",
        "        try:\n",
        "            gz_file = gzip.GzipFile(fileobj=io.BytesIO(file_content))\n",
        "            # Peek at the first line after decompression to determine delimiter\n",
        "            with io.TextIOWrapper(gz_file, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                header_peek = f.readline().rstrip(\"\\n\")\n",
        "                sep = ',' if ',' in header_peek.split() else '\\t'\n",
        "            # Reset gz_file for actual reading\n",
        "            gz_file.seek(0)\n",
        "            file_stream = io.BytesIO(gz_file.read())\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error reading gzipped file: {e}\")\n",
        "            return None, None, None, None, None, None\n",
        "    else:\n",
        "        # Determine delimiter for non-gzipped files\n",
        "        try:\n",
        "            header_peek = io.BytesIO(file_content).readline().decode('utf-8').rstrip(\"\\n\")\n",
        "            sep = ',' if ',' in header_peek.split() else '\\t'\n",
        "        except Exception as e:\n",
        "             st.error(f\"Error reading file header: {e}\")\n",
        "             return None, None, None, None, None, None\n",
        "        file_stream = io.BytesIO(file_content)\n",
        "\n",
        "    st.write(f\"Detected delimiter: '{sep}'\")\n",
        "\n",
        "    # Detect row mode\n",
        "    # Need a fresh file object for detect_row_mode\n",
        "    file_stream_for_detect = io.BytesIO(file_stream.getvalue())\n",
        "    row_mode = detect_row_mode(file_stream_for_detect)\n",
        "    st.write(f\"Detected row mode: {row_mode.upper()}\")\n",
        "\n",
        "    # Get header to identify sample columns and gene column\n",
        "    file_stream.seek(0) # Ensure stream is at the beginning\n",
        "    header_line = io.TextIOWrapper(file_stream, encoding=\"utf-8\", errors=\"replace\").readline().rstrip(\"\\n\")\n",
        "    cols = header_line.split(sep)\n",
        "    # Assuming the first column is the gene ID/symbol column\n",
        "    gene_col = cols[0]\n",
        "    # Assuming sample columns start from the second column\n",
        "    sample_cols_full = cols[1:]\n",
        "\n",
        "    # Select samples based on n_samples parameter\n",
        "    if n_samples > len(sample_cols_full):\n",
        "        st.warning(f\"Requested {n_samples} samples, but only {len(sample_cols_full)} available. Using all available samples.\")\n",
        "        selected_samples = sample_cols_full\n",
        "    else:\n",
        "        selected_samples = sample_cols_full[:n_samples]\n",
        "\n",
        "    st.write(f\"Processing {len(selected_samples)} samples.\")\n",
        "    st.write(f\"Using gene column: '{gene_col}'\")\n",
        "\n",
        "    # Stream-select rows and columns\n",
        "    # Need a fresh file object for stream_select_rows_columns\n",
        "    file_stream_for_select = io.BytesIO(file_stream.getvalue())\n",
        "    expr_sym_small = stream_select_rows_columns(file_stream_for_select, selected_samples, SIG_GENES, SYM2ENSG, row_mode, gene_col_name=gene_col, sep=sep)\n",
        "\n",
        "    if expr_sym_small.empty:\n",
        "        st.error(\"Failed to load expression data. Please check file format and contents.\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "    st.subheader(\"Expression Matrix (Subset)\")\n",
        "    st.write(\"Shape (genes x samples):\", expr_sym_small.shape)\n",
        "    st.dataframe(expr_sym_small.head())\n",
        "\n",
        "    # Pathway Scoring\n",
        "    st.subheader(\"Pathway Activity Scores\")\n",
        "    P = pathway_scores(expr_sym_small, SIGS)\n",
        "    st.write(\"Shape (pathways x samples):\", P.shape)\n",
        "    st.dataframe(P)\n",
        "\n",
        "    # QUBO Formulation and Drug Selection\n",
        "    st.subheader(\"Drug Selection Results\")\n",
        "    panel = example_drug_panel()\n",
        "    drugs = list(panel.keys())\n",
        "\n",
        "    all_sels = []\n",
        "    all_bhats = []\n",
        "    patient_reports_data = []\n",
        "\n",
        "    # Run analysis for each selected sample\n",
        "    if P.empty:\n",
        "        st.warning(\"No pathway scores computed. Cannot perform drug selection.\")\n",
        "        return P, pd.DataFrame(), pd.DataFrame(), [], drugs, panel\n",
        "\n",
        "    for sample_id in P.columns:\n",
        "        z_path = patient_vector(P, sample_id)\n",
        "        b_series = drug_benefit_prior(z_path, panel).reindex(drugs).fillna(0.0)\n",
        "        R = build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10)\n",
        "        b = b_series.to_numpy(float)\n",
        "        q, Q = build_qubo(b, R, lam=lam_value)\n",
        "\n",
        "        # Use exact solver for now\n",
        "        x_star, e_star = exact_qubo_solve(b, R, lam=lam_value)\n",
        "        sel = [drugs[i] for i, xi in enumerate(x_star) if xi == 1]\n",
        "        all_sels.append(sel)\n",
        "        all_bhats.append(b_series)\n",
        "\n",
        "        # Prepare data for patient report\n",
        "        top_pw = z_path.abs().sort_values(ascending=False).head(5)\n",
        "        patient_reports_data.append({\n",
        "            \"patient\": sample_id,\n",
        "            \"selected_drugs\": \", \".join(sel) if sel else \"(none)\",\n",
        "            \"top_pathways\": \"; \".join([f\"{p}:{z_path[p]:+.2f}\" for p in top_pw.index]),\n",
        "            **{f\"b̂.{d}\": float(b_series.get(d,0.0)) for d in drugs}\n",
        "        })\n",
        "\n",
        "    # Drug Selection Summary\n",
        "    st.subheader(\"Drug Selection Frequency\")\n",
        "    if not all_sels:\n",
        "        st.info(\"No drugs were selected for any patient.\")\n",
        "        freq_df = pd.DataFrame({\"drug\": drugs, \"frequency\": 0, \"frequency_pct\": 0.0})\n",
        "    else:\n",
        "        flat = [d for sel in all_sels for d in sel]\n",
        "        freq = Counter(flat)\n",
        "        freq_df = pd.DataFrame({\"drug\": drugs, \"frequency\": [freq[d] for d in drugs]})\n",
        "        freq_df[\"frequency_pct\"] = 100 * freq_df[\"frequency\"] / len(selected_samples)\n",
        "        freq_df = freq_df.sort_values(\"frequency_pct\", ascending=False).reset_index(drop=True)\n",
        "    st.dataframe(freq_df)\n",
        "\n",
        "    # Patient-level Report\n",
        "    st.subheader(\"Patient-level Report\")\n",
        "    patient_report_df = pd.DataFrame(patient_reports_data)\n",
        "    st.dataframe(patient_report_df)\n",
        "\n",
        "    return P, freq_df, patient_report_df, all_sels, drugs, panel\n",
        "\n",
        "# Function to create a download link\n",
        "def create_download_link(df, filename, text):\n",
        "    csv = df.to_csv(index=False)\n",
        "    b64 = base64.b64encode(csv.encode()).decode()\n",
        "    href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"{filename}\">{text}</a>'\n",
        "    return href\n",
        "\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "st.title(\"Digital Twin Workflow: Gene Expression to Drug Selection\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "This application demonstrates a workflow for identifying potential drug therapies\n",
        "for cancer patients based on their gene expression data, using pathway analysis\n",
        "and QUBO optimization.\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.header(\"Input Data\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload gene expression data (CSV, TSV, or gzipped)\", type=[\"csv\", \"tsv\", \"gz\"])\n",
        "\n",
        "st.sidebar.header(\"Parameters\")\n",
        "n_samples = st.sidebar.slider(\"Number of samples to process\", min_value=1, max_value=100, value=40, step=1)\n",
        "lam_value = st.sidebar.slider(\"Lambda (penalty) value for QUBO\", min_value=0.1, max_value=5.0, value=1.0, step=0.1)\n",
        "\n",
        "st.header(\"Analysis Results\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    if st.button(\"Run Analysis\"):\n",
        "        with st.spinner(f\"Running analysis for {n_samples} samples with lambda={lam_value}...\"):\n",
        "            pathway_scores_df, freq_df, patient_report_df, all_sels, drugs, panel = run_analysis(uploaded_file, n_samples, lam_value)\n",
        "\n",
        "            if pathway_scores_df is not None:\n",
        "                st.subheader(\"Visualizations\")\n",
        "\n",
        "                # Drug selection frequency barplot (Plotly)\n",
        "                if freq_df is not None and not freq_df.empty:\n",
        "                    try:\n",
        "                        fig1 = px.bar(freq_df, x=\"drug\", y=\"frequency_pct\", title=\"Drug selection stability across patients\")\n",
        "                        fig1.update_layout(xaxis_title=\"Drug\", yaxis_title=\"% patients selected\", xaxis_tickangle=-45)\n",
        "                        st.plotly_chart(fig1, use_container_width=True)\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not generate frequency barplot: {e}\")\n",
        "\n",
        "                # Co-selection heatmap and dendrogram (Plotly/Matplotlib - dendrogram is complex in Plotly)\n",
        "                if all_sels and drugs and panel:\n",
        "                    try:\n",
        "                        st.subheader(\"Co-selection Analysis\")\n",
        "                         # --- build co-selection counts ---\n",
        "                        co_mat = pd.DataFrame(0, index=drugs, columns=drugs, dtype=int)\n",
        "                        n_pat = max(1, len(all_sels))\n",
        "                        for sel in all_sels:\n",
        "                            uniq = list(dict.fromkeys(sel))\n",
        "                            for i in range(len(uniq)):\n",
        "                                for j in range(i, len(uniq)):\n",
        "                                    di, dj = uniq[i], uniq[j]\n",
        "                                    co_mat.loc[di, dj] += 1\n",
        "                                    if i != j:\n",
        "                                        co_mat.loc[dj, di] += 1\n",
        "\n",
        "                        # normalize to % of patients\n",
        "                        co_pct = co_mat / n_pat * 100.0\n",
        "                        st.write(\"Co-selection matrix (% of patients):\")\n",
        "                        st.dataframe(co_pct.round(1))\n",
        "\n",
        "                        # heatmap (Plotly)\n",
        "                        fig2 = px.imshow(co_pct, text_auto=True, color_continuous_scale='Blues',\n",
        "                                         title='Drug co-selection heatmap',\n",
        "                                         labels=dict(x=\"Drug\", y=\"Drug\", color=\"% patients co-selected\"))\n",
        "                        st.plotly_chart(fig2, use_container_width=True)\n",
        "\n",
        "\n",
        "                        # dendrogram (using Matplotlib for simplicity as Plotly dendrogram is complex)\n",
        "                        try:\n",
        "                             corr = np.corrcoef(co_pct.values)\n",
        "                             corr = np.clip(corr, -1.0, 1.0)\n",
        "                             dist = 1.0 - corr\n",
        "                             Z = linkage(dist, method=\"average\")\n",
        "\n",
        "                             fig3, ax3 = plt.subplots(figsize=(8,5))\n",
        "                             dendrogram(Z, labels=drugs, leaf_rotation=90, leaf_font_size=10,\n",
        "                                        color_threshold=0.7 * np.max(Z[:,2]), ax=ax3)\n",
        "                             ax3.set_title(\"Clustered dendrogram of drug co-selection\")\n",
        "                             ax3.set_ylabel(\"Distance (1 - correlation)\")\n",
        "                             plt.tight_layout()\n",
        "                             st.pyplot(fig3)\n",
        "                             plt.close(fig3) # Close figure\n",
        "\n",
        "                             # clustered heatmap (Plotly)\n",
        "                             order = leaves_list(Z)\n",
        "                             co_pct_ordered = co_pct.iloc[order, order]\n",
        "\n",
        "                             fig4 = px.imshow(co_pct_ordered, text_auto=True, color_continuous_scale='Blues',\n",
        "                                              title='Clustered heatmap of drug co-selection',\n",
        "                                              labels=dict(x=\"Drug\", y=\"Drug\", color=\"% patients co-selected\"))\n",
        "                             st.plotly_chart(fig4, use_container_width=True)\n",
        "                             st.write(\"Clustered co-selection matrix:\")\n",
        "                             st.dataframe(co_pct_ordered.round(1))\n",
        "\n",
        "                        except Exception as e:\n",
        "                             st.write(f\"Could not generate clustering plots (dendrogram/clustered heatmap): {e}\")\n",
        "\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not generate co-selection plots: {e}\")\n",
        "\n",
        "                # Selection size distribution histogram (Plotly)\n",
        "                if all_sels:\n",
        "                    try:\n",
        "                        sizes = [len(set(sel)) for sel in all_sels]\n",
        "                        size_hist = pd.Series(sizes).value_counts().sort_index()\n",
        "                        size_hist_df = size_hist.reset_index()\n",
        "                        size_hist_df.columns = [\"# drugs selected\", \"# patients\"]\n",
        "                        fig5 = px.bar(size_hist_df, x=\"# drugs selected\", y=\"# patients\",\n",
        "                                      title=\"Selection size distribution\")\n",
        "                        st.plotly_chart(fig5, use_container_width=True)\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not generate selection size histogram: {e}\")\n",
        "\n",
        "                # Synthetic Classification Validation (Plotly)\n",
        "                if all_sels and drugs and pathway_scores_df is not None and not pathway_scores_df.empty:\n",
        "                    try:\n",
        "                        st.subheader(\"Synthetic Classification Validation\")\n",
        "                        # 1) Construct patient-drug feature matrix\n",
        "                        N = len(all_sels)\n",
        "                        X = np.zeros((N, len(drugs)))\n",
        "                        for i, sel in enumerate(all_sels):\n",
        "                            for d in drugs: # Iterate through drugs to ensure correct indexing\n",
        "                                if d in sel:\n",
        "                                    X[i, drugs.index(d)] = 1\n",
        "\n",
        "                        # 2) Synthetic binary labels\n",
        "                        risk_scores = []\n",
        "                        # Ensure we only process samples that were successfully processed in run_analysis\n",
        "                        processed_sample_ids = pathway_scores_df.columns\n",
        "                        if len(processed_sample_ids) < N:\n",
        "                             st.warning(f\"Only {len(processed_sample_ids)} samples processed for classification.\")\n",
        "                             # Adjust N and X to match processed samples if necessary, or skip classification\n",
        "                             # For simplicity, skipping if mismatch or no processed samples\n",
        "                             if len(processed_sample_ids) == 0:\n",
        "                                  st.warning(\"No samples processed for synthetic classification.\")\n",
        "                                  raise ValueError(\"No processed samples\") # Trigger exception to skip\n",
        "                             N = len(processed_sample_ids)\n",
        "                             # Rebuild X with correct size if needed, but currently X is built from all_sels,\n",
        "                             # which should align with the number of processed samples if no errors occurred earlier.\n",
        "                             # The warning/exception above handles the case where processed_sample_ids is empty.\n",
        "                             # If there's a size mismatch while processed_sample_ids is not empty,\n",
        "                             # it indicates a more complex issue not easily fixable here, so we'll skip.\n",
        "                             st.warning(\"Skipping synthetic classification due to unexpected sample count mismatch.\")\n",
        "                             raise ValueError(\"Sample count mismatch\") # Trigger exception to skip\n",
        "\n",
        "\n",
        "                        for sid in processed_sample_ids:\n",
        "                             z_path = patient_vector(pathway_scores_df, sid)\n",
        "                             risk_scores.append(z_path.mean())\n",
        "                        risk_scores = np.array(risk_scores)\n",
        "\n",
        "                        # Handle case where all risk scores are the same\n",
        "                        if np.std(risk_scores) == 0:\n",
        "                             st.warning(\"Cannot generate synthetic labels: all pathway mean scores are the same.\")\n",
        "                             st.write(\"Synthetic labels (all 0):\")\n",
        "                             st.write(np.zeros(N, dtype=int))\n",
        "                        else:\n",
        "                             y = (risk_scores > np.median(risk_scores)).astype(int)\n",
        "                             st.write(\"Synthetic labels (0=low risk, 1=high risk):\")\n",
        "                             st.write(np.bincount(y))\n",
        "\n",
        "                             # 3) Train simple logistic regression\n",
        "                             if len(np.unique(y)) > 1: # Only train if there's more than one class\n",
        "                                 clf = LogisticRegression(max_iter=200)\n",
        "                                 clf.fit(X[:N], y) # Use adjusted N\n",
        "                                 y_pred = clf.predict_proba(X[:N])[:,1] # Use adjusted N\n",
        "\n",
        "                                 # 4) Metrics\n",
        "                                 roc_auc = roc_auc_score(y, y_pred)\n",
        "                                 pr_auc = average_precision_score(y, y_pred)\n",
        "\n",
        "                                 st.write(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "                                 st.write(f\"PR-AUC: {pr_auc:.3f}\")\n",
        "\n",
        "                                 # Optional barplot of learned coefficients (Plotly)\n",
        "                                 coefs = pd.Series(clf.coef_[0], index=drugs).sort_values()\n",
        "                                 coefs_df = coefs.reset_index()\n",
        "                                 coefs_df.columns = [\"Drug\", \"Weight\"]\n",
        "                                 fig6 = px.bar(coefs_df, y=\"Drug\", x=\"Weight\", orientation='h',\n",
        "                                               color=\"Weight\", color_continuous_scale=['salmon', 'steelblue'],\n",
        "                                               title=\"Drug selection coefficients (synthetic label prediction)\")\n",
        "                                 st.plotly_chart(fig6, use_container_width=True)\n",
        "\n",
        "                             else:\n",
        "                                 st.warning(\"Classification skipped: only one unique synthetic label class found.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not perform synthetic classification validation: {e}\")\n",
        "\n",
        "                # Downloadable reports\n",
        "                if patient_report_df is not None and not patient_report_df.empty:\n",
        "                    st.subheader(\"Download Reports\")\n",
        "                    st.markdown(create_download_link(patient_report_df, \"patient_report.csv\", \"Download Patient Report (CSV)\"), unsafe_allow_html=True)\n",
        "\n",
        "                if freq_df is not None and not freq_df.empty:\n",
        "                     st.markdown(create_download_link(freq_df, \"drug_frequency_report.csv\", \"Download Drug Frequency Report (CSV)\"), unsafe_allow_html=True)\n",
        "\n",
        "                st.success(\"Analysis complete!\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload a gene expression data file to begin.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bac0fac9"
      },
      "source": [
        "**Reasoning**:\n",
        "The Matplotlib dendrogram is still being generated. Convert the Matplotlib dendrogram to Plotly for consistency and interactivity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csJGENPu2x1F"
      },
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip\n",
        "import re\n",
        "from collections import Counter\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import io\n",
        "import base64\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Assume the core logic functions from the notebook are available here.\n",
        "# For demonstration, I'll include them directly. In a larger project,\n",
        "# these would be in a separate module.\n",
        "\n",
        "def norm_ensembl(x): return x.split('.',1)[0]\n",
        "def norm_symbol(x):  return re.sub(r'[^A-Za-z0-9_-]+','', x)\n",
        "\n",
        "def detect_row_mode(file_object, scan_rows=50000):\n",
        "    \"\"\"Detect row ID type (Ensembl vs symbol) from file-like object.\"\"\"\n",
        "    seen = Counter(); total = 0\n",
        "    # Use a TextIOWrapper to treat the bytes stream as text\n",
        "    # Ensure file_object is at the beginning\n",
        "    file_object.seek(0)\n",
        "    with io.TextIOWrapper(file_object, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        # Read the header line to skip it for row detection\n",
        "        header = f.readline()\n",
        "        # Use header=None because we already read the header\n",
        "        reader = pd.read_csv(f, sep=\"\\t\", chunksize=200_000, usecols=[0], dtype=str, header=None)\n",
        "        for ch in reader:\n",
        "            v = ch.iloc[:,0].astype(str)\n",
        "            vals = v.head(min(len(v), scan_rows-total)).tolist()\n",
        "            total += len(vals)\n",
        "            seen.update('ENSG' if x.startswith('ENSG') else 'OTHER' for x in vals)\n",
        "            if total >= scan_rows: break\n",
        "    ratio = seen['ENSG']/max(1,(seen['ENSG']+seen['OTHER']))\n",
        "    mode = 'ensembl' if ratio >= 0.6 else 'symbol'\n",
        "    return mode\n",
        "\n",
        "def stream_select_rows_columns(file_object, selected_samples, sig_genes, sym2ensg, row_mode, gene_col_name=\"Name\", sep='\\t'):\n",
        "    \"\"\"Stream-select rows and columns from file-like object.\"\"\"\n",
        "    if row_mode == 'ensembl':\n",
        "        target_rows = set(sym2ensg.get(g) for g in sig_genes if g in sym2ensg)\n",
        "        normalize = norm_ensembl\n",
        "    else:\n",
        "        target_rows = set(sig_genes)\n",
        "        normalize = norm_symbol\n",
        "\n",
        "    # Ensure file_object is at the beginning\n",
        "    file_object.seek(0)\n",
        "\n",
        "    with io.TextIOWrapper(file_object, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        # Read header to find the actual gene_col_name and sample columns\n",
        "        header = f.readline().rstrip(\"\\n\").split(sep)\n",
        "        try:\n",
        "            gene_col_idx = header.index(gene_col_name)\n",
        "        except ValueError:\n",
        "             # If gene_col_name not found, assume the first column is the gene column\n",
        "            gene_col_name = header[0]\n",
        "            gene_col_idx = 0\n",
        "\n",
        "        # Filter header to include only the gene column and selected sample columns\n",
        "        usecols_filter = [gene_col_name] + selected_samples\n",
        "\n",
        "        # Read data in chunks, selecting only necessary columns\n",
        "        # Use header=None as we've already read the header\n",
        "        reader = pd.read_csv(f, sep=sep, chunksize=50_000, dtype=str, header=None)\n",
        "        kept = []\n",
        "        for i, ch in enumerate(reader):\n",
        "            # Assign original header to the chunk\n",
        "            ch.columns = header\n",
        "            # Select only the columns we need\n",
        "            ch = ch[usecols_filter]\n",
        "\n",
        "            ch = ch.rename(columns={gene_col_name: \"row_id\"})\n",
        "\n",
        "            # Ensure 'row_id' column exists after renaming\n",
        "            if 'row_id' not in ch.columns:\n",
        "                 st.error(f\"Error: Could not find gene identifier column '{gene_col_name}' in the uploaded file.\")\n",
        "                 return pd.DataFrame() # Return empty DataFrame on error\n",
        "\n",
        "            ids = ch[\"row_id\"].astype(str).map(normalize)\n",
        "            mask = ids.isin(target_rows)\n",
        "\n",
        "            if mask.any():\n",
        "                out = ch.loc[mask].copy()\n",
        "                out[\"row_id\"] = out[\"row_id\"].map(normalize)\n",
        "                kept.append(out)\n",
        "\n",
        "    if not kept:\n",
        "        st.warning(\"No signature rows matched the provided gene list.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    expr_small = pd.concat(kept, axis=0, ignore_index=False).drop_duplicates(subset=[\"row_id\"]).set_index(\"row_id\")\n",
        "\n",
        "    if row_mode == 'ensembl':\n",
        "        ensg2sym = {v:k for k,v in SYM2ENSG.items()}\n",
        "        expr_sym_small = expr_small.copy()\n",
        "        expr_sym_small.index = [ensg2sym.get(e, e) for e in expr_small.index]\n",
        "    else:\n",
        "        expr_sym_small = expr_small.copy()\n",
        "\n",
        "    expr_sym_small = expr_sym_small[~expr_sym_small.index.duplicated(keep=\"first\")]\n",
        "    return expr_sym_small\n",
        "\n",
        "\n",
        "def zscore_by_gene(expr_symbols: pd.DataFrame) -> pd.DataFrame:\n",
        "    E = expr_symbols.copy()\n",
        "    E = E.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    E = E.loc[~E.isna().all(axis=1)]\n",
        "    gene_means = E.mean(axis=1)\n",
        "    E = E.apply(lambda col: col.fillna(gene_means), axis=0)\n",
        "    mu = E.mean(axis=1)\n",
        "    sd = E.std(axis=1) + 1e-8\n",
        "    return (E.sub(mu, axis=0)).div(sd, axis=0)\n",
        "\n",
        "def pathway_scores(expr_symbols: pd.DataFrame, signatures: dict) -> pd.DataFrame:\n",
        "    \"\"\"Return pathways x samples (mean z across member genes present).\"\"\"\n",
        "    Z = zscore_by_gene(expr_symbols)\n",
        "    rows = []\n",
        "    for pw, genes in signatures.items():\n",
        "        present = [g for g in genes if g in Z.index]\n",
        "        if present:\n",
        "            s = Z.loc[present].mean(axis=0)\n",
        "        else:\n",
        "            s = pd.Series([np.nan]*Z.shape[1], index=Z.columns)\n",
        "        s.name = pw\n",
        "        rows.append(s)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def example_drug_panel():\n",
        "    return {\n",
        "        \"EGFRi\": [\"REACTOME_SIGNALING_BY_EGFR\"],\n",
        "        \"ALKi\":  [\"REACTOME_SIGNALING_BY_ALK\"],\n",
        "        \"MEKi\":  [\"REACTOME_MAPK1_MAPK3_SIGNALING\"],\n",
        "        \"PI3Ki\": [\"REACTOME_PI3K_AKT_SIGNALING\"],\n",
        "        \"mTORi\": [\"REACTOME_MTORC1_MEDIATED_SIGNALLING\"],\n",
        "        \"PD1i\":  [\"REACTOME_PD1_SIGNALING\"],\n",
        "        \"VEGFi\": [\"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\"],\n",
        "        \"FGFRi\": [\"REACTOME_SIGNALING_BY_FGFR\"],\n",
        "    }\n",
        "\n",
        "def patient_vector(P: pd.DataFrame, sample_id: str) -> pd.Series:\n",
        "    \"\"\"z-normalize pathways across samples; return vector for one patient.\"\"\"\n",
        "    z = (P - P.mean(axis=1).values.reshape(-1,1)) / (P.std(axis=1).values.reshape(-1,1) + 1e-8)\n",
        "    return z[sample_id].fillna(0.0)\n",
        "\n",
        "def drug_benefit_prior(z_path: pd.Series, panel: dict) -> pd.Series:\n",
        "    \"\"\"Aggregate pathway z's per drug (ReLU to emphasize upregulated pathways).\"\"\"\n",
        "    s = pd.Series({d: float(np.sum([max(z_path.get(p, 0.0), 0.0) for p in pws])) for d, pws in panel.items()})\n",
        "    return s / s.max() if s.max() > 0 else s\n",
        "\n",
        "def build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10):\n",
        "    \"\"\"Pairwise penalties for overlapping mechanisms; diagonal = sparsity.\"\"\"\n",
        "    K = len(drugs); R = np.zeros((K, K), dtype=float)\n",
        "    for i in range(K):\n",
        "        for j in range(i+1, K):\n",
        "            overlap = len(set(panel[drugs[i]]) & set(panel[drugs[j]]))\n",
        "            if overlap > 0:\n",
        "                R[i, j] = R[j, i] = base_overlap * overlap\n",
        "    for i in range(K):\n",
        "        R[i, i] += sparsity\n",
        "    return R\n",
        "\n",
        "def build_qubo(b_hat, R, lam=1.0):\n",
        "    \"\"\"QUBO: minimize x^T (lam R) x + q^T x  where q = -b̂ + diag(lam R).\"\"\"\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy()\n",
        "    diag = np.diag(Q).copy()\n",
        "    np.fill_diagonal(Q, 0.0)  # keep off-diagonal in Q\n",
        "    q += diag\n",
        "    return q, np.triu(Q, 1)\n",
        "\n",
        "def exact_qubo_solve(b_hat: np.ndarray, R: np.ndarray, lam: float = 1.0):\n",
        "    \"\"\"\n",
        "    Exact minimization of QUBO:\n",
        "      E(x) = x^T (lam R) x + q^T x, with q = -b̂ + diag(lam R)\n",
        "    We enumerate all bitstrings (2^K). Returns best bitstring (as 0/1 np array).\n",
        "    \"\"\"\n",
        "    K = len(b_hat)\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy() + np.diag(Q)\n",
        "    np.fill_diagonal(Q, 0.0)  # keep only off-diagonal in Q\n",
        "\n",
        "    best_e = np.inf\n",
        "    best_x = None\n",
        "    # vectorize partial precomputations\n",
        "    upper_idx = np.triu_indices(K, 1)\n",
        "    for mask in range(1 << K):\n",
        "        # build x from bits\n",
        "        x = np.fromiter(((mask >> i) & 1 for i in range(K)), dtype=np.int8)\n",
        "        # E = x^T Q x + q^T x, where Q is strictly upper-triangular mirrored\n",
        "        e = np.dot(q, x) + 2.0 * np.sum(Q[upper_idx] * (x[upper_idx[0]] * x[upper_idx[1]]))\n",
        "        if e < best_e:\n",
        "            best_e, best_x = e, x\n",
        "    return best_x, float(best_e)\n",
        "\n",
        "\n",
        "# Placeholder for CUDA-Q function if needed later, currently uses exact solve\n",
        "def try_cudaq_qaoa(h, J, p=2, shots=2048, max_iters=60, seed=7):\n",
        "    return None # Not implemented in this web app version\n",
        "\n",
        "# --- Main analysis function ---\n",
        "def run_analysis(uploaded_file, n_samples, lam_value):\n",
        "    \"\"\"\n",
        "    Runs the full analysis workflow for the uploaded data.\n",
        "    Returns pathway scores, drug selection summary, and patient report.\n",
        "    \"\"\"\n",
        "    SIGS = {\n",
        "        \"REACTOME_SIGNALING_BY_EGFR\": [\n",
        "            \"EGFR\",\"ERBB2\",\"ERBB3\",\"GRB2\",\"SOS1\",\"SHC1\",\"PTPN11\",\"KRAS\",\"NRAS\",\"HRAS\",\n",
        "            \"BRAF\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"GAB1\"\n",
        "        ],\n",
        "        \"REACTOME_SIGNALING_BY_ALK\": [\n",
        "            \"ALK\",\"EML4\",\"GRB2\",\"SHC1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"STAT3\",\"MAP2K1\",\"MAPK1\",\"MAPK3\"\n",
        "        ],\n",
        "        \"REACTOME_MAPK1_MAPK3_SIGNALING\": [\n",
        "            \"BRAF\",\"RAF1\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"DUSP6\",\"DUSP4\",\"FOS\",\"JUN\",\"EGFR\"\n",
        "        ],\n",
        "        \"REACTOME_PI3K_AKT_SIGNALING\": [\n",
        "            \"PIK3CA\",\"PIK3CB\",\"PIK3CD\",\"PIK3R1\",\"PIK3R2\",\"AKT1\",\"AKT2\",\"AKT3\",\"PTEN\",\"MTOR\",\"RHEB\"\n",
        "        ],\n",
        "        \"REACTOME_MTORC1_MEDIATED_SIGNALLING\": [\n",
        "            \"MTOR\",\"RPTOR\",\"MLST8\",\"RHEB\",\"TSC1\",\"TSC2\",\"EIF4EBP1\",\"RPS6KB1\",\"RPS6\"\n",
        "        ],\n",
        "        \"REACTOME_PD1_SIGNALING\": [\n",
        "            \"PDCD1\",\"CD274\",\"PDCD1LG2\",\"PDCD1LG2\",\"JAK1\",\"JAK2\",\"STAT1\",\"IFNG\",\"GZMB\",\"LAG3\",\"TIGIT\",\"CXCL9\",\"CXCL10\"\n",
        "        ],\n",
        "        \"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\": [\n",
        "            \"VEGFA\",\"KDR\",\"FLT1\",\"FLT4\",\"PTPRB\",\"PLCG1\",\"MAP2K1\",\"MAPK1\",\"NOS3\"\n",
        "        ],\n",
        "        \"REACTOME_SIGNALING_BY_FGFR\": [\n",
        "            \"FGFR1\",\"FGFR2\",\"FGFR3\",\"FGFR4\",\"FRS2\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"MAP2K1\",\"MAPK1\"\n",
        "        ]\n",
        "    }\n",
        "    SIG_GENES = sorted({g for gs in SIGS.values() for g in gs})\n",
        "    SYM2ENSG = {\n",
        "        \"EGFR\":\"ENSG00000146648\",\"ERBB2\":\"ENSG00000141736\",\"ERBB3\":\"ENSG00000065361\",\"GRB2\":\"ENSG00000177885\",\n",
        "        \"SOS1\":\"ENSG00000115904\",\"SHC1\":\"ENSG00000154639\",\"PTPN11\":\"ENSG00000179295\",\"KRAS\":\"ENSG00000133703\",\n",
        "        \"NRAS\":\"ENSG00000213281\",\"HRAS\":\"ENSG00000174775\",\"BRAF\":\"ENSG00000157764\",\"MAP2K1\":\"ENSG00000169032\",\n",
        "        \"MAP2K2\":\"ENSG00000126934\",\"MAPK1\":\"ENSG00000100030\",\"MAPK3\":\"ENSG00000102882\",\"PLCG1\":\"ENSG00000124181\",\n",
        "        \"PIK3CA\":\"ENSG00000121879\",\"PIK3R1\":\"ENSG00000145675\",\"AKT1\":\"ENSG00000142208\",\"AKT2\":\"ENSG00000105221\",\n",
        "        \"AKT3\":\"ENSG00000117020\",\"GAB1\":\"ENSG00000117676\",\n",
        "        \"ALK\":\"ENSG00000171094\",\"EML4\":\"ENSG00000143924\",\"STAT3\":\"ENSG00000168610\",\n",
        "        \"DUSP6\":\"ENSG00000139318\",\"DUSP4\":\"ENSG00000120875\",\"FOS\":\"ENSG00000170345\",\"JUN\":\"ENSG00000177606\",\"RAF1\":\"ENSG00000132155\",\n",
        "        \"PIK3CB\":\"ENSG00000119402\",\"PIK3CD\":\"ENSG00000171608\",\"PIK3R2\":\"ENSG00000189403\",\"PTEN\":\"ENSG00000171862\",\n",
        "        \"MTOR\":\"ENSG00000198793\",\"RHEB\":\"ENSG00000106615\",\n",
        "        \"RPTOR\":\"ENSG00000141564\",\"MLST8\":\"ENSG00000105705\",\"TSC1\":\"ENSG00000165699\",\"TSC2\":\"ENSG00000103197\",\n",
        "        \"EIF4EBP1\":\"ENSG00000187840\",\"RPS6KB1\":\"ENSG00000108443\",\"RPS6\":\"ENSG00000137154\",\n",
        "        \"PDCD1\":\"ENSG00000276977\",\"CD274\":\"ENSG00000120217\",\"PDCD1LG2\":\"ENSG00000197646\",\"JAK1\":\"ENSG00000162434\",\n",
        "        \"JAK2\":\"ENSG00000096968\",\"STAT1\":\"ENSG00000115415\",\"IFNG\":\"ENSG00000111537\",\"GZMB\":\"ENSG00000100453\",\n",
        "        \"LAG3\":\"ENSG00000089692\",\"TIGIT\":\"ENSG00000181847\",\"CXCL9\":\"ENSG00000138755\",\"CXCL10\":\"ENSG00000169245\",\n",
        "        \"VEGFA\":\"ENSG00000112715\",\"KDR\":\"ENSG00000128052\",\"FLT1\":\"ENSG00000102755\",\"FLT4\":\"ENSG00000037280\",\n",
        "        \"PTPRB\":\"ENSG00000160593\",\"NOS3\":\"ENSG00000164867\",\n",
        "        \"FGFR1\":\"ENSG00000077782\",\"FGFR2\":\"ENSG00000066468\",\"FGFR3\":\"ENSG00000068078\",\"FGFR4\":\"ENSG00000069535\",\n",
        "        \"FRS2\":\"ENSG00000181873\"\n",
        "    }\n",
        "\n",
        "    # Data Loading and Initial Processing\n",
        "    # Handle potential gzip compression and determine delimiter\n",
        "    file_content = uploaded_file.getvalue()\n",
        "    if uploaded_file.name.endswith('.gz'):\n",
        "        try:\n",
        "            gz_file = gzip.GzipFile(fileobj=io.BytesIO(file_content))\n",
        "            # Peek at the first line after decompression to determine delimiter\n",
        "            with io.TextIOWrapper(gz_file, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                header_peek = f.readline().rstrip(\"\\n\")\n",
        "                sep = ',' if ',' in header_peek.split() else '\\t'\n",
        "            # Reset gz_file for actual reading\n",
        "            gz_file.seek(0)\n",
        "            file_stream = io.BytesIO(gz_file.read())\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error reading gzipped file: {e}\")\n",
        "            return None, None, None, None, None, None\n",
        "    else:\n",
        "        # Determine delimiter for non-gzipped files\n",
        "        try:\n",
        "            header_peek = io.BytesIO(file_content).readline().decode('utf-8').rstrip(\"\\n\")\n",
        "            sep = ',' if ',' in header_peek.split() else '\\t'\n",
        "        except Exception as e:\n",
        "             st.error(f\"Error reading file header: {e}\")\n",
        "             return None, None, None, None, None, None\n",
        "        file_stream = io.BytesIO(file_content)\n",
        "\n",
        "    st.write(f\"Detected delimiter: '{sep}'\")\n",
        "\n",
        "    # Detect row mode\n",
        "    # Need a fresh file object for detect_row_mode\n",
        "    file_stream_for_detect = io.BytesIO(file_stream.getvalue())\n",
        "    row_mode = detect_row_mode(file_stream_for_detect)\n",
        "    st.write(f\"Detected row mode: {row_mode.upper()}\")\n",
        "\n",
        "    # Get header to identify sample columns and gene column\n",
        "    file_stream.seek(0) # Ensure stream is at the beginning\n",
        "    header_line = io.TextIOWrapper(file_stream, encoding=\"utf-8\", errors=\"replace\").readline().rstrip(\"\\n\")\n",
        "    cols = header_line.split(sep)\n",
        "    # Assuming the first column is the gene ID/symbol column\n",
        "    gene_col = cols[0]\n",
        "    # Assuming sample columns start from the second column\n",
        "    sample_cols_full = cols[1:]\n",
        "\n",
        "    # Select samples based on n_samples parameter\n",
        "    if n_samples > len(sample_cols_full):\n",
        "        st.warning(f\"Requested {n_samples} samples, but only {len(sample_cols_full)} available. Using all available samples.\")\n",
        "        selected_samples = sample_cols_full\n",
        "    else:\n",
        "        selected_samples = sample_cols_full[:n_samples]\n",
        "\n",
        "    st.write(f\"Processing {len(selected_samples)} samples.\")\n",
        "    st.write(f\"Using gene column: '{gene_col}'\")\n",
        "\n",
        "    # Stream-select rows and columns\n",
        "    # Need a fresh file object for stream_select_rows_columns\n",
        "    file_stream_for_select = io.BytesIO(file_stream.getvalue())\n",
        "    expr_sym_small = stream_select_rows_columns(file_stream_for_select, selected_samples, SIG_GENES, SYM2ENSG, row_mode, gene_col_name=gene_col, sep=sep)\n",
        "\n",
        "    if expr_sym_small.empty:\n",
        "        st.error(\"Failed to load expression data. Please check file format and contents.\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "    st.subheader(\"Expression Matrix (Subset)\")\n",
        "    st.write(\"Shape (genes x samples):\", expr_sym_small.shape)\n",
        "    st.dataframe(expr_sym_small.head())\n",
        "\n",
        "    # Pathway Scoring\n",
        "    st.subheader(\"Pathway Activity Scores\")\n",
        "    P = pathway_scores(expr_sym_small, SIGS)\n",
        "    st.write(\"Shape (pathways x samples):\", P.shape)\n",
        "    st.dataframe(P)\n",
        "\n",
        "    # QUBO Formulation and Drug Selection\n",
        "    st.subheader(\"Drug Selection Results\")\n",
        "    panel = example_drug_panel()\n",
        "    drugs = list(panel.keys())\n",
        "\n",
        "    all_sels = []\n",
        "    all_bhats = []\n",
        "    patient_reports_data = []\n",
        "\n",
        "    # Run analysis for each selected sample\n",
        "    if P.empty:\n",
        "        st.warning(\"No pathway scores computed. Cannot perform drug selection.\")\n",
        "        return P, pd.DataFrame(), pd.DataFrame(), [], drugs, panel\n",
        "\n",
        "    for sample_id in P.columns:\n",
        "        z_path = patient_vector(P, sample_id)\n",
        "        b_series = drug_benefit_prior(z_path, panel).reindex(drugs).fillna(0.0)\n",
        "        R = build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10)\n",
        "        b = b_series.to_numpy(float)\n",
        "        q, Q = build_qubo(b, R, lam=lam_value)\n",
        "\n",
        "        # Use exact solver for now\n",
        "        x_star, e_star = exact_qubo_solve(b, R, lam=lam_value)\n",
        "        sel = [drugs[i] for i, xi in enumerate(x_star) if xi == 1]\n",
        "        all_sels.append(sel)\n",
        "        all_bhats.append(b_series)\n",
        "\n",
        "        # Prepare data for patient report\n",
        "        top_pw = z_path.abs().sort_values(ascending=False).head(5)\n",
        "        patient_reports_data.append({\n",
        "            \"patient\": sample_id,\n",
        "            \"selected_drugs\": \", \".join(sel) if sel else \"(none)\",\n",
        "            \"top_pathways\": \"; \".join([f\"{p}:{z_path[p]:+.2f}\" for p in top_pw.index]),\n",
        "            **{f\"b̂.{d}\": float(b_series.get(d,0.0)) for d in drugs}\n",
        "        })\n",
        "\n",
        "    # Drug Selection Summary\n",
        "    st.subheader(\"Drug Selection Frequency\")\n",
        "    if not all_sels:\n",
        "        st.info(\"No drugs were selected for any patient.\")\n",
        "        freq_df = pd.DataFrame({\"drug\": drugs, \"frequency\": 0, \"frequency_pct\": 0.0})\n",
        "    else:\n",
        "        flat = [d for sel in all_sels for d in sel]\n",
        "        freq = Counter(flat)\n",
        "        freq_df = pd.DataFrame({\"drug\": drugs, \"frequency\": [freq[d] for d in drugs]})\n",
        "        freq_df[\"frequency_pct\"] = 100 * freq_df[\"frequency\"] / len(selected_samples)\n",
        "        freq_df = freq_df.sort_values(\"frequency_pct\", ascending=False).reset_index(drop=True)\n",
        "    st.dataframe(freq_df)\n",
        "\n",
        "    # Patient-level Report\n",
        "    st.subheader(\"Patient-level Report\")\n",
        "    patient_report_df = pd.DataFrame(patient_reports_data)\n",
        "    st.dataframe(patient_report_df)\n",
        "\n",
        "    return P, freq_df, patient_report_df, all_sels, drugs, panel\n",
        "\n",
        "# Function to create a download link\n",
        "def create_download_link(df, filename, text):\n",
        "    csv = df.to_csv(index=False)\n",
        "    b64 = base64.b64encode(csv.encode()).decode()\n",
        "    href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"{filename}\">{text}</a>'\n",
        "    return href\n",
        "\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "st.title(\"Digital Twin Workflow: Gene Expression to Drug Selection\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "This application demonstrates a workflow for identifying potential drug therapies\n",
        "for cancer patients based on their gene expression data, using pathway analysis\n",
        "and QUBO optimization.\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.header(\"Input Data\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload gene expression data (CSV, TSV, or gzipped)\", type=[\"csv\", \"tsv\", \"gz\"])\n",
        "\n",
        "st.sidebar.header(\"Parameters\")\n",
        "n_samples = st.sidebar.slider(\"Number of samples to process\", min_value=1, max_value=100, value=40, step=1)\n",
        "lam_value = st.sidebar.slider(\"Lambda (penalty) value for QUBO\", min_value=0.1, max_value=5.0, value=1.0, step=0.1)\n",
        "\n",
        "st.header(\"Analysis Results\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    if st.button(\"Run Analysis\"):\n",
        "        with st.spinner(f\"Running analysis for {n_samples} samples with lambda={lam_value}...\"):\n",
        "            pathway_scores_df, freq_df, patient_report_df, all_sels, drugs, panel = run_analysis(uploaded_file, n_samples, lam_value)\n",
        "\n",
        "            if pathway_scores_df is not None:\n",
        "                st.subheader(\"Visualizations\")\n",
        "\n",
        "                # Drug selection frequency barplot (Plotly)\n",
        "                if freq_df is not None and not freq_df.empty:\n",
        "                    try:\n",
        "                        fig1 = px.bar(freq_df, x=\"drug\", y=\"frequency_pct\", title=\"Drug selection stability across patients\")\n",
        "                        fig1.update_layout(xaxis_title=\"Drug\", yaxis_title=\"% patients selected\", xaxis_tickangle=-45)\n",
        "                        st.plotly_chart(fig1, use_container_width=True)\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not generate frequency barplot: {e}\")\n",
        "\n",
        "                # Co-selection heatmap and dendrogram (Plotly)\n",
        "                if all_sels and drugs and panel:\n",
        "                    try:\n",
        "                        st.subheader(\"Co-selection Analysis\")\n",
        "                         # --- build co-selection counts ---\n",
        "                        co_mat = pd.DataFrame(0, index=drugs, columns=drugs, dtype=int)\n",
        "                        n_pat = max(1, len(all_sels))\n",
        "                        for sel in all_sels:\n",
        "                            uniq = list(dict.fromkeys(sel))\n",
        "                            for i in range(len(uniq)):\n",
        "                                for j in range(i, len(uniq)):\n",
        "                                    di, dj = uniq[i], uniq[j]\n",
        "                                    co_mat.loc[di, dj] += 1\n",
        "                                    if i != j:\n",
        "                                        co_mat.loc[dj, di] += 1\n",
        "\n",
        "                        # normalize to % of patients\n",
        "                        co_pct = co_mat / n_pat * 100.0\n",
        "                        st.write(\"Co-selection matrix (% of patients):\")\n",
        "                        st.dataframe(co_pct.round(1))\n",
        "\n",
        "                        # heatmap (Plotly)\n",
        "                        fig2 = px.imshow(co_pct, text_auto=True, color_continuous_scale='Blues',\n",
        "                                         title='Drug co-selection heatmap',\n",
        "                                         labels=dict(x=\"Drug\", y=\"Drug\", color=\"% patients co-selected\"))\n",
        "                        st.plotly_chart(fig2, use_container_width=True)\n",
        "\n",
        "\n",
        "                        # dendrogram (Plotly)\n",
        "                        try:\n",
        "                            corr = np.corrcoef(co_pct.values)\n",
        "                            corr = np.clip(corr, -1.0, 1.0)\n",
        "                            dist = 1.0 - corr\n",
        "                            Z = linkage(dist, method=\"average\")\n",
        "\n",
        "                            # Create Plotly Dendrogram\n",
        "                            fig3 = go.Figure(data=go.Heatmap(\n",
        "                                z=co_pct.values,\n",
        "                                x=co_pct.columns,\n",
        "                                y=co_pct.index,\n",
        "                                colorscale='Blues',\n",
        "                                colorbar=dict(title=\"% patients co-selected\")\n",
        "                            ))\n",
        "\n",
        "                            # Add dendrogram using Plotly figure factory (requires plotly-scientific)\n",
        "                            # from plotly.figure_factory import create_dendrogram\n",
        "                            # fig3 = create_dendrogram(co_pct.values, orientation='bottom', labels=co_pct.index)\n",
        "                            # fig3.update_layout(title='Clustered dendrogram of drug co-selection',\n",
        "                            #                    xaxis_title=\"Drug\", yaxis_title=\"Distance (1 - correlation)\")\n",
        "                            # st.plotly_chart(fig3, use_container_width=True)\n",
        "\n",
        "                            # Manual Plotly Dendrogram creation (more complex, using Matplotlib for now)\n",
        "                            # Reverting to Matplotlib dendrogram as Plotly dendrogram requires specific data format or fig_factory\n",
        "                            fig3_mpl, ax3_mpl = plt.subplots(figsize=(8,5))\n",
        "                            dendrogram(Z, labels=drugs, leaf_rotation=90, leaf_font_size=10,\n",
        "                                       color_threshold=0.7 * np.max(Z[:,2]), ax=ax3_mpl)\n",
        "                            ax3_mpl.set_title(\"Clustered dendrogram of drug co-selection (Matplotlib)\")\n",
        "                            ax3_mpl.set_ylabel(\"Distance (1 - correlation)\")\n",
        "                            plt.tight_layout()\n",
        "                            st.pyplot(fig3_mpl)\n",
        "                            plt.close(fig3_mpl) # Close figure\n",
        "\n",
        "\n",
        "                            # clustered heatmap (Plotly)\n",
        "                            order = leaves_list(Z)\n",
        "                            co_pct_ordered = co_pct.iloc[order, order]\n",
        "\n",
        "                            fig4 = px.imshow(co_pct_ordered, text_auto=True, color_continuous_scale='Blues',\n",
        "                                              title='Clustered heatmap of drug co-selection',\n",
        "                                              labels=dict(x=\"Drug\", y=\"Drug\", color=\"% patients co-selected\"))\n",
        "                            st.plotly_chart(fig4, use_container_width=True)\n",
        "                            st.write(\"Clustered co-selection matrix:\")\n",
        "                            st.dataframe(co_pct_ordered.round(1))\n",
        "\n",
        "                        except Exception as e:\n",
        "                             st.write(f\"Could not generate clustering plots (dendrogram/clustered heatmap): {e}\")\n",
        "\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not generate co-selection plots: {e}\")\n",
        "\n",
        "                # Selection size distribution histogram (Plotly)\n",
        "                if all_sels:\n",
        "                    try:\n",
        "                        sizes = [len(set(sel)) for sel in all_sels]\n",
        "                        size_hist = pd.Series(sizes).value_counts().sort_index()\n",
        "                        size_hist_df = size_hist.reset_index()\n",
        "                        size_hist_df.columns = [\"# drugs selected\", \"# patients\"]\n",
        "                        fig5 = px.bar(size_hist_df, x=\"# drugs selected\", y=\"# patients\",\n",
        "                                      title=\"Selection size distribution\")\n",
        "                        fig5.update_layout(xaxis=dict(tickmode='linear')) # Ensure all integer ticks are shown\n",
        "                        st.plotly_chart(fig5, use_container_width=True)\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not generate selection size histogram: {e}\")\n",
        "\n",
        "                # Synthetic Classification Validation (Plotly)\n",
        "                if all_sels and drugs and pathway_scores_df is not None and not pathway_scores_df.empty:\n",
        "                    try:\n",
        "                        st.subheader(\"Synthetic Classification Validation\")\n",
        "                        # 1) Construct patient-drug feature matrix\n",
        "                        N = len(all_sels)\n",
        "                        X = np.zeros((N, len(drugs)))\n",
        "                        for i, sel in enumerate(all_sels):\n",
        "                            for d in drugs: # Iterate through drugs to ensure correct indexing\n",
        "                                if d in sel:\n",
        "                                    X[i, drugs.index(d)] = 1\n",
        "\n",
        "                        # 2) Synthetic binary labels\n",
        "                        risk_scores = []\n",
        "                        # Ensure we only process samples that were successfully processed in run_analysis\n",
        "                        processed_sample_ids = pathway_scores_df.columns\n",
        "                        if len(processed_sample_ids) < N:\n",
        "                             st.warning(f\"Only {len(processed_sample_ids)} samples processed for classification.\")\n",
        "                             # Adjust N and X to match processed samples if necessary, or skip classification\n",
        "                             # For simplicity, skipping if mismatch or no processed samples\n",
        "                             if len(processed_sample_ids) == 0:\n",
        "                                  st.warning(\"No samples processed for synthetic classification.\")\n",
        "                                  raise ValueError(\"No processed samples\") # Trigger exception to skip\n",
        "                             N = len(processed_sample_ids)\n",
        "                             # Rebuild X with correct size if needed, but currently X is built from all_sels,\n",
        "                             # which should align with the number of processed samples if no errors occurred earlier.\n",
        "                             # The warning/exception above handles the case where processed_sample_ids is empty.\n",
        "                             # If there's a size mismatch while processed_sample_ids is not empty,\n",
        "                             # it indicates a more complex issue not easily fixable here, so we'll skip.\n",
        "                             st.warning(\"Skipping synthetic classification due to unexpected sample count mismatch.\")\n",
        "                             raise ValueError(\"Sample count mismatch\") # Trigger exception to skip\n",
        "\n",
        "\n",
        "                        for sid in processed_sample_ids:\n",
        "                             z_path = patient_vector(pathway_scores_df, sid)\n",
        "                             risk_scores.append(z_path.mean())\n",
        "                        risk_scores = np.array(risk_scores)\n",
        "\n",
        "                        # Handle case where all risk scores are the same\n",
        "                        if np.std(risk_scores) == 0:\n",
        "                             st.warning(\"Cannot generate synthetic labels: all pathway mean scores are the same.\")\n",
        "                             st.write(\"Synthetic labels (all 0):\")\n",
        "                             st.write(np.zeros(N, dtype=int))\n",
        "                        else:\n",
        "                             y = (risk_scores > np.median(risk_scores)).astype(int)\n",
        "                             st.write(\"Synthetic labels (0=low risk, 1=high risk):\")\n",
        "                             st.write(np.bincount(y))\n",
        "\n",
        "                             # 3) Train simple logistic regression\n",
        "                             if len(np.unique(y)) > 1: # Only train if there's more than one class\n",
        "                                 clf = LogisticRegression(max_iter=200)\n",
        "                                 clf.fit(X[:N], y) # Use adjusted N\n",
        "                                 y_pred = clf.predict_proba(X[:N])[:,1] # Use adjusted N\n",
        "\n",
        "                                 # 4) Metrics\n",
        "                                 roc_auc = roc_auc_score(y, y_pred)\n",
        "                                 pr_auc = average_precision_score(y, y_pred)\n",
        "\n",
        "                                 st.write(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "                                 st.write(f\"PR-AUC: {pr_auc:.3f}\")\n",
        "\n",
        "                                 # Optional barplot of learned coefficients (Plotly)\n",
        "                                 coefs = pd.Series(clf.coef_[0], index=drugs).sort_values()\n",
        "                                 coefs_df = coefs.reset_index()\n",
        "                                 coefs_df.columns = [\"Drug\", \"Weight\"]\n",
        "                                 fig6 = px.bar(coefs_df, y=\"Drug\", x=\"Weight\", orientation='h',\n",
        "                                               color=\"Weight\", color_continuous_scale=['salmon', 'steelblue'],\n",
        "                                               title=\"Drug selection coefficients (synthetic label prediction)\")\n",
        "                                 st.plotly_chart(fig6, use_container_width=True)\n",
        "\n",
        "                             else:\n",
        "                                 st.warning(\"Classification skipped: only one unique synthetic label class found.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not perform synthetic classification validation: {e}\")\n",
        "\n",
        "                # Downloadable reports\n",
        "                if patient_report_df is not None and not patient_report_df.empty:\n",
        "                    st.subheader(\"Download Reports\")\n",
        "                    st.markdown(create_download_link(patient_report_df, \"patient_report.csv\", \"Download Patient Report (CSV)\"), unsafe_allow_html=True)\n",
        "\n",
        "                if freq_df is not None and not freq_df.empty:\n",
        "                     st.markdown(create_download_link(freq_df, \"drug_frequency_report.csv\", \"Download Drug Frequency Report (CSV)\"), unsafe_allow_html=True)\n",
        "\n",
        "                st.success(\"Analysis complete!\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload a gene expression data file to begin.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68697d51"
      },
      "source": [
        "## Consider scalability and performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the current implementation for scalability and performance limitations and propose potential optimizations or alternative approaches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e844ad29"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the current implementation for scalability and performance limitations and propose potential optimizations or alternative approaches by reviewing the code and identifying bottlenecks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3adfcb4b"
      },
      "source": [
        "# Reviewing the code for scalability and performance limitations.\n",
        "\n",
        "# 1. Data Loading and Initial Processing:\n",
        "# - Reading the entire file into memory with uploaded_file.getvalue() can be a bottleneck for very large files.\n",
        "# - io.BytesIO(gz_file.read()) for gzipped files also reads the entire decompressed content into memory.\n",
        "# - The chunking in stream_select_rows_columns helps with row selection but still processes chunks in memory.\n",
        "# - The approach of reading the header twice (once for mode detection, once for column names) involves redundant file reads.\n",
        "\n",
        "# 2. Pathway Scoring:\n",
        "# - zscore_by_gene copies the DataFrame. For large DataFrames, this can be memory intensive.\n",
        "# - Mean imputation and z-scoring are done in memory.\n",
        "# - pathway_scores iterates through pathways and genes, which is efficient for the current small SIGS but could be slower with many pathways or very large gene lists per pathway.\n",
        "\n",
        "# 3. QUBO Formulation and Solving:\n",
        "# - exact_qubo_solve enumerates all 2^K possible drug combinations. K is the number of drugs.\n",
        "# - For K=8 (current panel size), 2^8 = 256 combinations, which is very fast.\n",
        "# - If the drug panel size increases significantly (e.g., K=25), 2^25 is huge, making the exact solver infeasible. This is a major scalability limitation.\n",
        "# - The QUBO matrix R and vector q are built in memory. For a very large number of drugs, these could become large, but the primary bottleneck is the 2^K enumeration.\n",
        "\n",
        "# 4. Multi-patient Analysis:\n",
        "# - The analysis is run sequentially for each patient. This is fine for a small number of samples but can be slow for thousands or millions of samples.\n",
        "# - Storing all_sels and all_bhats in memory for all patients can consume significant memory for a large cohort.\n",
        "\n",
        "# 5. Visualizations:\n",
        "# - Generating plots using Plotly or Matplotlib is generally efficient for the current scale.\n",
        "# - Creating intermediate DataFrames for plotting (e.g., for co-selection) adds memory overhead.\n",
        "\n",
        "# Proposed Optimizations and Alternative Approaches:\n",
        "\n",
        "# 1. Data Loading:\n",
        "# - For very large files, consider using Dask DataFrames or iterating over chunks without reading the entire file into memory first.\n",
        "# - Optimize header reading to minimize file pointer manipulation or redundant reads.\n",
        "\n",
        "# 2. Pathway Scoring:\n",
        "# - Explore libraries like Dask or Vaex for out-of-core processing if the expression matrix becomes too large for memory.\n",
        "# - If using pandas, ensure operations are vectorized where possible (already done for z-scoring).\n",
        "\n",
        "# 3. QUBO Solving:\n",
        "# - For larger drug panels (K > ~20-25), the exact solver is not scalable.\n",
        "# - Alternative solvers are needed:\n",
        "#     - Approximate classical solvers (e.g., simulated annealing, genetic algorithms, specialized QUBO solvers like those in dimod or PyQUBO).\n",
        "#     - Quantum-inspired optimization (QIO) algorithms on classical hardware (e.g., Fujitsu Digital Annealer, Hitachi CMOS Annealing).\n",
        "#     - Quantum computing (QAOA on actual quantum hardware or simulators, as hinted in the original notebook, but currently not used in the web app).\n",
        "# - The choice of solver depends on the desired scale of the drug panel and available hardware/software.\n",
        "\n",
        "# 4. Multi-patient Analysis:\n",
        "# - Parallelize the patient analysis loop using libraries like concurrent.futures or joblib.\n",
        "# - If processing a massive cohort, consider distributed computing frameworks like Spark or Dask.\n",
        "# - Process patient results incrementally or store them in a database instead of keeping all results in memory.\n",
        "\n",
        "# 5. Memory Management:\n",
        "# - Explicitly delete large objects (DataFrames, arrays) that are no longer needed.\n",
        "# - Use more memory-efficient data types if possible (e.g., lower precision floats if appropriate).\n",
        "\n",
        "# 6. Code Optimization:\n",
        "# - Profile the code to identify the exact bottlenecks for typical dataset sizes.\n",
        "# - Optimize specific functions identified as slow (e.g., the exact_qubo_solve loop for slightly larger K before it becomes completely infeasible).\n",
        "\n",
        "# Documenting limitations and solutions:\n",
        "print(\"\\n--- Scalability and Performance Limitations ---\")\n",
        "print(\"1. Data Loading: Reading entire large files into memory (especially gzipped) can cause OutOfMemory errors.\")\n",
        "print(\"2. Exact QUBO Solver: Scales exponentially with the number of drugs (2^K). Infeasible for K > ~20-25.\")\n",
        "print(\"3. Multi-patient Processing: Sequential processing can be slow for large patient cohorts.\")\n",
        "print(\"4. Memory Usage: Storing full expression matrix, pathway scores, and all patient results in memory can be limiting.\")\n",
        "\n",
        "print(\"\\n--- Proposed Optimizations and Alternative Approaches ---\")\n",
        "print(\"1. Data Loading: Use Dask or chunked processing without full in-memory read for large files.\")\n",
        "print(\"2. QUBO Solver: Implement approximate classical solvers (simulated annealing, genetic algorithms) or QIO solvers for larger drug panels.\")\n",
        "print(\"3. Multi-patient Processing: Parallelize the patient loop (concurrent.futures, joblib) or use distributed computing (Spark, Dask) for large cohorts.\")\n",
        "print(\"4. Memory Management: Use Dask/Vaex for out-of-core processing, process results incrementally, explicitly delete objects.\")\n",
        "print(\"5. Code Optimization: Profile and optimize specific functions; use vectorized operations.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e4fa9e6"
      },
      "source": [
        "## Deploy the web application\n",
        "\n",
        "### Subtask:\n",
        "Choose a hosting platform (e.g., Heroku, AWS, Google Cloud Platform) and deploy the web application so that it can be accessed by users.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bde309da"
      },
      "source": [
        "**Reasoning**:\n",
        "The current subtask is to deploy the Streamlit application. Streamlit Community Cloud is the simplest option for deploying a Streamlit app directly from a GitHub repository. This involves creating a `requirements.txt` file, ensuring the Streamlit code is in a Python file (e.g., `app.py`), setting up a Git repository, and deploying via the Streamlit Community Cloud dashboard. I will create the `requirements.txt` and `app.py` files in the current environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFwK57Bw3b6z"
      },
      "source": [
        "# Save the Streamlit application code to app.py\n",
        "# Regenerate the string content, being careful with escapes and quotes.\n",
        "streamlit_code = \"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip\n",
        "import re\n",
        "from collections import Counter\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import io\n",
        "import base64\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Assume the core logic functions from the notebook are available here.\n",
        "# For demonstration, I'll include them directly. In a larger project,\n",
        "# these would be in a separate module.\n",
        "\n",
        "def norm_ensembl(x): return x.split('.',1)[0]\n",
        "def norm_symbol(x):  return re.sub(r'[^A-Za-z0-9_-]+','', x)\n",
        "\n",
        "def detect_row_mode(file_object, scan_rows=50000):\n",
        "    \\\\\"\\\\\"\\\\\"Detect row ID type (Ensembl vs symbol) from file-like object.\\\\\"\\\\\"\\\\\"\n",
        "    seen = Counter(); total = 0\n",
        "    # Use a TextIOWrapper to treat the bytes stream as text\n",
        "    # Ensure file_object is at the beginning\n",
        "    file_object.seek(0)\n",
        "    with io.TextIOWrapper(file_object, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        # Read the header line to skip it for row detection\n",
        "        header = f.readline()\n",
        "        # Use header=None because we already read the header\n",
        "        reader = pd.read_csv(f, sep=\"\\\\t\", chunksize=200_000, usecols=[0], dtype=str, header=None)\n",
        "        for ch in reader:\n",
        "            v = ch.iloc[:,0].astype(str)\n",
        "            vals = v.head(min(len(v), scan_rows-total)).tolist()\n",
        "            total += len(vals)\n",
        "            seen.update('ENSG' if x.startswith('ENSG') else 'OTHER' for x in vals)\n",
        "            if total >= scan_rows: break\n",
        "    ratio = seen['ENSG']/max(1,(seen['ENSG']+seen['OTHER']))\n",
        "    mode = 'ensembl' if ratio >= 0.6 else 'symbol'\n",
        "    return mode\n",
        "\n",
        "def stream_select_rows_columns(file_object, selected_samples, sig_genes, sym2ensg, row_mode, gene_col_name=\"Name\", sep='\\\\t'):\n",
        "    \\\\\"\\\\\"\\\\\"Stream-select rows and columns from file-like object.\\\\\"\\\\\"\\\\\"\n",
        "    if row_mode == 'ensembl':\n",
        "        target_rows = set(sym2ensg.get(g) for g in sig_genes if g in sym2ensg)\n",
        "        normalize = norm_ensembl\n",
        "    else:\n",
        "        target_rows = set(sig_genes)\n",
        "        normalize = norm_symbol\n",
        "\n",
        "    # Ensure file_object is at the beginning\n",
        "    file_object.seek(0)\n",
        "\n",
        "    with io.TextIOWrapper(file_object, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        # Read header to find the actual gene_col_name and sample columns\n",
        "        header = f.readline().rstrip(\"\\\\\\\\n\").split(sep)\n",
        "        try:\n",
        "            gene_col_idx = header.index(gene_col_name)\n",
        "        except ValueError:\n",
        "             # If gene_col_name not found, assume the first column is the gene column\n",
        "            gene_col_name = header[0]\n",
        "            gene_col_idx = 0\n",
        "\n",
        "        # Filter header to include only the gene column and selected sample columns\n",
        "        usecols_filter = [gene_col_name] + selected_samples\n",
        "\n",
        "        # Read data in chunks, selecting only necessary columns\n",
        "        # Use header=None as we've already read the header\n",
        "        reader = pd.read_csv(f, sep=sep, chunksize=50_000, dtype=str, header=None)\n",
        "        kept = []\n",
        "        for i, ch in enumerate(reader):\n",
        "            # Assign original header to the chunk\n",
        "            ch.columns = header\n",
        "            # Select only the columns we need\n",
        "            ch = ch[usecols_filter]\n",
        "\n",
        "            ch = ch.rename(columns={gene_col_name: \"row_id\"})\n",
        "\n",
        "            # Ensure 'row_id' column exists after renaming\n",
        "            if 'row_id' not in ch.columns:\n",
        "                 st.error(f\"Error: Could not find gene identifier column '{gene_col_name}' in the uploaded file.\")\n",
        "                 return pd.DataFrame() # Return empty DataFrame on error\n",
        "\n",
        "            ids = ch[\"row_id\"].astype(str).map(normalize)\n",
        "            mask = ids.isin(target_rows)\n",
        "\n",
        "            if mask.any():\n",
        "                out = ch.loc[mask].copy()\n",
        "                out[\"row_id\"] = out[\"row_id\"].map(normalize)\n",
        "                kept.append(out)\n",
        "\n",
        "    if not kept:\n",
        "        st.warning(\"No signature rows matched the provided gene list.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    expr_small = pd.concat(kept, axis=0, ignore_index=False).drop_duplicates(subset=[\"row_id\"]).set_index(\"row_id\")\n",
        "\n",
        "    if row_mode == 'ensembl':\n",
        "        ensg2sym = {v:k for k,v in sym2ensg.items()}\n",
        "        expr_sym_small = expr_small.copy()\n",
        "        expr_sym_small.index = [ensg2sym.get(e, e) for e in expr_sym_small.index]\n",
        "    else:\n",
        "        expr_sym_small = expr_small.copy()\n",
        "\n",
        "    expr_sym_small = expr_sym_small[~expr_sym_small.index.duplicated(keep=\"first\")]\n",
        "    return expr_sym_small\n",
        "\n",
        "\n",
        "def zscore_by_gene(expr_symbols: pd.DataFrame) -> pd.DataFrame:\n",
        "    E = expr_symbols.copy()\n",
        "    E = E.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    E = E.loc[~E.isna().all(axis=1)]\n",
        "    gene_means = E.mean(axis=1)\n",
        "    E = E.apply(lambda col: col.fillna(gene_means), axis=0)\n",
        "    mu = E.mean(axis=1)\n",
        "    sd = E.std(axis=1) + 1e-8\n",
        "    return (E.sub(mu, axis=0)).div(sd, axis=0)\n",
        "\n",
        "def pathway_scores(expr_symbols: pd.DataFrame, signatures: dict) -> pd.DataFrame:\n",
        "    \\\"\\\"\\\"Return pathways x samples (mean z across member genes present).\\\"\\\"\\\"\n",
        "    Z = zscore_by_gene(expr_symbols)\n",
        "    rows = []\n",
        "    for pw, genes in signatures.items():\n",
        "        present = [g for g in genes if g in Z.index]\n",
        "        if present:\n",
        "            s = Z.loc[present].mean(axis=0)\n",
        "        else:\n",
        "            s = pd.Series([np.nan]*Z.shape[1], index=Z.columns)\n",
        "        s.name = pw\n",
        "        rows.append(s)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def example_drug_panel():\n",
        "    return {\n",
        "        \"EGFRi\": [\"REACTOME_SIGNALING_BY_EGFR\"],\n",
        "        \"ALKi\":  [\"REACTOME_SIGNALING_BY_ALK\"],\n",
        "        \"MEKi\":  [\"REACTOME_MAPK1_MAPK3_SIGNALING\"],\n",
        "        \"PI3Ki\": [\"REACTOME_PI3K_AKT_SIGNALING\"],\n",
        "        \"mTORi\": [\"REACTOME_MTORC1_MEDIATED_SIGNALLING\"],\n",
        "        \"PD1i\":  [\"REACTOME_PD1_SIGNALING\"],\n",
        "        \"VEGFi\": [\"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\"],\n",
        "        \"FGFRi\": [\"REACTOME_SIGNALING_BY_FGFR\"],\n",
        "    }\n",
        "\n",
        "def patient_vector(P: pd.DataFrame, sample_id: str) -> pd.Series:\n",
        "    \\\"\\\"\\\"z-normalize pathways across samples; return vector for one patient.\\\"\\\"\\\"\n",
        "    z = (P - P.mean(axis=1).values.reshape(-1,1)) / (P.std(axis=1).values.reshape(-1,1) + 1e-8)\n",
        "    return z[sample_id].fillna(0.0)\n",
        "\n",
        "def drug_benefit_prior(z_path: pd.Series, panel: dict) -> pd.Series:\n",
        "    \\\"\\\"\\\"Aggregate pathway z's per drug (ReLU to emphasize upregulated pathways).\\\"\\\"\\\"\n",
        "    s = pd.Series({d: float(np.sum([max(z_path.get(p, 0.0), 0.0) for p in pws])) for d, pws in panel.items()})\n",
        "    return s / s.max() if s.max() > 0 else s\n",
        "\n",
        "def build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10):\n",
        "    \\\"\\\"\\\"Pairwise penalties for overlapping mechanisms; diagonal = sparsity.\\\"\\\"\\\"\n",
        "    K = len(drugs); R = np.zeros((K, K), dtype=float)\n",
        "    for i in range(K):\n",
        "        for j in range(i+1, K):\n",
        "            overlap = len(set(panel[drugs[i]]) & set(panel[drugs[j]]))\n",
        "            if overlap > 0:\n",
        "                R[i, j] = R[j, i] = base_overlap * overlap\n",
        "    for i in range(K):\n",
        "        R[i, i] += sparsity\n",
        "    return R\n",
        "\n",
        "def build_qubo(b_hat, R, lam=1.0):\n",
        "    \\\"\\\"\\\"QUBO: minimize x^T (lam R) x + q^T x  where q = -b̂ + diag(lam R).\\\"\\\"\\\"\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy()\n",
        "    diag = np.diag(Q).copy()\n",
        "    np.fill_diagonal(Q, 0.0)  # keep off-diagonal in Q\n",
        "    q += diag\n",
        "    return q, np.triu(Q, 1)\n",
        "\n",
        "def exact_qubo_solve(b_hat: np.ndarray, R: np.ndarray, lam: float = 1.0):\n",
        "    \\\"\\\"\\\"\n",
        "    Exact minimization of QUBO:\n",
        "      E(x) = x^T (lam R) x + q^T x, with q = -b̂ + diag(lam R)\n",
        "    We enumerate all bitstrings (2^K). Returns best bitstring (as 0/1 np array).\n",
        "    \\\"\\\"\\\"\n",
        "    K = len(b_hat)\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy() + np.diag(Q)\n",
        "    np.fill_diagonal(Q, 0.0)  # keep only off-diagonal in Q\n",
        "\n",
        "    best_e = np.inf\n",
        "    best_x = None\n",
        "    # vectorize partial precomputations\n",
        "    upper_idx = np.triu_indices(K, 1)\n",
        "    for mask in range(1 << K):\n",
        "        # build x from bits\n",
        "        x = np.fromiter(((mask >> i) & 1 for i in range(K)), dtype=np.int8)\n",
        "        # E = x^T Q x + q^T x, where Q is strictly upper-triangular mirrored\n",
        "        e = np.dot(q, x) + 2.0 * np.sum(Q[upper_idx] * (x[upper_idx[0]] * x[upper_idx[1]]))\n",
        "        if e < best_e:\n",
        "            best_e, best_x = e, x\n",
        "    return best_x, float(best_e)\n",
        "\n",
        "\n",
        "# Placeholder for CUDA-Q function if needed later, currently uses exact solve\n",
        "def try_cudaq_qaoa(h, J, p=2, shots=2048, max_iters=60, seed=7):\n",
        "    return None # Not implemented in this web app version\n",
        "\n",
        "# --- Main analysis function ---\n",
        "def run_analysis(uploaded_file, n_samples, lam_value):\n",
        "    \\\"\\\"\\\"\n",
        "    Runs the full analysis workflow for the uploaded data.\n",
        "    Returns pathway scores, drug selection summary, and patient report.\n",
        "    \\\"\\\"\\\"\n",
        "    SIGS = {\n",
        "        \"REACTOME_SIGNALING_BY_EGFR\": [\n",
        "            \"EGFR\",\"ERBB2\",\"ERBB3\",\"GRB2\",\"SOS1\",\"SHC1\",\"PTPN11\",\"KRAS\",\"NRAS\",\"HRAS\",\n",
        "            \"BRAF\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"GAB1\"\n",
        "        ],\n",
        "        \"REACTOME_SIGNALING_BY_ALK\": [\n",
        "            \"ALK\",\"EML4\",\"GRB2\",\"SHC1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"STAT3\",\"MAP2K1\",\"MAPK1\",\"MAPK3\"\n",
        "        ],\n",
        "        \"REACTOME_MAPK1_MAPK3_SIGNALING\": [\n",
        "            \"BRAF\",\"RAF1\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"DUSP6\",\"DUSP4\",\"FOS\",\"JUN\",\"EGFR\"\n",
        "        ],\n",
        "        \"REACTOME_PI3K_AKT_SIGNALING\": [\n",
        "            \"PIK3CA\",\"PIK3CB\",\"PIK3CD\",\"PIK3R1\",\"PIK3R2\",\"AKT1\",\"AKT2\",\"AKT3\",\"PTEN\",\"MTOR\",\"RHEB\"\n",
        "        ],\n",
        "        \"REACTOME_MTORC1_MEDIATED_SIGNALLING\": [\n",
        "            \"MTOR\",\"RPTOR\",\"MLST8\",\"RHEB\",\"TSC1\",\"TSC2\",\"EIF4EBP1\",\"RPS6KB1\",\"RPS6\"\n",
        "        ],\n",
        "        \"REACTOME_PD1_SIGNALING\": [\n",
        "            \"PDCD1\",\"CD274\",\"PDCD1LG2\",\"PDCD1LG2\",\"JAK1\",\"JAK2\",\"STAT1\",\"IFNG\",\"GZMB\",\"LAG3\",\"TIGIT\",\"CXCL9\",\"CXCL10\"\n",
        "        ],\n",
        "        \"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\": [\n",
        "            \"VEGFA\",\"KDR\",\"FLT1\",\"FLT4\",\"PTPRB\",\"PLCG1\",\"MAP2K1\",\"MAPK1\",\"NOS3\"\n",
        "        ],\n",
        "        \"REACTOME_SIGNALING_BY_FGFR\": [\n",
        "            \"FGFR1\",\"FGFR2\",\"FGFR3\",\"FGFR4\",\"FRS2\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"MAP2K1\",\"MAPK1\"\n",
        "        ]\n",
        "    }\n",
        "    SIG_GENES = sorted({g for gs in SIGS.values() for g in gs})\n",
        "    SYM2ENSG = {\n",
        "        \"EGFR\":\"ENSG00000146648\",\"ERBB2\":\"ENSG00000141736\",\"ERBB3\":\"ENSG00000065361\",\"GRB2\":\"ENSG00000177885\",\n",
        "        \"SOS1\":\"ENSG00000115904\",\"SHC1\":\"ENSG00000154639\",\"PTPN11\":\"ENSG00000179295\",\"KRAS\":\"ENSG00000133703\",\n",
        "        \"NRAS\":\"ENSG00000213281\",\"HRAS\":\"ENSG00000174775\",\"BRAF\":\"ENSG00000157764\",\"MAP2K1\":\"ENSG00000169032\",\n",
        "        \"MAP2K2\":\"ENSG00000126934\",\"MAPK1\":\"ENSG00000100030\",\"MAPK3\":\"ENSG00000102882\",\"PLCG1\":\"ENSG00000124181\",\n",
        "        \"PIK3CA\":\"ENSG00000121879\",\"PIK3R1\":\"ENSG00000145675\",\"AKT1\":\"ENSG00000142208\",\"AKT2\":\"ENSG00000105221\",\n",
        "        \"AKT3\":\"ENSG00000117020\",\"GAB1\":\"ENSG00000117676\",\n",
        "        \"ALK\":\"ENSG00000171094\",\"EML4\":\"ENSG00000143924\",\"STAT3\":\"ENSG00000168610\",\n",
        "        \"DUSP6\":\"ENSG00000139318\",\"DUSP4\":\"ENSG00000120875\",\"FOS\":\"ENSG00000170345\",\"JUN\":\"ENSG00000177606\",\"RAF1\":\"ENSG00000132155\",\n",
        "        \"PIK3CB\":\"ENSG00000119402\",\"PIK3CD\":\"ENSG00000171608\",\"PIK3R2\":\"ENSG00000189403\",\"PTEN\":\"ENSG00000171862\",\n",
        "        \"MTOR\":\"ENSG00000198793\",\"RHEB\":\"ENSG00000106615\",\n",
        "        \"RPTOR\":\"ENSG00000141564\",\"MLST8\":\"ENSG00000105705\",\"TSC1\":\"ENSG00000165699\",\"TSC2\":\"ENSG00000103197\",\n",
        "        \"EIF4EBP1\":\"ENSG00000187840\",\"RPS6KB1\":\"ENSG00000108443\",\"RPS6\":\"ENSG00000137154\",\n",
        "        \"PDCD1\":\"ENSG00000276977\",\"CD274\":\"ENSG00000120217\",\"PDCD1LG2\":\"ENSG00000197646\",\"JAK1\":\"ENSG00000162434\",\n",
        "        \"JAK2\":\"ENSG00000096968\",\"STAT1\":\"ENSG00000115415\",\"IFNG\":\"ENSG00000111537\",\"GZMB\":\"ENSG00000100453\",\n",
        "        \"LAG3\":\"ENSG00000089692\",\"TIGIT\":\"ENSG00000181847\",\"CXCL9\":\"ENSG00000138755\",\"CXCL10\":\"ENSG00000169245\",\n",
        "        \"VEGFA\":\"ENSG00000112715\",\"KDR\":\"ENSG00000128052\",\"FLT1\":\"ENSG00000102755\",\"FLT4\":\"ENSG00000037280\",\n",
        "        \"PTPRB\":\"ENSG00000160593\",\"NOS3\":\"ENSG00000164867\",\n",
        "        \"FGFR1\":\"ENSG00000077782\",\"FGFR2\":\"ENSG00000066468\",\"FGFR3\":\"ENSG00000068078\",\"FGFR4\":\"ENSG00000069535\",\n",
        "        \"FRS2\":\"ENSG00000181873\"\n",
        "    }\n",
        "\n",
        "    # Data Loading and Initial Processing\n",
        "    # Handle potential gzip compression and determine delimiter\n",
        "    file_content = uploaded_file.getvalue()\n",
        "    if uploaded_file.name.endswith('.gz'):\n",
        "        try:\n",
        "            gz_file = gzip.GzipFile(fileobj=io.BytesIO(file_content))\n",
        "            # Peek at the first line after decompression to determine delimiter\n",
        "            with io.TextIOWrapper(gz_file, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                header_peek = f.readline().rstrip(\"\\\\\\\\n\")\n",
        "                sep = ',' if ',' in header_peek.split() else '\\\\t'\n",
        "            # Reset gz_file for actual reading\n",
        "            gz_file.seek(0)\n",
        "            file_stream = io.BytesIO(gz_file.read())\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error reading gzipped file: {e}\")\n",
        "            return None, None, None, None, None, None\n",
        "    else:\n",
        "        # Determine delimiter for non-gzipped files\n",
        "        try:\n",
        "            header_peek = io.BytesIO(file_content).readline().decode('utf-8').rstrip(\"\\\\\\\\n\")\n",
        "            sep = ',' if ',' in header_peek.split() else '\\\\t'\n",
        "        except Exception as e:\n",
        "             st.error(f\"Error reading file header: {e}\")\n",
        "             return None, None, None, None, None, None\n",
        "        file_stream = io.BytesIO(file_content)\n",
        "\n",
        "    st.write(f\"Detected delimiter: '{sep}'\")\n",
        "\n",
        "    # Detect row mode\n",
        "    # Need a fresh file object for detect_row_mode\n",
        "    file_stream_for_detect = io.BytesIO(file_stream.getvalue())\n",
        "    row_mode = detect_row_mode(file_stream_for_detect)\n",
        "    st.write(f\"Detected row mode: {row_mode.upper()}\")\n",
        "\n",
        "    # Get header to identify sample columns and gene column\n",
        "    file_stream.seek(0) # Ensure stream is at the beginning\n",
        "    header_line = io.TextIOWrapper(file_stream, encoding=\"utf-8\", errors=\"replace\").readline().rstrip(\"\\\\\\\\n\")\n",
        "    cols = header_line.split(sep)\n",
        "    # Assuming the first column is the gene ID/symbol column\n",
        "    gene_col = cols[0]\n",
        "    # Assuming sample columns start from the second column\n",
        "    sample_cols_full = cols[1:]\n",
        "\n",
        "    # Select samples based on n_samples parameter\n",
        "    if n_samples > len(sample_cols_full):\n",
        "        st.warning(f\"Requested {n_samples} samples, but only {len(sample_cols_full)} available. Using all available samples.\")\n",
        "        selected_samples = sample_cols_full\n",
        "    else:\n",
        "        selected_samples = sample_cols_full[:n_samples]\n",
        "\n",
        "    st.write(f\"Processing {len(selected_samples)} samples.\")\n",
        "    st.write(f\"Using gene column: '{gene_col}'\")\n",
        "\n",
        "    # Stream-select rows and columns\n",
        "    # Need a fresh file object for stream_select_rows_columns\n",
        "    file_stream_for_select = io.BytesIO(file_stream.getvalue())\n",
        "    expr_sym_small = stream_select_rows_columns(file_stream_for_select, selected_samples, SIG_GENES, SYM2ENSG, row_mode, gene_col_name=gene_col, sep=sep)\n",
        "\n",
        "    if expr_sym_small.empty:\n",
        "        st.error(\"Failed to load expression data. Please check file format and contents.\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "    st.subheader(\"Expression Matrix (Subset)\")\n",
        "    st.write(\"Shape (genes x samples):\", expr_sym_small.shape)\n",
        "    st.dataframe(expr_sym_small.head())\n",
        "\n",
        "    # Pathway Scoring\n",
        "    st.subheader(\"Pathway Activity Scores\")\n",
        "    P = pathway_scores(expr_sym_small, SIGS)\n",
        "    st.write(\"Shape (pathways x samples):\", P.shape)\n",
        "    st.dataframe(P)\n",
        "\n",
        "    # QUBO Formulation and Drug Selection\n",
        "    st.subheader(\"Drug Selection Results\")\n",
        "    panel = example_drug_panel()\n",
        "    drugs = list(panel.keys())\n",
        "\n",
        "    all_sels = []\n",
        "    all_bhats = []\n",
        "    patient_reports_data = []\n",
        "\n",
        "    # Run analysis for each selected sample\n",
        "    if P.empty:\n",
        "        st.warning(\"No pathway scores computed. Cannot perform drug selection.\")\n",
        "        return P, pd.DataFrame(), pd.DataFrame(), [], drugs, panel\n",
        "\n",
        "    for sample_id in P.columns:\n",
        "        z_path = patient_vector(P, sample_id)\n",
        "        b_series = drug_benefit_prior(z_path, panel).reindex(drugs).fillna(0.0)\n",
        "        R = build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10)\n",
        "        b = b_series.to_numpy(float)\n",
        "        q, Q = build_qubo(b, R, lam=lam_value)\n",
        "\n",
        "        # Use exact solver for now\n",
        "        x_star, e_star = exact_qubo_solve(b, R, lam=lam_value)\n",
        "        sel = [drugs[i] for i, xi in enumerate(x_star) if xi == 1]\n",
        "        all_sels.append(sel)\n",
        "        all_bhats.append(b_series)\n",
        "\n",
        "        # Prepare data for patient report\n",
        "        top_pw = z_path.abs().sort_values(ascending=False).head(5)\n",
        "        patient_reports_data.append({\n",
        "            \"patient\": sample_id,\n",
        "            \"selected_drugs\": \", \".join(sel) if sel else \"(none)\",\n",
        "            \"top_pathways\": \"; \".join([f\"{p}:{z_path[p]:+.2f}\" for p in top_pw.index]),\n",
        "            **{f\"b̂.{d}\": float(b_series.get(d,0.0)) for d in drugs}\n",
        "        })\n",
        "\n",
        "    # Drug Selection Summary\n",
        "    st.subheader(\"Drug Selection Frequency\")\n",
        "    if not all_sels:\n",
        "        st.info(\"No drugs were selected for any patient.\")\n",
        "        freq_df = pd.DataFrame({\"drug\": drugs, \"frequency\": 0, \"frequency_pct\": 0.0})\n",
        "    else:\n",
        "        flat = [d for sel in all_sels for d in sel]\n",
        "        freq = Counter(flat)\n",
        "        freq_df = pd.DataFrame({\"drug\": drugs, \"frequency\": [freq[d] for d in drugs]})\n",
        "        freq_df[\"frequency_pct\"] = 100 * freq_df[\"frequency\"] / len(selected_samples)\n",
        "        freq_df = freq_df.sort_values(\"frequency_pct\", ascending=False).reset_index(drop=True)\n",
        "    st.dataframe(freq_df)\n",
        "\n",
        "    # Patient-level Report\n",
        "    st.subheader(\"Patient-level Report\")\n",
        "    patient_report_df = pd.DataFrame(patient_reports_data)\n",
        "    st.dataframe(patient_report_df)\n",
        "\n",
        "    return P, freq_df, patient_report_df, all_sels, drugs, panel\n",
        "\n",
        "# Function to create a download link\n",
        "def create_download_link(df, filename, text):\n",
        "    csv = df.to_csv(index=False)\n",
        "    b64 = base64.b64encode(csv.encode()).decode()\n",
        "    href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"{filename}\">{text}</a>'\n",
        "    return href\n",
        "\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "st.title(\"Digital Twin Workflow: Gene Expression to Drug Selection\")\n",
        "\n",
        "st.markdown(\\\"\\\"\\\"\n",
        "This application demonstrates a workflow for identifying potential drug therapies\n",
        "for cancer patients based on their gene expression data, using pathway analysis\n",
        "and QUBO optimization.\n",
        "\\\"\\\"\\\")\n",
        "\n",
        "st.sidebar.header(\"Input Data\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload gene expression data (CSV, TSV, or gzipped)\", type=[\"csv\", \"tsv\", \"gz\"])\n",
        "\n",
        "st.sidebar.header(\"Parameters\")\n",
        "n_samples = st.sidebar.slider(\"Number of samples to process\", min_value=1, max_value=100, value=40, step=1)\n",
        "lam_value = st.sidebar.slider(\"Lambda (penalty) value for QUBO\", min_value=0.1, max_value=5.0, value=1.0, step=0.1)\n",
        "\n",
        "st.header(\"Analysis Results\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    if st.button(\"Run Analysis\"):\n",
        "        with st.spinner(f\"Running analysis for {n_samples} samples with lambda={lam_value}...\"):\n",
        "            pathway_scores_df, freq_df, patient_report_df, all_sels, drugs, panel = run_analysis(uploaded_file, n_samples, lam_value)\n",
        "\n",
        "            if pathway_scores_df is not None:\n",
        "                st.subheader(\"Visualizations\")\n",
        "\n",
        "                # Drug selection frequency barplot (Plotly)\n",
        "                if freq_df is not None and not freq_df.empty:\n",
        "                    try:\n",
        "                        fig1 = px.bar(freq_df, x=\"drug\", y=\"frequency_pct\", title=\"Drug selection stability across patients\")\n",
        "                        fig1.update_layout(xaxis_title=\"Drug\", yaxis_title=\"% patients selected\", xaxis_tickangle=-45)\n",
        "                        st.plotly_chart(fig1, use_container_width=True)\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not generate frequency barplot: {e}\")\n",
        "\n",
        "                # Co-selection heatmap and dendrogram (Plotly & Matplotlib)\n",
        "                if all_sels and drugs and panel:\n",
        "                    try:\n",
        "                        st.subheader(\"Co-selection Analysis\")\n",
        "                         # --- build co-selection counts ---\n",
        "                        co_mat = pd.DataFrame(0, index=drugs, columns=drugs, dtype=int)\n",
        "                        n_pat = max(1, len(all_sels))\n",
        "                        for sel in all_sels:\n",
        "                            uniq = list(dict.fromkeys(sel))\n",
        "                            for i in range(len(uniq)):\n",
        "                                for j in range(i, len(uniq)):\n",
        "                                    di, dj = uniq[i], uniq[j]\n",
        "                                    co_mat.loc[di, dj] += 1\n",
        "                                    if i != j:\n",
        "                                        co_mat.loc[dj, di] += 1\n",
        "\n",
        "                        # normalize to % of patients\n",
        "                        co_pct = co_mat / n_pat * 100.0\n",
        "                        st.write(\"Co-selection matrix (% of patients):\")\n",
        "                        st.dataframe(co_pct.round(1))\n",
        "\n",
        "                        # heatmap (Plotly)\n",
        "                        fig2 = px.imshow(co_pct, text_auto=False, color_continuous_scale='Blues', # text_auto=True can make it crowded\n",
        "                                         title='Drug co-selection heatmap',\n",
        "                                         labels=dict(x=\"Drug\", y=\"Drug\", color=\"% patients co-selected\"))\n",
        "                        st.plotly_chart(fig2, use_container_width=True)\n",
        "\n",
        "\n",
        "                        # dendrogram (Matplotlib) - Using Matplotlib for dendrogram due to Plotly complexity\n",
        "                        try:\n",
        "                            corr = np.corrcoef(co_pct.values)\n",
        "                            corr = np.clip(corr, -1.0, 1.0)\n",
        "                            dist = 1.0 - corr\n",
        "                            Z = linkage(dist, method=\"average\")\n",
        "\n",
        "                            fig3_mpl, ax3_mpl = plt.subplots(figsize=(8,5))\n",
        "                            dendrogram(Z, labels=drugs, leaf_rotation=90, leaf_font_size=10,\n",
        "                                       color_threshold=0.7 * np.max(Z[:,2]), ax=ax3_mpl)\n",
        "                            ax3_mpl.set_title(\"Clustered dendrogram of drug co-selection (Matplotlib)\")\n",
        "                            ax3_mpl.set_ylabel(\"Distance (1 - correlation)\")\n",
        "                            plt.tight_layout()\n",
        "                            st.pyplot(fig3_mpl)\n",
        "                            plt.close(fig3_mpl) # Close figure\n",
        "\n",
        "                            # clustered heatmap (Plotly)\n",
        "                            order = leaves_list(Z)\n",
        "                            co_pct_ordered = co_pct.iloc[order, order]\n",
        "\n",
        "                            fig4 = px.imshow(co_pct_ordered, text_auto=False, color_continuous_scale='Blues', # text_auto=True can make it crowded\n",
        "                                              title='Clustered heatmap of drug co-selection (Ordered)',\n",
        "                                              labels=dict(x=\"Drug\", y=\"Drug\", color=\"% patients co-selected\"))\n",
        "                            st.plotly_chart(fig4, use_container_width=True)\n",
        "                            st.write(\"Clustered co-selection matrix:\")\n",
        "                            st.dataframe(co_pct_ordered.round(1))\n",
        "\n",
        "                        except Exception as e:\n",
        "                             st.write(f\"Could not generate clustering plots (dendrogram/clustered heatmap): {e}\")\n",
        "\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not perform co-selection analysis: {e}\")\n",
        "\n",
        "                # Selection size distribution histogram (Plotly)\n",
        "                if all_sels:\n",
        "                    try:\n",
        "                        sizes = [len(set(sel)) for sel in all_sels]\n",
        "                        if sizes: # Check if sizes list is not empty\n",
        "                            size_hist = pd.Series(sizes).value_counts().sort_index()\n",
        "                            size_hist_df = size_hist.reset_index()\n",
        "                            size_hist_df.columns = [\"# drugs selected\", \"# patients\"]\n",
        "                            fig5 = px.bar(size_hist_df, x=\"# drugs selected\", y=\"# patients\",\n",
        "                                          title=\"Selection size distribution\")\n",
        "                            fig5.update_layout(xaxis=dict(tickmode='linear')) # Ensure all integer ticks are shown\n",
        "                            st.plotly_chart(fig5, use_container_width=True)\n",
        "                        else:\n",
        "                             st.info(\"No drug selections were made to plot selection size distribution.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not generate selection size histogram: {e}\")\n",
        "\n",
        "                # Synthetic Classification Validation (Plotly)\n",
        "                if all_sels and drugs and pathway_scores_df is not None and not pathway_scores_df.empty:\n",
        "                    try:\n",
        "                        st.subheader(\"Synthetic Classification Validation\")\n",
        "                        # 1) Construct patient-drug feature matrix\n",
        "                        N = len(all_sels)\n",
        "                        X = np.zeros((N, len(drugs)))\n",
        "                        for i, sel in enumerate(all_sels):\n",
        "                            for d in drugs: # Iterate through drugs to ensure correct indexing\n",
        "                                if d in sel:\n",
        "                                    X[i, drugs.index(d)] = 1\n",
        "\n",
        "                        # 2) Synthetic binary labels\n",
        "                        risk_scores = []\n",
        "                        # Ensure we only process samples that were successfully processed in run_analysis\n",
        "                        processed_sample_ids = pathway_scores_df.columns\n",
        "                        if len(processed_sample_ids) == 0:\n",
        "                            st.warning(\"No samples processed for synthetic classification.\")\n",
        "                            raise ValueError(\"No processed samples\") # Trigger exception to skip\n",
        "                        if len(processed_sample_ids) < N:\n",
        "                             st.warning(f\"Only {len(processed_sample_ids)} samples processed for classification (less than requested {N}). Using processed samples.\")\n",
        "                             # Adjust N and X to match processed samples\n",
        "                             N_processed = len(processed_sample_ids)\n",
        "                             # Need to map all_sels back to processed_sample_ids order if necessary\n",
        "                             # Assuming all_sels corresponds to P.columns order\n",
        "                             X_processed = np.zeros((N_processed, len(drugs)))\n",
        "                             processed_sels = all_sels[:N_processed] # Assuming order matches\n",
        "                             for i, sel in enumerate(processed_sels):\n",
        "                                  for d in drugs:\n",
        "                                       if d in sel:\n",
        "                                            X_processed[i, drugs.index(d)] = 1\n",
        "                             X = X_processed # Use the adjusted feature matrix\n",
        "                             N = N_processed # Use the adjusted sample count\n",
        "\n",
        "\n",
        "                        for sid in processed_sample_ids:\n",
        "                             z_path = patient_vector(pathway_scores_df, sid)\n",
        "                             risk_scores.append(z_path.mean())\n",
        "                        risk_scores = np.array(risk_scores)\n",
        "\n",
        "                        # Handle case where all risk scores are the same\n",
        "                        if np.std(risk_scores) == 0:\n",
        "                             st.warning(\"Cannot generate synthetic labels: all pathway mean scores are the same.\")\n",
        "                             st.write(\"Synthetic labels (all 0):\")\n",
        "                             st.write(np.zeros(N, dtype=int))\n",
        "                        else:\n",
        "                             y = (risk_scores > np.median(risk_scores)).astype(int)\n",
        "                             st.write(\"Synthetic labels (0=low risk, 1=high risk):\")\n",
        "                             st.write(np.bincount(y))\n",
        "\n",
        "                             # 3) Train simple logistic regression\n",
        "                             if len(np.unique(y)) > 1: # Only train if there's more than one class\n",
        "                                 clf = LogisticRegression(max_iter=200)\n",
        "                                 clf.fit(X, y)\n",
        "                                 y_pred = clf.predict_proba(X)[:,1]\n",
        "\n",
        "                                 # 4) Metrics\n",
        "                                 roc_auc = roc_auc_score(y, y_pred)\n",
        "                                 pr_auc = average_precision_score(y, y_pred)\n",
        "\n",
        "                                 st.write(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "                                 st.write(f\"PR-AUC: {pr_auc:.3f}\")\n",
        "\n",
        "                                 # Optional barplot of learned coefficients (Plotly)\n",
        "                                 coefs = pd.Series(clf.coef_[0], index=drugs).sort_values()\n",
        "                                 coefs_df = coefs.reset_index()\n",
        "                                 coefs_df.columns = [\"Drug\", \"Weight\"]\n",
        "                                 fig6 = px.bar(coefs_df, y=\"Drug\", x=\"Weight\", orientation='h',\n",
        "                                               color=\"Weight\", color_continuous_scale=['salmon', 'steelblue'],\n",
        "                                               title=\"Drug selection coefficients (synthetic label prediction)\")\n",
        "                                 st.plotly_chart(fig6, use_container_width=True)\n",
        "\n",
        "                             else:\n",
        "                                 st.warning(\"Classification skipped: only one unique synthetic label class found.\")\n",
        "\n",
        "                    except ValueError as ve:\n",
        "                         # Catch the specific ValueError raised for skipping classification\n",
        "                         st.warning(f\"Synthetic classification skipped: {ve}\")\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not perform synthetic classification validation: {e}\")\n",
        "\n",
        "\n",
        "                # Downloadable reports\n",
        "                if patient_report_df is not None and not patient_report_df.empty:\n",
        "                    st.subheader(\"Download Reports\")\n",
        "                    st.markdown(create_download_link(patient_report_df, \"patient_report.csv\", \"Download Patient Report (CSV)\"), unsafe_allow_html=True)\n",
        "\n",
        "                if freq_df is not None and not freq_df.empty:\n",
        "                     st.markdown(create_download_link(freq_df, \"drug_frequency_report.csv\", \"Download Drug Frequency Report (CSV)\"), unsafe_allow_html=True)\n",
        "\n",
        "                st.success(\"Analysis complete!\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload a gene expression data file to begin.\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "print(\"Created app.py\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46ec5309"
      },
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46484499"
      },
      "source": [
        "## Add documentation and examples\n",
        "\n",
        "### Subtask:\n",
        "Add documentation and examples to the web application to provide clear instructions on how to use it and include example data or use cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "401bf35e"
      },
      "source": [
        "**Reasoning**:\n",
        "Add documentation and examples to the Streamlit application code in `app.py` by including markdown sections for overview, data format, parameter explanations, results description, and download options. Include a note about example data sources.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "404265b4"
      },
      "source": [
        "# Save the Streamlit application code to app.py\n",
        "streamlit_code = \"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gzip\n",
        "import re\n",
        "from collections import Counter\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import io\n",
        "import base64\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Assume the core logic functions from the notebook are available here.\n",
        "# For demonstration, I'll include them directly. In a larger project,\n",
        "# these would be in a separate module.\n",
        "\n",
        "def norm_ensembl(x): return x.split('.',1)[0]\n",
        "def norm_symbol(x):  return re.sub(r'[^A-Za-z0-9_-]+','', x)\n",
        "\n",
        "def detect_row_mode(file_object, scan_rows=50000):\n",
        "    \\\\\"\\\\\"\\\\\"Detect row ID type (Ensembl vs symbol) from file-like object.\\\\\"\\\\\"\\\\\"\n",
        "    seen = Counter(); total = 0\n",
        "    # Use a TextIOWrapper to treat the bytes stream as text\n",
        "    # Ensure file_object is at the beginning\n",
        "    file_object.seek(0)\n",
        "    with io.TextIOWrapper(file_object, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        # Read the header line to skip it for row detection\n",
        "        header = f.readline()\n",
        "        # Use header=None because we already read the header\n",
        "        reader = pd.read_csv(f, sep=\"\\\\t\", chunksize=200_000, usecols=[0], dtype=str, header=None)\n",
        "        for ch in reader:\n",
        "            v = ch.iloc[:,0].astype(str)\n",
        "            vals = v.head(min(len(v), scan_rows-total)).tolist()\n",
        "            total += len(vals)\n",
        "            seen.update('ENSG' if x.startswith('ENSG') else 'OTHER' for x in vals)\n",
        "            if total >= scan_rows: break\n",
        "    ratio = seen['ENSG']/max(1,(seen['ENSG']+seen['OTHER']))\n",
        "    mode = 'ensembl' if ratio >= 0.6 else 'symbol'\n",
        "    return mode\n",
        "\n",
        "def stream_select_rows_columns(file_object, selected_samples, sig_genes, sym2ensg, row_mode, gene_col_name=\"Name\", sep='\\\\t'):\n",
        "    \\\\\"\\\\\"\\\\\"Stream-select rows and columns from file-like object.\\\\\"\\\\\"\\\\\"\n",
        "    if row_mode == 'ensembl':\n",
        "        target_rows = set(sym2ensg.get(g) for g in sig_genes if g in sym2ensg)\n",
        "        normalize = norm_ensembl\n",
        "    else:\n",
        "        target_rows = set(sig_genes)\n",
        "        normalize = norm_symbol\n",
        "\n",
        "    # Ensure file_object is at the beginning\n",
        "    file_object.seek(0)\n",
        "\n",
        "    with io.TextIOWrapper(file_object, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        # Read header to find the actual gene_col_name and sample columns\n",
        "        header = f.readline().rstrip(\"\\\\\\\\n\").split(sep)\n",
        "        try:\n",
        "            gene_col_idx = header.index(gene_col_name)\n",
        "        except ValueError:\n",
        "             # If gene_col_name not found, assume the first column is the gene column\n",
        "            gene_col_name = header[0]\n",
        "            gene_col_idx = 0\n",
        "\n",
        "        # Filter header to include only the gene column and selected sample columns\n",
        "        usecols_filter = [gene_col_name] + selected_samples\n",
        "\n",
        "        # Read data in chunks, selecting only necessary columns\n",
        "        # Use header=None as we've already read the header\n",
        "        reader = pd.read_csv(f, sep=sep, chunksize=50_000, dtype=str, header=None)\n",
        "        kept = []\n",
        "        for i, ch in enumerate(reader):\n",
        "            # Assign original header to the chunk\n",
        "            ch.columns = header\n",
        "            # Select only the columns we need\n",
        "            ch = ch[usecols_filter]\n",
        "\n",
        "            ch = ch.rename(columns={gene_col_name: \"row_id\"})\n",
        "\n",
        "            # Ensure 'row_id' column exists after renaming\n",
        "            if 'row_id' not in ch.columns:\n",
        "                 st.error(f\"Error: Could not find gene identifier column '{gene_col_name}' in the uploaded file.\")\n",
        "                 return pd.DataFrame() # Return empty DataFrame on error\n",
        "\n",
        "            ids = ch[\"row_id\"].astype(str).map(normalize)\n",
        "            mask = ids.isin(target_rows)\n",
        "\n",
        "            if mask.any():\n",
        "                out = ch.loc[mask].copy()\n",
        "                out[\"row_id\"] = out[\"row_id\"].map(normalize)\n",
        "                kept.append(out)\n",
        "\n",
        "    if not kept:\n",
        "        st.warning(\"No signature rows matched the provided gene list.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    expr_small = pd.concat(kept, axis=0, ignore_index=False).drop_duplicates(subset=[\"row_id\"]).set_index(\"row_id\")\n",
        "\n",
        "    if row_mode == 'ensembl':\n",
        "        ensg2sym = {v:k for k,v in SYM2ENSG.items()}\n",
        "        expr_sym_small = expr_small.copy()\n",
        "        expr_sym_small.index = [ensg2sym.get(e, e) for e in expr_sym_small.index]\n",
        "    else:\n",
        "        expr_sym_small = expr_small.copy()\n",
        "\n",
        "    expr_sym_small = expr_sym_small[~expr_sym_small.index.duplicated(keep=\"first\")]\n",
        "    return expr_sym_small\n",
        "\n",
        "\n",
        "def zscore_by_gene(expr_symbols: pd.DataFrame) -> pd.DataFrame:\n",
        "    E = expr_symbols.copy()\n",
        "    E = E.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    E = E.loc[~E.isna().all(axis=1)]\n",
        "    gene_means = E.mean(axis=1)\n",
        "    E = E.apply(lambda col: col.fillna(gene_means), axis=0)\n",
        "    mu = E.mean(axis=1)\n",
        "    sd = E.std(axis=1) + 1e-8\n",
        "    return (E.sub(mu, axis=0)).div(sd, axis=0)\n",
        "\n",
        "def pathway_scores(expr_symbols: pd.DataFrame, signatures: dict) -> pd.DataFrame:\n",
        "    \\\"\\\"\\\"Return pathways x samples (mean z across member genes present).\\\"\\\"\\\"\n",
        "    Z = zscore_by_gene(expr_symbols)\n",
        "    rows = []\n",
        "    for pw, genes in signatures.items():\n",
        "        present = [g for g in genes if g in Z.index]\n",
        "        if present:\n",
        "            s = Z.loc[present].mean(axis=0)\n",
        "        else:\n",
        "            s = pd.Series([np.nan]*Z.shape[1], index=Z.columns)\n",
        "        s.name = pw\n",
        "        rows.append(s)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def example_drug_panel():\n",
        "    return {\n",
        "        \"EGFRi\": [\"REACTOME_SIGNALING_BY_EGFR\"],\n",
        "        \"ALKi\":  [\"REACTOME_SIGNALING_BY_ALK\"],\n",
        "        \"MEKi\":  [\"REACTOME_MAPK1_MAPK3_SIGNALING\"],\n",
        "        \"PI3Ki\": [\"REACTOME_PI3K_AKT_SIGNALING\"],\n",
        "        \"mTORi\": [\"REACTOME_MTORC1_MEDIATED_SIGNALLING\"],\n",
        "        \"PD1i\":  [\"REACTOME_PD1_SIGNALING\"],\n",
        "        \"VEGFi\": [\"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\"],\n",
        "        \"FGFRi\": [\"REACTOME_SIGNALING_BY_FGFR\"],\n",
        "    }\n",
        "\n",
        "def patient_vector(P: pd.DataFrame, sample_id: str) -> pd.Series:\n",
        "    \\\"\\\"\\\"z-normalize pathways across samples; return vector for one patient.\\\"\\\"\\\"\n",
        "    z = (P - P.mean(axis=1).values.reshape(-1,1)) / (P.std(axis=1).values.reshape(-1,1) + 1e-8)\n",
        "    return z[sample_id].fillna(0.0)\n",
        "\n",
        "def drug_benefit_prior(z_path: pd.Series, panel: dict) -> pd.Series:\n",
        "    \\\"\\\"\\\"Aggregate pathway z's per drug (ReLU to emphasize upregulated pathways).\\\"\\\"\\\"\n",
        "    s = pd.Series({d: float(np.sum([max(z_path.get(p, 0.0), 0.0) for p in pws])) for d, pws in panel.items()})\n",
        "    return s / s.max() if s.max() > 0 else s\n",
        "\n",
        "def build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10):\n",
        "    \\\"\\\"\\\"Pairwise penalties for overlapping mechanisms; diagonal = sparsity.\\\"\\\"\\\"\n",
        "    K = len(drugs); R = np.zeros((K, K), dtype=float)\n",
        "    for i in range(K):\n",
        "        for j in range(i+1, K):\n",
        "            overlap = len(set(panel[drugs[i]]) & set(panel[drugs[j]]))\n",
        "            if overlap > 0:\n",
        "                R[i, j] = R[j, i] = base_overlap * overlap\n",
        "    for i in range(K):\n",
        "        R[i, i] += sparsity\n",
        "    return R\n",
        "\n",
        "def build_qubo(b_hat, R, lam=1.0):\n",
        "    \\\"\\\"\\\"QUBO: minimize x^T (lam R) x + q^T x  where q = -b̂ + diag(lam R).\\\"\\\"\\\"\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy()\n",
        "    diag = np.diag(Q).copy()\n",
        "    np.fill_diagonal(Q, 0.0)  # keep off-diagonal in Q\n",
        "    q += diag\n",
        "    return q, np.triu(Q, 1)\n",
        "\n",
        "def exact_qubo_solve(b_hat: np.ndarray, R: np.ndarray, lam: float = 1.0):\n",
        "    \\\"\\\"\\\"\n",
        "    Exact minimization of QUBO:\n",
        "      E(x) = x^T (lam R) x + q^T x, with q = -b̂ + diag(lam R)\n",
        "    We enumerate all bitstrings (2^K). Returns best bitstring (as 0/1 np array).\n",
        "    \\\"\\\"\\\"\n",
        "    K = len(b_hat)\n",
        "    Q = lam * R.copy()\n",
        "    q = -b_hat.copy() + np.diag(Q)\n",
        "    np.fill_diagonal(Q, 0.0)  # keep only off-diagonal in Q\n",
        "\n",
        "    best_e = np.inf\n",
        "    best_x = None\n",
        "    # vectorize partial precomputations\n",
        "    upper_idx = np.triu_indices(K, 1)\n",
        "    for mask in range(1 << K):\n",
        "        # build x from bits\n",
        "        x = np.fromiter(((mask >> i) & 1 for i in range(K)), dtype=np.int8)\n",
        "        # E = x^T Q x + q^T x, where Q is strictly upper-triangular mirrored\n",
        "        e = np.dot(q, x) + 2.0 * np.sum(Q[upper_idx] * (x[upper_idx[0]] * x[upper_idx[1]]))\n",
        "        if e < best_e:\n",
        "            best_e, best_x = e, x\n",
        "    return best_x, float(best_e)\n",
        "\n",
        "\n",
        "# Placeholder for CUDA-Q function if needed later, currently uses exact solve\n",
        "def try_cudaq_qaoa(h, J, p=2, shots=2048, max_iters=60, seed=7):\n",
        "    return None # Not implemented in this web app version\n",
        "\n",
        "# --- Main analysis function ---\n",
        "def run_analysis(uploaded_file, n_samples, lam_value):\n",
        "    \\\"\\\"\\\"\n",
        "    Runs the full analysis workflow for the uploaded data.\n",
        "    Returns pathway scores, drug selection summary, and patient report.\n",
        "    \\\"\\\"\\\"\n",
        "    SIGS = {\n",
        "        \"REACTOME_SIGNALING_BY_EGFR\": [\n",
        "            \"EGFR\",\"ERBB2\",\"ERBB3\",\"GRB2\",\"SOS1\",\"SHC1\",\"PTPN11\",\"KRAS\",\"NRAS\",\"HRAS\",\n",
        "            \"BRAF\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"GAB1\"\n",
        "        ],\n",
        "        \"REACTOME_SIGNALING_BY_ALK\": [\n",
        "            \"ALK\",\"EML4\",\"GRB2\",\"SHC1\",\"PIK3CA\",\"PIK3R1\",\"AKT1\",\"AKT2\",\"AKT3\",\"STAT3\",\"MAP2K1\",\"MAPK1\",\"MAPK3\"\n",
        "        ],\n",
        "        \"REACTOME_MAPK1_MAPK3_SIGNALING\": [\n",
        "            \"BRAF\",\"RAF1\",\"MAP2K1\",\"MAP2K2\",\"MAPK1\",\"MAPK3\",\"DUSP6\",\"DUSP4\",\"FOS\",\"JUN\",\"EGFR\"\n",
        "        ],\n",
        "        \"REACTOME_PI3K_AKT_SIGNALING\": [\n",
        "            \"PIK3CA\",\"PIK3CB\",\"PIK3CD\",\"PIK3R1\",\"PIK3R2\",\"AKT1\",\"AKT2\",\"AKT3\",\"PTEN\",\"MTOR\",\"RHEB\"\n",
        "        ],\n",
        "        \"REACTOME_MTORC1_MEDIATED_SIGNALLING\": [\n",
        "            \"MTOR\",\"RPTOR\",\"MLST8\",\"RHEB\",\"TSC1\",\"TSC2\",\"EIF4EBP1\",\"RPS6KB1\",\"RPS6\"\n",
        "        ],\n",
        "        \"REACTOME_PD1_SIGNALING\": [\n",
        "            \"PDCD1\",\"CD274\",\"PDCD1LG2\",\"PDCD1LG2\",\"JAK1\",\"JAK2\",\"STAT1\",\"IFNG\",\"GZMB\",\"LAG3\",\"TIGIT\",\"CXCL9\",\"CXCL10\"\n",
        "        ],\n",
        "        \"REACTOME_VEGFA_VEGFR2_SIGNALING_PATHWAY\": [\n",
        "            \"VEGFA\",\"KDR\",\"FLT1\",\"FLT4\",\"PTPRB\",\"PLCG1\",\"MAP2K1\",\"MAPK1\",\"NOS3\"\n",
        "        ],\n",
        "        \"REACTOME_SIGNALING_BY_FGFR\": [\n",
        "            \"FGFR1\",\"FGFR2\",\"FGFR3\",\"FGFR4\",\"FRS2\",\"PLCG1\",\"PIK3CA\",\"PIK3R1\",\"MAP2K1\",\"MAPK1\"\n",
        "        ]\n",
        "    }\n",
        "    SIG_GENES = sorted({g for gs in SIGS.values() for g in gs})\n",
        "    SYM2ENSG = {\n",
        "        \"EGFR\":\"ENSG00000146648\",\"ERBB2\":\"ENSG00000141736\",\"ERBB3\":\"ENSG00000065361\",\"GRB2\":\"ENSG00000177885\",\n",
        "        \"SOS1\":\"ENSG00000115904\",\"SHC1\":\"ENSG00000154639\",\"PTPN11\":\"ENSG00000179295\",\"KRAS\":\"ENSG00000133703\",\n",
        "        \"NRAS\":\"ENSG00000213281\",\"HRAS\":\"ENSG00000174775\",\"BRAF\":\"ENSG00000157764\",\"MAP2K1\":\"ENSG00000169032\",\n",
        "        \"MAP2K2\":\"ENSG00000126934\",\"MAPK1\":\"ENSG00000100030\",\"MAPK3\":\"ENSG00000102882\",\"PLCG1\":\"ENSG00000124181\",\n",
        "        \"PIK3CA\":\"ENSG00000121879\",\"PIK3R1\":\"ENSG00000145675\",\"AKT1\":\"ENSG00000142208\",\"AKT2\":\"ENSG00000105221\",\n",
        "        \"AKT3\":\"ENSG00000117020\",\"GAB1\":\"ENSG00000117676\",\n",
        "        \"ALK\":\"ENSG00000171094\",\"EML4\":\"ENSG00000143924\",\"STAT3\":\"ENSG00000168610\",\n",
        "        \"DUSP6\":\"ENSG00000139318\",\"DUSP4\":\"ENSG00000120875\",\"FOS\":\"ENSG00000170345\",\"JUN\":\"ENSG00000177606\",\"RAF1\":\"ENSG00000132155\",\n",
        "        \"PIK3CB\":\"ENSG00000119402\",\"PIK3CD\":\"ENSG00000171608\",\"PIK3R2\":\"ENSG00000189403\",\"PTEN\":\"ENSG00000171862\",\n",
        "        \"MTOR\":\"ENSG00000198793\",\"RHEB\":\"ENSG00000106615\",\n",
        "        \"RPTOR\":\"ENSG00000141564\",\"MLST8\":\"ENSG00000105705\",\"TSC1\":\"ENSG00000165699\",\"TSC2\":\"ENSG00000103197\",\n",
        "        \"EIF4EBP1\":\"ENSG00000187840\",\"RPS6KB1\":\"ENSG00000108443\",\"RPS6\":\"ENSG00000137154\",\n",
        "        \"PDCD1\":\"ENSG00000276977\",\"CD274\":\"ENSG00000120217\",\"PDCD1LG2\":\"ENSG00000197646\",\"JAK1\":\"ENSG00000162434\",\n",
        "        \"JAK2\":\"ENSG00000096968\",\"STAT1\":\"ENSG00000115415\",\"IFNG\":\"ENSG00000111537\",\"GZMB\":\"ENSG00000100453\",\n",
        "        \"LAG3\":\"ENSG00000089692\",\"TIGIT\":\"ENSG00000181847\",\"CXCL9\":\"ENSG00000138755\",\"CXCL10\":\"ENSG00000169245\",\n",
        "        \"VEGFA\":\"ENSG00000112715\",\"KDR\":\"ENSG00000128052\",\"FLT1\":\"ENSG00000102755\",\"FLT4\":\"ENSG00000037280\",\n",
        "        \"PTPRB\":\"ENSG00000160593\",\"NOS3\":\"ENSG00000164867\",\n",
        "        \"FGFR1\":\"ENSG00000077782\",\"FGFR2\":\"ENSG00000066468\",\"FGFR3\":\"ENSG00000068078\",\"FGFR4\":\"ENSG00000069535\",\n",
        "        \"FRS2\":\"ENSG00000181873\"\n",
        "    }\n",
        "\n",
        "    # Data Loading and Initial Processing\n",
        "    # Handle potential gzip compression and determine delimiter\n",
        "    file_content = uploaded_file.getvalue()\n",
        "    if uploaded_file.name.endswith('.gz'):\n",
        "        try:\n",
        "            gz_file = gzip.GzipFile(fileobj=io.BytesIO(file_content))\n",
        "            # Peek at the first line after decompression to determine delimiter\n",
        "            with io.TextIOWrapper(gz_file, encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "                header_peek = f.readline().rstrip(\"\\\\\\\\n\")\n",
        "                sep = ',' if ',' in header_peek.split() else '\\\\t'\n",
        "            # Reset gz_file for actual reading\n",
        "            gz_file.seek(0)\n",
        "            file_stream = io.BytesIO(gz_file.read())\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error reading gzipped file: {e}\")\n",
        "            return None, None, None, None, None, None\n",
        "    else:\n",
        "        # Determine delimiter for non-gzipped files\n",
        "        try:\n",
        "            header_peek = io.BytesIO(file_content).readline().decode('utf-8').rstrip(\"\\\\\\\\n\")\n",
        "            sep = ',' if ',' in header_peek.split() else '\\\\t'\n",
        "        except Exception as e:\n",
        "             st.error(f\"Error reading file header: {e}\")\n",
        "             return None, None, None, None, None, None\n",
        "        file_stream = io.BytesIO(file_content)\n",
        "\n",
        "    st.write(f\"Detected delimiter: '{sep}'\")\n",
        "\n",
        "    # Detect row mode\n",
        "    # Need a fresh file object for detect_row_mode\n",
        "    file_stream_for_detect = io.BytesIO(file_stream.getvalue())\n",
        "    row_mode = detect_row_mode(file_stream_for_detect)\n",
        "    st.write(f\"Detected row mode: {row_mode.upper()}\")\n",
        "\n",
        "    # Get header to identify sample columns and gene column\n",
        "    file_stream.seek(0) # Ensure stream is at the beginning\n",
        "    header_line = io.TextIOWrapper(file_stream, encoding=\"utf-8\", errors=\"replace\").readline().rstrip(\"\\\\\\\\n\")\n",
        "    cols = header_line.split(sep)\n",
        "    # Assuming the first column is the gene ID/symbol column\n",
        "    gene_col = cols[0]\n",
        "    # Assuming sample columns start from the second column\n",
        "    sample_cols_full = cols[1:]\n",
        "\n",
        "    # Select samples based on n_samples parameter\n",
        "    if n_samples > len(sample_cols_full):\n",
        "        st.warning(f\"Requested {n_samples} samples, but only {len(sample_cols_full)} available. Using all available samples.\")\n",
        "        selected_samples = sample_cols_full\n",
        "    else:\n",
        "        selected_samples = sample_cols_full[:n_samples]\n",
        "\n",
        "    st.write(f\"Processing {len(selected_samples)} samples.\")\n",
        "    st.write(f\"Using gene column: '{gene_col}'\")\n",
        "\n",
        "    # Stream-select rows and columns\n",
        "    # Need a fresh file object for stream_select_rows_columns\n",
        "    file_stream_for_select = io.BytesIO(file_stream.getvalue())\n",
        "    expr_sym_small = stream_select_rows_columns(file_stream_for_select, selected_samples, SIG_GENES, SYM2ENSG, row_mode, gene_col_name=gene_col, sep=sep)\n",
        "\n",
        "    if expr_sym_small.empty:\n",
        "        st.error(\"Failed to load expression data. Please check file format and contents.\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "    st.subheader(\"Expression Matrix (Subset)\")\n",
        "    st.write(\"Shape (genes x samples):\", expr_sym_small.shape)\n",
        "    st.dataframe(expr_sym_small.head())\n",
        "\n",
        "    # Pathway Scoring\n",
        "    st.subheader(\"Pathway Activity Scores\")\n",
        "    P = pathway_scores(expr_sym_small, SIGS)\n",
        "    st.write(\"Shape (pathways x samples):\", P.shape)\n",
        "    st.dataframe(P)\n",
        "\n",
        "    # QUBO Formulation and Drug Selection\n",
        "    st.subheader(\"Drug Selection Results\")\n",
        "    panel = example_drug_panel()\n",
        "    drugs = list(panel.keys())\n",
        "\n",
        "    all_sels = []\n",
        "    all_bhats = []\n",
        "    patient_reports_data = []\n",
        "\n",
        "    # Run analysis for each selected sample\n",
        "    if P.empty:\n",
        "        st.warning(\"No pathway scores computed. Cannot perform drug selection.\")\n",
        "        return P, pd.DataFrame(), pd.DataFrame(), [], drugs, panel\n",
        "\n",
        "    for sample_id in P.columns:\n",
        "        z_path = patient_vector(P, sample_id)\n",
        "        b_series = drug_benefit_prior(z_path, panel).reindex(drugs).fillna(0.0)\n",
        "        R = build_penalty_matrix(drugs, panel, base_overlap=0.25, sparsity=0.10)\n",
        "        b = b_series.to_numpy(float)\n",
        "        q, Q = build_qubo(b, R, lam=lam_value)\n",
        "\n",
        "        # Use exact solver for now\n",
        "        x_star, e_star = exact_qubo_solve(b, R, lam=lam_value)\n",
        "        sel = [drugs[i] for i, xi in enumerate(x_star) if xi == 1]\n",
        "        all_sels.append(sel)\n",
        "        all_bhats.append(b_series)\n",
        "\n",
        "        # Prepare data for patient report\n",
        "        top_pw = z_path.abs().sort_values(ascending=False).head(5)\n",
        "        patient_reports_data.append({\n",
        "            \"patient\": sample_id,\n",
        "            \"selected_drugs\": \", \".join(sel) if sel else \"(none)\",\n",
        "            \"top_pathways\": \"; \".join([f\"{p}:{z_path[p]:+.2f}\" for p in top_pw.index]),\n",
        "            **{f\"b̂.{d}\": float(b_series.get(d,0.0)) for d in drugs}\n",
        "        })\n",
        "\n",
        "    # Drug Selection Summary\n",
        "    st.subheader(\"Drug Selection Frequency\")\n",
        "    if not all_sels:\n",
        "        st.info(\"No drugs were selected for any patient.\")\n",
        "        freq_df = pd.DataFrame({\"drug\": drugs, \"frequency\": 0, \"frequency_pct\": 0.0})\n",
        "    else:\n",
        "        flat = [d for sel in all_sels for d in sel]\n",
        "        freq = Counter(flat)\n",
        "        freq_df = pd.DataFrame({\"drug\": drugs, \"frequency\": [freq[d] for d in drugs]})\n",
        "        freq_df[\"frequency_pct\"] = 100 * freq_df[\"frequency\"] / len(selected_samples)\n",
        "        freq_df = freq_df.sort_values(\"frequency_pct\", ascending=False).reset_index(drop=True)\n",
        "    st.dataframe(freq_df)\n",
        "\n",
        "    # Patient-level Report\n",
        "    st.subheader(\"Patient-level Report\")\n",
        "    patient_report_df = pd.DataFrame(patient_reports_data)\n",
        "    st.dataframe(patient_report_df)\n",
        "\n",
        "    return P, freq_df, patient_report_df, all_sels, drugs, panel\n",
        "\n",
        "# Function to create a download link\n",
        "def create_download_link(df, filename, text):\n",
        "    csv = df.to_csv(index=False)\n",
        "    b64 = base64.b64encode(csv.encode()).decode()\n",
        "    #href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"{filename}\">{text}</a>'\n",
        "    # Using st.download_button instead of markdown link for better UX\n",
        "    st.download_button(\n",
        "        label=text,\n",
        "        data=csv,\n",
        "        file_name=filename,\n",
        "        mime='text/csv',\n",
        "    )\n",
        "\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "st.title(\"Digital Twin Workflow: Gene Expression to Drug Selection\")\n",
        "\n",
        "st.markdown(\\\"\\\"\\\"\n",
        "This application demonstrates a workflow for identifying potential drug therapies\n",
        "for cancer patients based on their gene expression data. It utilizes pathway\n",
        "analysis to score biological activity and formulates a Quadratic Unconstrained\n",
        "Binary Optimization (QUBO) problem to select a combination of drugs.\n",
        "\n",
        "**How to Use:**\n",
        "\n",
        "1.  **Upload Data:** Use the file uploader in the sidebar to upload your gene\n",
        "    expression data. The expected format is a tab-separated or comma-separated\n",
        "    file (CSV or TSV), with genes in rows and samples in columns. The first\n",
        "    column should contain gene identifiers (either gene symbols or Ensembl IDs).\n",
        "    The file can be optionally gzipped (`.gz`).\n",
        "2.  **Set Parameters:** Adjust the \"Number of samples to process\" slider to\n",
        "    specify how many samples from your uploaded file should be included in the\n",
        "    analysis. Modify the \"Lambda (penalty) value for QUBO\" slider to control\n",
        "    the trade-off between maximizing potential drug benefit and minimizing drug\n",
        "    overlap in the optimization step.\n",
        "3.  **Run Analysis:** Click the \"Run Analysis\" button to start the workflow.\n",
        "    The application will process your data, calculate pathway scores, perform\n",
        "    drug selection using the QUBO model, and generate various results and\n",
        "    visualizations.\n",
        "4.  **Explore Results:** Scroll down to view the analysis results, including\n",
        "    subsets of the processed expression matrix and pathway scores, a summary of\n",
        "    drug selection frequencies across the cohort, a patient-level report detailing\n",
        "    selected drugs and pathway context for each sample, and several visualizations.\n",
        "5.  **Download Reports:** Download the generated reports (Patient Report and\n",
        "    Drug Frequency Report) as CSV files for further analysis or record-keeping.\n",
        "\n",
        "**Example Data:**\n",
        "\n",
        "You can obtain example gene expression data in the expected format from public\n",
        "repositories like the TCGA (The Cancer Genome Atlas) or GTEx (Genotype-Tissue\n",
        "Expression) projects, available through resources like the UCSC Xena Browser\n",
        "(e.g., the Toil RNA-seq recompute data). Ensure the data is in a matrix format\n",
        "(genes as rows, samples as columns) and saved as a tab-separated or comma-separated\n",
        "file.\n",
        "\n",
        "*Note: The current analysis focuses on a predefined set of signaling pathways\n",
        "and a simplified drug panel. The synthetic classification validation is for\n",
        "demonstration purposes only and does not represent a real-world clinical prediction.*\n",
        "\\\"\\\"\\\")\n",
        "\n",
        "st.sidebar.header(\"Input Data\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload gene expression data (CSV, TSV, or gzipped)\", type=[\"csv\", \"tsv\", \"gz\"])\n",
        "\n",
        "st.sidebar.header(\"Parameters\")\n",
        "n_samples = st.sidebar.slider(\"Number of samples to process\", min_value=1, max_value=100, value=40, step=1)\n",
        "lam_value = st.sidebar.slider(\"Lambda (penalty) value for QUBO\", min_value=0.1, max_value=5.0, value=1.0, step=0.1)\n",
        "\n",
        "st.header(\"Analysis Results\")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    if st.button(\"Run Analysis\"):\n",
        "        with st.spinner(f\"Running analysis for {n_samples} samples with lambda={lam_value}...\"):\n",
        "            pathway_scores_df, freq_df, patient_report_df, all_sels, drugs, panel = run_analysis(uploaded_file, n_samples, lam_value)\n",
        "\n",
        "            if pathway_scores_df is not None:\n",
        "                st.subheader(\"Visualizations\")\n",
        "\n",
        "                # Drug selection frequency barplot (Plotly)\n",
        "                if freq_df is not None and not freq_df.empty:\n",
        "                    try:\n",
        "                        fig1 = px.bar(freq_df, x=\"drug\", y=\"frequency_pct\", title=\"Drug selection stability across patients\")\n",
        "                        fig1.update_layout(xaxis_title=\"Drug\", yaxis_title=\"% patients selected\", xaxis_tickangle=-45)\n",
        "                        st.plotly_chart(fig1, use_container_width=True)\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not generate frequency barplot: {e}\")\n",
        "\n",
        "                # Co-selection heatmap and dendrogram (Plotly & Matplotlib)\n",
        "                if all_sels and drugs and panel:\n",
        "                    try:\n",
        "                        st.subheader(\"Co-selection Analysis\")\n",
        "                         # --- build co-selection counts ---\n",
        "                        co_mat = pd.DataFrame(0, index=drugs, columns=drugs, dtype=int)\n",
        "                        n_pat = max(1, len(all_sels))\n",
        "                        if n_pat > 0:\n",
        "                            for sel in all_sels:\n",
        "                                uniq = list(dict.fromkeys(sel))\n",
        "                                for i in range(len(uniq)):\n",
        "                                    for j in range(i, len(uniq)):\n",
        "                                        di, dj = uniq[i], uniq[j]\n",
        "                                        co_mat.loc[di, dj] += 1\n",
        "                                        if i != j:\n",
        "                                            co_mat.loc[dj, di] += 1\n",
        "\n",
        "                            # normalize to % of patients\n",
        "                            co_pct = co_mat / n_pat * 100.0\n",
        "                            st.write(\"Co-selection matrix (% of patients):\")\n",
        "                            st.dataframe(co_pct.round(1))\n",
        "\n",
        "                            # heatmap (Plotly)\n",
        "                            fig2 = px.imshow(co_pct, text_auto=False, color_continuous_scale='Blues', # text_auto=True can make it crowded\n",
        "                                             title='Drug co-selection heatmap',\n",
        "                                             labels=dict(x=\"Drug\", y=\"Drug\", color=\"% patients co-selected\"))\n",
        "                            st.plotly_chart(fig2, use_container_width=True)\n",
        "\n",
        "\n",
        "                            # dendrogram (Matplotlib) - Using Matplotlib for dendrogram due to Plotly complexity\n",
        "                            try:\n",
        "                                corr = np.corrcoef(co_pct.values)\n",
        "                                corr = np.clip(corr, -1.0, 1.0)\n",
        "                                dist = 1.0 - corr\n",
        "                                # Check if distance matrix is valid for linkage\n",
        "                                if np.isfinite(dist).all() and dist.shape[0] > 1:\n",
        "                                    Z = linkage(dist, method=\"average\")\n",
        "\n",
        "                                    fig3_mpl, ax3_mpl = plt.subplots(figsize=(8,5))\n",
        "                                    dendrogram(Z, labels=drugs, leaf_rotation=90, leaf_font_size=10,\n",
        "                                               color_threshold=0.7 * np.max(Z[:,2]), ax=ax3_mpl)\n",
        "                                    ax3_mpl.set_title(\"Clustered dendrogram of drug co-selection (Matplotlib)\")\n",
        "                                    ax3_mpl.set_ylabel(\"Distance (1 - correlation)\")\n",
        "                                    plt.tight_layout()\n",
        "                                    st.pyplot(fig3_mpl)\n",
        "                                    plt.close(fig3_mpl) # Close figure\n",
        "\n",
        "                                    # clustered heatmap (Plotly)\n",
        "                                    order = leaves_list(Z)\n",
        "                                    co_pct_ordered = co_pct.iloc[order, order]\n",
        "\n",
        "                                    fig4 = px.imshow(co_pct_ordered, text_auto=False, color_continuous_scale='Blues', # text_auto=True can make it crowded\n",
        "                                                      title='Clustered heatmap of drug co-selection (Ordered)',\n",
        "                                                      labels=dict(x=\"Drug\", y=\"Drug\", color=\"% patients co-selected\"))\n",
        "                                    st.plotly_chart(fig4, use_container_width=True)\n",
        "                                    st.write(\"Clustered co-selection matrix:\")\n",
        "                                    st.dataframe(co_pct_ordered.round(1))\n",
        "                                else:\n",
        "                                     st.info(\"Could not generate clustering plots (dendrogram/clustered heatmap): Insufficient data or invalid distance matrix.\")\n",
        "\n",
        "                            except Exception as e:\n",
        "                                 st.write(f\"Could not generate clustering plots (dendrogram/clustered heatmap): {e}\")\n",
        "                        else:\n",
        "                             st.info(\"Not enough patient data to perform co-selection analysis.\")\n",
        "\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not perform co-selection analysis: {e}\")\n",
        "\n",
        "                # Selection size distribution histogram (Plotly)\n",
        "                if all_sels:\n",
        "                    try:\n",
        "                        sizes = [len(set(sel)) for sel in all_sels]\n",
        "                        if sizes: # Check if sizes list is not empty\n",
        "                            size_hist = pd.Series(sizes).value_counts().sort_index()\n",
        "                            size_hist_df = size_hist.reset_index()\n",
        "                            size_hist_df.columns = [\"# drugs selected\", \"# patients\"]\n",
        "                            fig5 = px.bar(size_hist_df, x=\"# drugs selected\", y=\"# patients\",\n",
        "                                          title=\"Selection size distribution\")\n",
        "                            fig5.update_layout(xaxis=dict(tickmode='linear')) # Ensure all integer ticks are shown\n",
        "                            st.plotly_chart(fig5, use_container_width=True)\n",
        "                        else:\n",
        "                             st.info(\"No drug selections were made to plot selection size distribution.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not generate selection size histogram: {e}\")\n",
        "\n",
        "                # Synthetic Classification Validation (Plotly)\n",
        "                if all_sels and drugs and pathway_scores_df is not None and not pathway_scores_df.empty:\n",
        "                    try:\n",
        "                        st.subheader(\"Synthetic Classification Validation\")\n",
        "                        # 1) Construct patient-drug feature matrix\n",
        "                        N = len(all_sels)\n",
        "                        X = np.zeros((N, len(drugs)))\n",
        "                        for i, sel in enumerate(all_sels):\n",
        "                            for d in drugs: # Iterate through drugs to ensure correct indexing\n",
        "                                if d in sel:\n",
        "                                    X[i, drugs.index(d)] = 1\n",
        "\n",
        "                        # 2) Synthetic binary labels\n",
        "                        risk_scores = []\n",
        "                        # Ensure we only process samples that were successfully processed in run_analysis\n",
        "                        processed_sample_ids = pathway_scores_df.columns\n",
        "                        if len(processed_sample_ids) == 0:\n",
        "                            st.warning(\"No samples processed for synthetic classification.\")\n",
        "                            raise ValueError(\"No processed samples\") # Trigger exception to skip\n",
        "                        if len(processed_sample_ids) < N:\n",
        "                             st.warning(f\"Only {len(processed_sample_ids)} samples processed for classification (less than requested {N}). Using processed samples.\")\n",
        "                             # Adjust N and X to match processed samples\n",
        "                             N_processed = len(processed_sample_ids)\n",
        "                             # Need to map all_sels back to processed_sample_ids order if necessary\n",
        "                             # Assuming all_sels corresponds to P.columns order\n",
        "                             X_processed = np.zeros((N_processed, len(drugs)))\n",
        "                             processed_sels = all_sels[:N_processed] # Assuming order matches\n",
        "                             for i, sel in enumerate(processed_sels):\n",
        "                                  for d in drugs:\n",
        "                                       if d in sel:\n",
        "                                            X_processed[i, drugs.index(d)] = 1\n",
        "                             X = X_processed # Use the adjusted feature matrix\n",
        "                             N = N_processed # Use the adjusted sample count\n",
        "\n",
        "\n",
        "                        for sid in processed_sample_ids:\n",
        "                             z_path = patient_vector(pathway_scores_df, sid)\n",
        "                             risk_scores.append(z_path.mean())\n",
        "                        risk_scores = np.array(risk_scores)\n",
        "\n",
        "                        # Handle case where all risk scores are the same\n",
        "                        if np.std(risk_scores) == 0:\n",
        "                             st.warning(\"Cannot generate synthetic labels: all pathway mean scores are the same.\")\n",
        "                             st.write(\"Synthetic labels (all 0):\")\n",
        "                             st.write(np.zeros(N, dtype=int))\n",
        "                        else:\n",
        "                             y = (risk_scores > np.median(risk_scores)).astype(int)\n",
        "                             st.write(\"Synthetic labels (0=low risk, 1=high risk):\")\n",
        "                             st.write(np.bincount(y))\n",
        "\n",
        "                             # 3) Train simple logistic regression\n",
        "                             if len(np.unique(y)) > 1: # Only train if there's more than one class\n",
        "                                 clf = LogisticRegression(max_iter=200)\n",
        "                                 clf.fit(X, y)\n",
        "                                 y_pred = clf.predict_proba(X)[:,1]\n",
        "\n",
        "                                 # 4) Metrics\n",
        "                                 roc_auc = roc_auc_score(y, y_pred)\n",
        "                                 pr_auc = average_precision_score(y, y_pred)\n",
        "\n",
        "                                 st.write(f\"ROC-AUC: {roc_auc:.3f}\")\n",
        "                                 st.write(f\"PR-AUC: {pr_auc:.3f}\")\n",
        "\n",
        "                                 # Optional barplot of learned coefficients (Plotly)\n",
        "                                 coefs = pd.Series(clf.coef_[0], index=drugs).sort_values()\n",
        "                                 coefs_df = coefs.reset_index()\n",
        "                                 coefs_df.columns = [\"Drug\", \"Weight\"]\n",
        "                                 fig6 = px.bar(coefs_df, y=\"Drug\", x=\"Weight\", orientation='h',\n",
        "                                               color=\"Weight\", color_continuous_scale=['salmon', 'steelblue'],\n",
        "                                               title=\"Drug selection coefficients (synthetic label prediction)\")\n",
        "                                 st.plotly_chart(fig6, use_container_width=True)\n",
        "\n",
        "                             else:\n",
        "                                 st.warning(\"Classification skipped: only one unique synthetic label class found.\")\n",
        "\n",
        "                    except ValueError as ve:\n",
        "                         # Catch the specific ValueError raised for skipping classification\n",
        "                         st.warning(f\"Synthetic classification skipped: {ve}\")\n",
        "                    except Exception as e:\n",
        "                        st.write(f\"Could not perform synthetic classification validation: {e}\")\n",
        "\n",
        "                # Downloadable reports\n",
        "                if patient_report_df is not None and not patient_report_df.empty:\n",
        "                    st.subheader(\"Download Reports\")\n",
        "                    # Using st.download_button\n",
        "                    create_download_link(patient_report_df, \"patient_report.csv\", \"Download Patient Report (CSV)\")\n",
        "\n",
        "\n",
        "                if freq_df is not None and not freq_df.empty:\n",
        "                     # Using st.download_button\n",
        "                     create_download_link(freq_df, \"drug_frequency_report.csv\", \"Download Drug Frequency Report (CSV)\")\n",
        "\n",
        "\n",
        "                st.success(\"Analysis complete!\")\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload a gene expression data file to begin.\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(streamlit_code)\n",
        "\n",
        "print(\"Updated app.py with documentation and download buttons.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21041c2b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The core logic from the original Jupyter notebook, including functions for data normalization (`norm_ensembl`, `norm_symbol`), data mode detection (`detect_row_mode`), data selection and streaming (`stream_select_rows_columns`), gene-level z-scoring (`zscore_by_gene`), pathway scoring (`pathway_scores`), example drug panel definition (`example_drug_panel`), patient vector creation (`patient_vector`), drug benefit prior calculation (`drug_benefit_prior`), penalty matrix building (`build_penalty_matrix`), QUBO building (`build_qubo`), and exact QUBO solving (`exact_qubo_solve`), was successfully extracted and integrated into the web application's backend logic.\n",
        "*   Streamlit was selected as the web framework for its ease of use in building data science applications.\n",
        "*   A user interface was successfully implemented in Streamlit, featuring a file uploader for gene expression data, sliders for setting the number of samples and lambda parameter, and dedicated sections for displaying results in tables and plots.\n",
        "*   The backend analysis workflow was successfully connected to the Streamlit frontend, allowing the \"Run Analysis\" button to trigger the full data processing and drug selection pipeline using uploaded data and user-defined parameters.\n",
        "*   The application successfully handles data input from CSV/TSV/gzipped files, detects gene ID format (Ensembl vs. Symbol), and outputs results as tables (subset expression, pathway scores, drug frequency, patient report) and visualizations.\n",
        "*   Matplotlib plots were largely converted to interactive Plotly charts (frequency barplot, co-selection heatmaps, selection size histogram, classification coefficients), enhancing the user experience, while the co-selection dendrogram remained a Matplotlib plot embedded in the app.\n",
        "*   Downloadable reports for patient-level results and drug frequency were implemented using `st.download_button`.\n",
        "*   An evaluation of scalability highlighted limitations in memory usage for large files/cohorts and, critically, the exponential scaling of the exact QUBO solver with the number of drugs (infeasible beyond ~20-25 drugs).\n",
        "*   Potential optimizations proposed include using out-of-core processing (Dask, Vaex), implementing approximate classical or QIO solvers for QUBO, parallelizing multi-patient analysis, and improving memory management.\n",
        "*   Comprehensive documentation on application usage, data format, parameters, and results was added to the Streamlit app's main page using markdown.\n",
        "*   The necessary files (`app.py` and `requirements.txt`) were prepared for deployment on a platform like Streamlit Community Cloud.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current exact QUBO solver severely limits the scalability of the drug panel size. Implementing an approximate solver (e.g., simulated annealing) is crucial for applying this workflow to larger, more realistic drug panels.\n",
        "*   For very large gene expression datasets or patient cohorts, incorporating out-of-core processing libraries (like Dask or Vaex) or parallelizing the patient analysis loop will be necessary to handle memory constraints and improve processing time.\n"
      ]
    }
  ]
}